\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{caption}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Compact lists
\setlist{nosep,leftmargin=*}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\placeholder}[1]{\textcolor{blue}{[PLACEHOLDER: #1]}}

\title{Tracing the Adoption of Adversarial Machine Learning Research into Industry Practice (2014--2025)}

\author{
    [Author Names] \\
    [Affiliations] \\
    \texttt{[emails]}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Despite a decade of adversarial machine learning (AML) research producing over 66 catalogued attack techniques and numerous defense mechanisms, industry adoption remains limited---surveys indicate only 5\% of practitioners have experienced AI-specific attacks, yet 86\% express security concerns. This systematic review measures the temporal lag between publication of AML research and evidence of industry adoption across tool integration, benchmark inclusion, regulatory citation, and vendor acknowledgment. Using an artifact-anchored methodology, we trace techniques implemented in major security tools (IBM ART, CleverHans, PyRIT), standardized benchmarks (RobustBench, HarmBench), and regulatory frameworks (MITRE ATLAS, OWASP Top 10) back to their originating academic papers. We code approximately 100 papers spanning 2014--2025 across foundational attacks, privacy threats, physical-world attacks, and LLM-specific vulnerabilities. We examine how adoption speed varies across domains and identify acceleration factors including standardized evaluation, regulatory mandates, and commercial investment. Our coding framework and adoption evidence database provide a foundation for future research-practice alignment studies.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Machine learning systems now influence decisions affecting millions daily---from facial recognition at border crossings~\citep{grother2019frvt} to fraud detection in financial services~\citep{dalpozzolo2015fraud} and content moderation on social platforms~\citep{gorwa2020algorithmic}. This widespread deployment has intensified scrutiny of adversarial vulnerabilities: inputs or interactions crafted to cause models to behave in unintended ways~\citep{biggio2018wild}.

The field of adversarial machine learning emerged with Szegedy et al.'s demonstration that imperceptible perturbations could fool state-of-the-art classifiers~\citep{szegedy2014intriguing}, followed by Goodfellow et al.'s Fast Gradient Sign Method establishing the dominant attack paradigm~\citep{goodfellow2015explaining}. Over the subsequent decade, researchers catalogued 66 attack techniques spanning evasion, poisoning, and privacy threats~\citep{mitreatlas2024}. MITRE ATLAS now documents 33 real-world case studies, and regulatory frameworks including the EU AI Act mandate adversarial robustness testing for high-risk systems~\citep{euaiact2024}.

Yet industry surveys reveal a persistent gap. Kumar et al.~\citep{kumar2020adversarial} found practitioners ``not equipped with tactical and strategic tools'' for ML-specific attacks. Grosse et al.~\citep{grosse2023mlsecurity} reported only 5\% of AI practitioners had experienced AI-specific attacks, despite 86\% expressing concern. Mink et al.~\citep{mink2023security} identified organizational barriers including lack of institutional motivation, inability to assess AML risk, and structures discouraging implementation. Apruzzese et al.~\citep{apruzzese2023real} crystallized these concerns, arguing that ``real attackers don't compute gradients''---academic threat models assume capabilities rarely available in practice.

This gap matters because real-world incidents demonstrate adversarial threats are not merely theoretical. In November 2025, Anthropic disclosed the first documented large-scale AI-orchestrated cyber campaign, with Chinese state-sponsored actors using Claude Code to execute 80--90\% of operational tasks autonomously~\citep{anthropic2025claude}. Tesla Autopilot was fooled by adversarial tape modifications~\citep{keenlab2019tesla,mcafee2020tesla}. Training data extraction from ChatGPT recovered megabytes of verbatim training data for under \$200~\citep{nasr2023extracting}. The OWASP Top 10 for LLM Applications lists prompt injection as the \#1 vulnerability across all versions~\citep{owasp2024llm}.

\subsection{Research Questions}

We address three questions about the research-to-practice transfer in adversarial ML:

\begin{enumerate}
    \item \textbf{RQ1 (Adoption Lag):} What is the typical time lag between publication of landmark AML research and evidence of industry adoption, measured through tool integration, commercial reference, regulatory citation, and production deployment?
    
    \item \textbf{RQ2 (Domain Variation):} How does adoption speed vary across application domains (computer vision, NLP, malware detection, autonomous systems, LLMs), and what factors explain these differences?
    
    \item \textbf{RQ3 (Acceleration Factors):} What mechanisms---regulatory frameworks, standardized benchmarks, industry consortiums, commercial tools---have accelerated adoption, particularly for foundation model security post-2022?
\end{enumerate}

\subsection{Contributions}

This review makes three contributions:

\begin{enumerate}
    \item \textbf{Artifact-anchored methodology:} We introduce a reverse-engineering approach that traces industry artifacts back to source papers, ensuring every paper in our sample has verified adoption evidence by construction.
    
    \item \textbf{Reproducible coding framework:} We release a 12-variable codebook (Table~\ref{tab:codebook}) with decision rules for coding paper characteristics and measuring adoption timelines.
    
    \item \textbf{Quantified adoption analysis:} We provide the first systematic measurement of research-to-industry lag across domains (vision, NLP, LLM, malware), artifact types, and publication eras.
\end{enumerate}

%==============================================================================
\section{Background}
\label{sec:background}
%==============================================================================

\subsection{The Emergence of Adversarial Vulnerabilities}

Modern adversarial ML research began with Szegedy et al.'s December 2013 demonstration that small perturbations could cause misclassification with high confidence~\citep{szegedy2014intriguing}. This work, which received ICLR's 2024 Test of Time Award, revealed that adversarial examples transfer across independently trained models---a finding with profound security implications.

Goodfellow et al.~\citep{goodfellow2015explaining} attributed these vulnerabilities to linear behavior in high-dimensional spaces and introduced the Fast Gradient Sign Method (FGSM), appearing on arXiv December 20, 2014 and published at ICLR 2015. FGSM established gradient-based perturbation as the dominant paradigm and introduced adversarial training as a defense. These foundational works established conventions that shaped subsequent research: white-box access assumptions, $L_p$ perturbation constraints, and optimization-based attack formulations.

Subsequent work refined attack formulations. The Carlini \& Wagner attack~\citep{carlini2017towards} (IEEE S\&P 2017) achieved state-of-the-art success across multiple norms. Madry et al.'s PGD attack~\citep{madry2018towards} (ICLR 2018) established the robust optimization framework:
\begin{equation}
\min_\theta \mathbb{E}_{(x,y)\sim D} \left[ \max_{\|\delta\| \leq \epsilon} L(f_\theta(x + \delta), y) \right]
\end{equation}
This min-max formulation remains the gold standard---10 of the top-10 models on RobustBench use PGD-based adversarial training~\citep{croce2021robustbench}.

\subsection{Expanding Threat Landscape}

Research expanded beyond test-time evasion to encompass the full ML pipeline. \textbf{Privacy attacks} demonstrated that models leak information: Shokri et al.~\citep{shokri2017membership} introduced membership inference at IEEE S\&P 2017; Tram\`{e}r et al.~\citep{tramer2016stealing} demonstrated model extraction from commercial APIs at USENIX Security 2016; Carlini et al.~\citep{carlini2021extracting} extracted verbatim training data from GPT-2 at USENIX Security 2021.

\textbf{Integrity attacks} compromise models during training. BadNets~\citep{gu2017badnets} (arXiv August 2017) demonstrated backdoor injection through poisoned training data. Subsequent work extended backdoors to federated learning~\citep{bagdasaryan2020backdoor} and self-supervised learning~\citep{jia2021badencoder}.

\textbf{Physical-world attacks} demonstrated that adversarial examples survive real-world conditions. Eykholt et al.~\citep{eykholt2018robust} (CVPR 2018) created adversarial stop signs robust to viewing angles and distances. DolphinAttack~\citep{zhang2017dolphinattack} (ACM CCS 2017, Best Paper) compromised voice assistants via ultrasonic commands inaudible to humans.

\subsection{The LLM Security Paradigm Shift}

Large language models introduced qualitatively different adversarial challenges. Unlike traditional attacks requiring gradient access and imperceptible perturbations, LLM attacks exploit semantic properties through natural language.

\textbf{Prompt injection} was first documented by Simon Willison in September 2022 and formalized by Perez \& Ribeiro~\citep{perez2022ignore} at the NeurIPS 2022 Workshop on ML Safety. Greshake et al.~\citep{greshake2023indirect} demonstrated indirect prompt injection against retrieval-augmented systems (arXiv February 2023, ACM AISec November 2023), showing attackers can embed malicious instructions in external content.

\textbf{Automated jailbreaking} emerged with Zou et al.'s GCG attack~\citep{zou2023universal} (arXiv July 2023, NeurIPS 2023 Spotlight), which optimizes adversarial suffixes achieving the first automated jailbreaks against aligned LLMs including ChatGPT, Bard, and Claude. Subsequent work developed more efficient black-box methods: PAIR~\citep{chao2024jailbreaking} achieves jailbreaks in approximately 20 queries; TAP~\citep{mehrotra2024tree} uses tree-of-thought reasoning for further efficiency.

\textbf{LLM defenses} include alignment techniques (RLHF~\citep{ouyang2022instructgpt}, Constitutional AI~\citep{bai2022constitutional}), specialized guardrails (LlamaGuard~\citep{inan2023llamaguard} released December 2023, NeMo Guardrails open-sourced April 2023), and input/output filtering. However, these defenses exhibit brittleness---the HackAPrompt competition~\citep{schulhoff2023hackaprompt} saw all 44 defenses eventually bypassed across 137,000+ adversarial interactions.

\subsection{The Theory-Practice Gap}

Apruzzese et al.~\citep{apruzzese2023real} synthesized concerns about research-practice disconnect through real-world case studies at IEEE SaTML 2023. Their core observation: academic threat models assume attackers possess white-box access, unlimited queries, and gradient computation capabilities rarely available in practice. Real incidents typically involve simpler tactics---basic input manipulation, system-level exploitation, social engineering.

Industry surveys quantify this gap. Kumar et al.~\citep{kumar2020adversarial} interviewed 28 organizations at IEEE S\&P Workshops 2020, finding widespread uncertainty about assessing adversarial risks. Grosse et al.~\citep{grosse2023mlsecurity} surveyed 139 practitioners (IEEE TIFS 2023), finding only 5\% had experienced AI-specific attacks despite 86\% expressing concern. Mink et al.~\citep{mink2023security} conducted 21 interviews at USENIX Security 2023, identifying three barriers: lack of institutional motivation, inability to assess AML risk, and organizational structures discouraging implementation.

However, prior work characterized this gap qualitatively. Our contribution is to measure adoption timelines quantitatively, identifying specific lag durations and acceleration factors. This work does not assess the severity of adversarial threats, but quantitatively measures when research techniques enter practitioner ecosystems.

%==============================================================================
\section{Methodology}
\label{sec:methodology}
%==============================================================================

\subsection{Study Design Overview}

This study measures the temporal lag between adversarial machine learning (AML) research and demonstrable industry adoption. Rather than starting from academic publications and speculating about downstream impact, we adopt an \textit{artifact-anchored backward traceability} methodology. We begin with concrete industry artifacts---open-source security tools, standardized benchmarks, regulatory frameworks, and vendor security documentation---and trace each adopted technique back to the academic paper that originally introduced it.

This design ensures that every paper in our dataset has verifiable evidence of adoption by construction, enabling precise measurement of research-to-practice timelines while avoiding subjective judgments about a paper's ``importance'' or speculative claims about industry relevance.

\subsection{Artifact Universe Definition}

Before extracting any papers, we define and freeze the set of industry artifacts examined in this study. Artifacts were selected according to objective inclusion criteria to minimize selection bias.

\subsubsection{Open-Source Security Tools}

We include open-source AML tools satisfying at least one of the following criteria:

\begin{itemize}[noitemsep]
    \item $\geq$1,000 GitHub stars, or
    \item $\geq$100 academic citations (via Semantic Scholar)
\end{itemize}

Based on these criteria, we analyze five tools:

\begin{enumerate}[noitemsep]
    \item \textbf{IBM Adversarial Robustness Toolbox (ART)}~\citep{nicolae2018art}: 4,800+ stars, Linux Foundation AI project (graduated February 2022)
    
    \item \textbf{CleverHans}~\citep{papernot2016cleverhans}: 6,100+ stars, first major AML library (October 2016)
    
    \item \textbf{Foolbox}: 2,700+ stars, extensive attack implementations
    
    \item \textbf{TextAttack}: 2,900+ stars, NLP-specific attacks
    
    \item \textbf{Microsoft PyRIT}~\citep{pyrit2024}: LLM red-teaming toolkit (February 2024)
\end{enumerate}

For each tool, we extract: (a) technique name, (b) source paper citation from documentation/docstrings, (c) date of first Git commit implementing the technique (via \texttt{git log --follow}), and (d) release version containing the technique.

\subsubsection{Standardized Benchmarks}

We include benchmarks that evaluate AML attacks or defenses as named techniques and are used in peer-reviewed research or cited by industry:

\begin{enumerate}[noitemsep]
    \item \textbf{RobustBench}~\citep{croce2021robustbench}: Leaderboard tracking 120+ defense models with paper citations. We record when each defense was added to the leaderboard (via GitHub commit history).
    
    \item \textbf{AutoAttack}~\citep{croce2020autoattack}: Ensemble evaluation method with component attacks (APGD, FAB, Square Attack). We extract source papers for each component.
    
    \item \textbf{HarmBench}~\citep{mazeika2024harmbench}: Standardized jailbreak evaluation across 33 LLMs, evaluating 18 red-teaming methods with source paper citations.
\end{enumerate}

Only techniques explicitly evaluated and attributed to specific academic papers are included.

\subsubsection{Regulatory and Threat Frameworks}

We analyze the following frameworks:

\begin{enumerate}[noitemsep]
    \item \textbf{MITRE ATLAS}~\citep{mitreatlas2024}: 66 adversarial ML techniques with academic references. We extract technique ID, name, and cited papers. Technique page creation dates determined via Wayback Machine archives.
    
    \item \textbf{NIST AI RMF Adversarial ML Taxonomy} (AI 100-2e2025): Technique categorization with literature references. Publication date: January 2023 (v1.0), updated July 2024 (GenAI Profile).
    
    \item \textbf{OWASP Top 10 for LLM Applications}~\citep{owasp2024llm}: Vulnerability descriptions citing foundational research. Version 1.0 (August 2023) and Version 2.0 (November 2024) analyzed.
\end{enumerate}

We extract only techniques explicitly linked to academic references. Descriptive categories without citations are excluded.

\subsubsection{Vendor Security Documentation}

We analyze publicly available security documentation from major cloud and AI providers:

\begin{itemize}[noitemsep]
    \item AWS SageMaker security documentation
    \item Microsoft Azure AI security whitepapers and Responsible AI documentation  
    \item Google Cloud Vertex AI security guides
    \item OpenAI system cards and security disclosures
    \item Anthropic system cards and security disclosures
\end{itemize}

Only documents that explicitly reference AML techniques or academic work are included. For each reference, we record: (a) technique name, (b) document URL, (c) document publication date, and (d) cited academic paper (if any).

\subsection{Paper Extraction and Validation}

For each artifact, we extract tuples of the form: $(\text{artifact\_name}, \text{artifact\_type}, \text{technique\_name}, \text{cited\_reference})$. Extraction rules:

\begin{enumerate}[noitemsep]
    \item The technique must be named explicitly (e.g., ``FGSM,'' ``Carlini--Wagner,'' ``GCG'').
    \item The artifact must cite a paper directly or attribute the technique to identifiable authors.
    \item Techniques lacking an academic anchor are excluded.
\end{enumerate}

Each extracted reference is resolved to a canonical paper record with DOI (or arXiv ID) and publication date (first arXiv submission if it precedes peer review, otherwise conference/journal date). Papers are validated to ensure they introduce the technique (not merely apply it), the technique name matches the artifact reference, and the paper is publicly accessible.

\subsection{Adoption Event Definition}

We define four observable adoption events, each treated independently:

\begin{table}[H]
\centering
\small
\caption{Adoption event definitions}
\begin{tabular}{p{3cm}p{5.5cm}}
\toprule
\textbf{Adoption Event} & \textbf{Operational Definition} \\
\midrule
Tool adoption & First Git commit implementing the technique \\
Benchmark adoption & Technique evaluated as a named method \\
Regulatory adoption & Technique referenced in regulatory framework \\
Vendor acknowledgment & Technique referenced in vendor security documentation \\
\bottomrule
\end{tabular}
\end{table}

Adoption is defined strictly as acknowledgment or integration, not mitigation or successful defense.

\subsection{Adoption Lag Measurement}

For each (paper, adoption event) pair, we compute:

\begin{equation}
\text{Adoption Lag} = \text{Date}_{\text{artifact}} - \text{Date}_{\text{publication}}
\end{equation}

Artifact dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Tools:} Git commit timestamp (UTC)
    \item \textbf{Benchmarks:} Leaderboard inclusion date or paper publication date
    \item \textbf{Regulatory:} Framework publication or technique page creation date (Wayback Machine)
    \item \textbf{Vendor:} Document publication date (from document metadata or press release)
\end{itemize}

Publication dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Peer-reviewed:} Conference/journal publication date
    \item \textbf{arXiv-first:} First arXiv submission date
\end{itemize}

For papers appearing in multiple artifacts, we report: (a) \textit{first adoption lag} (minimum across all events), (b) \textit{median adoption lag}, and (c) \textit{regulatory adoption lag} (time to MITRE/NIST/OWASP inclusion, if any).

\subsection{Paper Coding Scheme}

Each paper is coded along three dimensions using a fixed codebook (Table~\ref{tab:codebook}). All coding is performed by the authors based on the paper text only. Threat model attributes are coded using author-stated assumptions; if unstated, code as ``Not specified.''

\begin{table}[H]
\centering
\small
\caption{Complete codebook for paper coding}
\label{tab:codebook}
\begin{tabular}{p{2.2cm}p{3.2cm}p{4.3cm}}
\toprule
\textbf{Variable} & \textbf{Values} & \textbf{Coding Rule} \\
\midrule
\multicolumn{3}{l}{\textit{Research Characteristics (G1--G7)}} \\
\midrule
G1: Type & Attack / Defense / Evaluation & Primary contribution \\
G2: Threat category & Evasion / Poisoning / Privacy / N/A & Attack category; N/A for defenses \\
G3: Domain & Vision / NLP / Malware / Audio / Tabular / LLM / Cross-domain & Primary evaluation domain \\
G4: Venue & ML / Security / Journal / arXiv-only & See venue list below \\
G5: Code available & Yes / No & Code link exists at time of coding \\
G6: Code timing & At-pub / Post-pub / Never & At-pub = within 1 month of paper \\
G7: Year & 2014--2025 & Earliest of arXiv or venue date \\
\midrule
\multicolumn{3}{l}{\textit{Threat Model (T1--T2) -- Attack papers only}} \\
\midrule
T1: Access level & White / Gray / Black & White = full model access \\
T2: Gradient required & Yes / No & Gradients used at any stage \\
\midrule
\multicolumn{3}{l}{\textit{Practical Evaluation (Q1--Q3)}} \\
\midrule
Q1: Real-world eval & Yes / Partial / No & Yes = production system tested \\
Q2: Cost reported & Yes / No & Explicit FLOPs, time, or queries \\
Q3: Defense-aware & Yes / No / N/A & N/A for defense papers \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Venue classification:} ML = NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, ACL, EMNLP, NAACL. Security = IEEE S\&P, ACM CCS, USENIX Security, NDSS, IEEE SaTML. Journal = TPAMI, TIFS, TDSC, etc. arXiv-only = no peer-reviewed venue.

\subsubsection{Decision Rules}

\begin{enumerate}[noitemsep]
    \item \textbf{G1:} If paper proposes both attack and defense, code based on which receives more experimental evaluation.
    
    \item \textbf{G2:} For defense papers, code ``N/A.'' Evasion = test-time input perturbation. Poisoning = training-time data manipulation. Privacy = membership inference, model extraction, training data extraction.
    
    \item \textbf{G3:} Code ``Cross-domain'' only if paper evaluates on 2+ distinct domains with separate experiments.
    
    \item \textbf{G4:} Use venue of first peer-reviewed publication. If workshop paper later becomes full paper, use full paper venue.
    
    \item \textbf{T1/T2:} For defense papers, leave blank (these apply to attacks only). White-box = gradients from target model. Gray-box = surrogate model gradients transferred to target. Black-box = query access only, no gradients.
    
    \item \textbf{Q1:} ``Yes'' = tested on production system (commercial API, deployed vehicle). ``Partial'' = realistic simulation or industry dataset. ``No'' = standard benchmarks only (CIFAR, ImageNet, etc.).
    
    \item \textbf{Q3:} For defense papers, code ``N/A.'' For attacks, ``Yes'' = tests against adaptive defenses or AutoAttack.
\end{enumerate}

\textbf{Coding procedure:} For each paper: (1) record metadata (title, authors, DOI/arXiv, publication date, venue); (2) read abstract and introduction to identify technique name and G1 (type); (3) locate threat model section to code T1 and T2 (attack papers only); (4) review experiments to code G3, Q1, Q2, Q3; (5) check code availability for G5, G6; (6) record adoption events with dates from artifacts; (7) calculate adoption lag in months. Estimated time: 20--30 minutes per paper.

\subsection{Statistical Analysis}

We address each research question through pre-specified analyses:

\textbf{RQ1 (Adoption Lag):} We report distributions of first-adoption lag and median-adoption lag, stratified by artifact type (tool, benchmark, regulatory, vendor). We compare lags across publication eras (2014--2017, 2018--2021, 2022--2025) using Kruskal-Wallis tests with post-hoc Dunn tests.

\textbf{RQ2 (Domain Variation):} We compare adoption lags across domains using pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05 / \binom{n}{2}$ for $n$ domains). Primary hypothesis: LLM security papers show significantly shorter lags than computer vision papers.

\textbf{RQ3 (Acceleration Factors):} We fit a Cox proportional hazards model predicting time-to-first-adoption, with covariates: publication year (continuous), domain (categorical), venue type (ML vs. Security), code availability (binary), and threat model (white-box vs. gray-box vs. black-box). Hazard ratios $>$1 indicate faster adoption. Model diagnostics include proportional hazards tests and residual analysis.

\subsection{Reliability, Reproducibility, and Limitations}

\textbf{Reliability:} We employ intra-rater reliability (15\% of papers re-coded after two weeks, targeting $\kappa \geq 0.80$) and inter-rater reliability for papers coded by multiple authors. Adoption dates are verified via \texttt{git log} and Wayback Machine archives.

\textbf{Data release:} All extracted metadata, coding decisions, and artifact URLs will be released as a structured CSV dataset with accompanying codebook.

\textbf{Limitations:} (1) By construction, our sample excludes papers never adopted---we measure adoption timelines, not adoption rates. (2) Our five open-source tools may miss proprietary implementations; timelines represent lower bounds. (3) Vendor documentation dates may reflect updates rather than first publication. (4) Observational design precludes causal claims. (5) We focus on English-language publications and Western regulatory frameworks.

%==============================================================================
\section{Artifact Timeline}
\label{sec:tools}
%==============================================================================

Table~\ref{tab:tools} documents the artifacts we analyze, with release dates verified from GitHub releases and official announcements. These dates establish the timeline against which we measure research adoption.

\begin{table}[H]
\centering
\small
\caption{Artifact release timeline}
\label{tab:tools}
\begin{tabular}{llll}
\toprule
\textbf{Category} & \textbf{Artifact} & \textbf{Release} & \textbf{Source} \\
\midrule
\multirow{5}{*}{Tools} & CleverHans & Oct 2016 & GitHub \\
& Foolbox & Jul 2017 & GitHub \\
& IBM ART & Jul 2018 & GitHub \\
& TextAttack & May 2020 & GitHub \\
& PyRIT & Feb 2024 & GitHub \\
\midrule
\multirow{3}{*}{Benchmarks} & AutoAttack & Mar 2020 & arXiv \\
& RobustBench & Oct 2020 & GitHub \\
& HarmBench & Feb 2024 & GitHub \\
\midrule
\multirow{3}{*}{Regulatory} & MITRE ATLAS & Jun 2021 & Website \\
& NIST AI RMF & Jan 2023 & NIST \\
& OWASP LLM Top 10 & Aug 2023 & Website \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\todo{Complete after paper coding. This section will report:
\begin{itemize}
    \item Sample construction summary (extraction counts by artifact source)
    \item RQ1: Adoption lag distributions (overall, by artifact type, by publication era)
    \item RQ2: Domain variation analysis (lag comparisons across Vision, NLP, LLM, Malware, Audio)
    \item RQ3: Cox regression results (predictors of adoption speed)
\end{itemize}
}

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\todo{Complete after results analysis. This section will include:
\begin{itemize}
    \item Summary of key findings (adoption lag compression, LLM paradigm shift, predictors)
    \item Why some research is never adopted (naming, tooling, threat model barriers)
    \item Implications for researchers and practitioners
    \item Limitations
\end{itemize}
}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

\todo{Complete after results. Will summarize:
\begin{itemize}
    \item Key quantitative findings on adoption lag
    \item Domain-specific patterns
    \item Acceleration factors
    \item Contribution of coding framework and dataset
\end{itemize}
}

%==============================================================================
\appendix
\section{Artifact Source URLs}
\label{sec:appendix}
%==============================================================================

\begin{itemize}[noitemsep]
    \item \textbf{IBM ART:} \url{https://github.com/Trusted-AI/adversarial-robustness-toolbox}
    \item \textbf{CleverHans:} \url{https://github.com/cleverhans-lab/cleverhans}
    \item \textbf{Foolbox:} \url{https://github.com/bethgelab/foolbox}
    \item \textbf{TextAttack:} \url{https://github.com/QData/TextAttack}
    \item \textbf{PyRIT:} \url{https://github.com/Azure/PyRIT}
    \item \textbf{RobustBench:} \url{https://robustbench.github.io/}
    \item \textbf{HarmBench:} \url{https://github.com/centerforaisafety/HarmBench}
    \item \textbf{MITRE ATLAS:} \url{https://atlas.mitre.org/techniques}
    \item \textbf{OWASP LLM Top 10:} \url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}
\end{itemize}

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}