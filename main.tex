\documentclass[sigconf,review]{acmart}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath}

% Remove copyright for review
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}

% Paper metadata
\acmConference[CCS '26]{ACM Conference on Computer and Communications Security}{October 2026}{Salt Lake City, UT, USA}
\acmYear{2026}

\begin{document}

\title{From Research to Reality: Measuring the Adoption Lag of Adversarial Machine Learning Techniques in Industry Practice}

\author{Anonymous Authors}
\affiliation{%
  \institution{Paper \#XXX}
  \country{Unknown}
}
\email{redacted@institution.edu}

\begin{abstract}
The adversarial machine learning (AML) research community has produced over a decade of publications on attacks and defenses, yet practitioners report persistent gaps between academic advances and industry deployment. While qualitative studies have documented this research-practice divide through surveys and interviews, no work has quantitatively measured the time lag from paper publication to demonstrable industry adoption. We address this gap using a novel \textit{artifact-anchored backward traceability} methodology. Starting from 9 authoritative industry artifacts (5 adversarial ML libraries, 3 standardized benchmarks, and the MITRE ATLAS regulatory framework), we trace backward to extract and code 71 papers cited across these artifacts. The libraries include CleverHans, IBM ART, TextAttack, PyRIT, and Foolbox; the benchmarks include RobustBench, AutoAttack, and HarmBench. Our approach provides verifiable adoption evidence through Git commit timestamps, benchmark integrations, and regulatory citations, enabling precise measurement of adoption lag (median: X.X years, IQR: X.X--X.X years). We find [key findings placeholder]. Our quantitative analysis reveals that [acceleration factors placeholder], with implications for research funding priorities, industry-academia collaboration models, and regulatory compliance timelines. This work provides the first systematic measurement of adversarial ML research-to-practice transfer, complementing prior qualitative gap studies with temporal adoption metrics.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003014</concept_id>
<concept_desc>Security and privacy~Malware and its mitigation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Malware and its mitigation}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{adversarial machine learning, technology adoption, research-to-practice gap, empirical measurement}

\maketitle

\section{Introduction}

Adversarial machine learning has emerged as a critical security concern, with over a decade of academic research demonstrating vulnerabilities in ML systems~\cite{szegedy2014intriguing, goodfellow2015explaining, carlini2017towards, madry2018towards}. The field has produced hundreds of attack techniques, defense mechanisms, and robustness evaluations across computer vision~\cite{carlini2017towards}, natural language processing~\cite{morris2020textattack}, and more recently, large language models~\cite{zou2023universal, mazeika2024harmbench}. Yet despite this substantial academic output, industry practitioners consistently report a persistent gap between research advances and operational deployment~\cite{kumar2020adversarial, mink2023security, apruzzese2023realgradients}.

This research-practice divide manifests in concrete ways. Kumar et al.'s interviews with 28 organizations revealed that most ML engineers and incident responders are ``not equipped with tactical and strategic tools to protect, detect and respond to attacks on their ML systems''~\cite{kumar2020adversarial}. Mink et al.'s qualitative analysis found that practitioners face barriers including ``lack of institutional motivation and educational resources,'' ``inability to adequately assess AML risk,'' and ``organizational structures that discourage implementation''~\cite{mink2023security}. Most tellingly, Apruzzese et al. observed that while 89\% of adversarial ML papers focus exclusively on deep learning and 63\% evaluate only image data, ``real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems''~\cite{apruzzese2023realgradients}.

These qualitative gap studies provide rich insights into \textit{why} adoption barriers exist, but they cannot answer fundamental questions about \textit{when} and \textit{how fast} research translates into practice. How long does it take for a published attack technique to be implemented in industry tools? Do defenses get adopted faster than attacks? Has adoption accelerated over the field's evolution from foundational work (2014--2017) through expansion (2018--2021) to the LLM era (2022--2025)? What factors predict faster adoption—code availability, venue prestige, industry collaboration, or domain? Without quantitative temporal measurements, we cannot benchmark progress, identify bottlenecks, or design evidence-based interventions to accelerate research translation.

\subsection{Our Approach: Artifact-Anchored Backward Traceability}

We introduce a novel methodology to measure adversarial ML adoption lag through \textit{reverse-engineering from authoritative industry artifacts}. Rather than starting from papers and speculating about impact through forward citation analysis, we begin with concrete evidence of industry adoption: widely-used tools, standardized benchmarks, and regulatory frameworks. We then trace backward to the research papers they cite and implement.

Our approach selects 9 artifacts representing different adoption pathways. First, we examine five \textbf{tools}: CleverHans (6,401 GitHub stars), IBM Adversarial Robustness Toolbox (5,789 stars), TextAttack (3,348 stars), Microsoft PyRIT (3,343 stars), and Foolbox (2,936 stars). Second, we analyze three \textbf{benchmarks}: RobustBench (NeurIPS 2021, 750+ citations), AutoAttack (ICML 2020, 1,987 citations), and HarmBench (ICML 2024). Third, we consider the \textbf{regulatory framework} MITRE ATLAS (15 tactics, 66 techniques, 33 real-world case studies). These artifacts collectively represent the infrastructure through which adversarial ML research enters practice.

From these 9 artifacts, we automatically extracted 277 unique papers via Git repository scanning of all arXiv references, academic citations, and documentation. We then applied selection criteria prioritizing papers with strongest adoption evidence: 61 papers cited by 2+ artifacts (cross-validated adoption) and 10 papers cited only by MITRE ATLAS (regulatory adoption), yielding a final sample of \textbf{71 papers} for detailed coding. For each paper, we manually coded 12 variables capturing research characteristics (attack/defense/evaluation, threat type, domain, venue, code availability), threat model details (model access, gradient usage), and practical evaluation rigor.

Our methodology provides \textit{verifiable timestamps} for adoption events: Git commit dates when tools first reference papers, benchmark publication dates incorporating research, and MITRE ATLAS case study documentation dates. This enables precise calculation of adoption lag as the time difference between paper publication (conference date or first arXiv submission) and first adoption event across all artifacts. Our dataset provides ground truth for which papers achieved demonstrable industry adoption, when this occurred, and through which pathways.

\subsection{Contributions}

This work makes the following contributions:

\begin{itemize}
\item \textbf{First quantitative measurement} of adversarial ML research-to-practice adoption lag, spanning 2014--2025 across computer vision, NLP, LLMs, and regulatory domains.

\item \textbf{Novel artifact-anchored methodology} using reverse citation tracing from 9 authoritative industry artifacts (5 tools, 3 benchmarks, 1 regulatory framework) to 71 research papers with verified adoption evidence.

\item \textbf{Systematic coding framework} capturing 12 variables per paper (research type, domain, venue, code availability, threat model, practical evaluation), enabling analysis of factors predicting adoption speed.

\item \textbf{Statistical analysis} using Kruskal-Wallis tests, Mann-Whitney U tests with Bonferroni correction, and Cox proportional hazards regression to identify adoption lag patterns by artifact type, publication era, and domain, plus acceleration factors including code availability, venue, and industry collaboration.

\item \textbf{Reproducible dataset and analysis code} providing complete adoption timestamps, coding decisions with inter-rater reliability metrics, and statistical analysis scripts for community validation and extension.
\end{itemize}

\subsection{Paper Organization}

Section~\ref{sec:background} provides background on adversarial ML threats and reviews qualitative gap studies. Section~\ref{sec:rqs} formalizes our research questions. Section~\ref{sec:methodology} details our artifact-anchored methodology, paper selection criteria, coding framework, and statistical analysis plan. Section~\ref{sec:results} presents adoption lag measurements and domain comparisons. Section~\ref{sec:discussion} interprets findings and discusses implications. Section~\ref{sec:conclusion} concludes with recommendations for accelerating research translation.

\section{Background and Related Work}
\label{sec:background}

\subsection{Adversarial Machine Learning Landscape}

Adversarial machine learning encompasses threats across three primary attack categories, formalized in frameworks including MITRE ATLAS and NIST AI 100-2~\cite{nist2025adversarial}.

\textbf{Evasion attacks} modify inputs at test time to cause misclassification while preserving semantic meaning. Foundational work includes Szegedy et al.'s discovery of adversarial examples~\cite{szegedy2014intriguing}, Goodfellow et al.'s Fast Gradient Sign Method (FGSM)~\cite{goodfellow2015explaining}, Carlini \& Wagner's optimization-based attacks~\cite{carlini2017towards}, and Madry et al.'s Projected Gradient Descent (PGD)~\cite{madry2018towards}. Physical-world evasion attacks have demonstrated real-world risks including traffic sign misclassification~\cite{eykholt2018robust} and autonomous vehicle manipulation.

\textbf{Poisoning attacks} corrupt training data or model parameters to degrade performance or insert backdoors. Key work includes backdoor attacks via training data manipulation~\cite{gu2017badnets} and federated learning poisoning. Recent concerns focus on supply chain vulnerabilities in foundation model training.

\textbf{Privacy attacks} extract sensitive information from trained models. Membership inference attacks~\cite{shokri2017membership} determine whether specific records were in training data, while model extraction attacks~\cite{tramer2016stealing} reconstruct model parameters through black-box queries.

For large language models, new attack surfaces have emerged including prompt injection~\cite{greshake2023not}, jailbreaking~\cite{zou2023universal}, and alignment failures~\cite{wei2023jailbroken}. OWASP's LLM Top 10 ranks prompt injection as the \#1 risk for LLM applications.

\subsection{The Research-Practice Gap: Qualitative Evidence}

Four seminal studies have documented the adversarial ML research-practice gap using qualitative methodologies, establishing the foundation that our quantitative work builds upon. Critically, while these studies identify barriers to adoption, \textit{none provide temporal measurements} of how long adoption takes when it does occur.

\textbf{Kumar et al. (2020)}~\cite{kumar2020adversarial} conducted interviews with 28 organizations across 11 industries, finding that practitioners ``are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their ML systems.'' Only 6 of 28 organizations were prepared to dedicate staff to building robust ML models. Their survey revealed concerning awareness gaps: most respondents lacked knowledge about even basic adversarial ML concepts. The study highlighted \textit{organizational process delays}: budget approval cycles, staff allocation timelines, and competing priority assessments that defer security work indefinitely.

\textbf{Grosse et al. (2023)}~\cite{grosse2023mlsecurity} provided the largest quantitative survey with 139 industrial practitioners, finding that approximately 5\% of AI practitioners had experienced AI-specific attacks—remarkably low given academic attention. Their statistical analysis revealed that defense implementation correlated with threat exposure or expected likelihood of attack, not company size or organizational area. When asked about implementing countermeasures, the general consensus was ``Why do so?'' This economic barrier—requiring ROI demonstration before adoption—introduces significant temporal delays as practitioners wait for business cases to materialize.

\textbf{Mink et al. (2023)}~\cite{mink2023security} conducted 21 semi-structured interviews with data scientists and engineers, identifying three primary barriers with implicit temporal dimensions: (1) lack of institutional motivation and educational resources for AML concepts, (2) inability to adequately assess AML risk, and (3) organizational structures discouraging implementation in favor of other objectives. Critically, practitioners explicitly cited \textit{time constraints}: one participant noted that reading research papers is ``a large time sink and a significant barrier,'' while another stated ``I don't have the time to read all research papers to be as up-to-date as a specialist in the area.'' Less than 25\% of surveyed developers had access to security experts, suggesting that knowledge transfer requires years of workforce development.

\textbf{Apruzzese et al. (2023)}~\cite{apruzzese2023realgradients} provided the most comprehensive analysis, examining 88 papers from top security venues (CCS, USENIX Security, NDSS, S\&P) from 2019--2021 plus three real-world case studies. Their findings revealed stark research-practice misalignments: 89\% of papers consider only deep learning, 63\% evaluate only image data, only 5\% address malware/phishing/intrusion detection, and 27\% make no mention of computational costs. The computational cost dimension has direct temporal implications: adversarial training can require 8$\times$ more resources~\cite{apruzzese2023realgradients}, creating technical barriers that extend implementation timelines. Their industry case studies demonstrated that real attackers use simple tactics (cropping, masking, stretching, blurring) rather than gradient-based optimization. They concluded: ``Real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems, and as a result security practitioners have not prioritized adversarial ML defenses.''

\textbf{Boenisch et al. (2021)}~\cite{boenisch2021security} surveyed 83 ML practitioners and found a significant correlation between years of experience and security awareness (r = .36, p = .005), suggesting that knowledge acquisition occurs on \textit{multi-year timescales} rather than through formal education. Only one-third obtained security knowledge from university programs, with 73.5\% learning through practice. Notably, 24\% had never heard of main attack types, indicating generational gaps in workforce readiness. This finding implies that improving adoption speed requires sustained educational interventions over years, not months.

\subsection{Barrier Categories with Temporal Dimensions}

Building on these qualitative studies, we synthesize five barrier categories that create measurable adoption delays, though prior work has not quantified the resulting lag.

\textbf{Organizational barriers} impose structural delays through budget cycles, approval processes, and resource allocation timelines. Kumar et al. found that only 21\% (6 of 28) of organizations were ready to dedicate staff immediately~\cite{kumar2020adversarial}—the remainder face indefinite delays before work can begin. Quarterly and annual planning cycles mean even urgent security needs may wait 3--12 months for budget approval.

\textbf{Technical barriers} create measurable implementation costs. Apruzzese et al. documented that adversarial training requires 8$\times$ computational resources in some configurations~\cite{apruzzese2023realgradients}, while human effort studies in evasion competitions showed domain expertise approaches requiring 42 days versus 12 days for simpler methods. Integration complexity, testing requirements, and validation periods compound these delays.

\textbf{Knowledge transfer barriers} operate on multi-year timescales. Boenisch et al.'s finding that experience duration correlates with security awareness (r = .36, p = .005)~\cite{boenisch2021security} suggests a minimum 1--3 year ramp-up period for individual practitioners, with generational timescales for full workforce transformation. The dominance of practice-based learning (73.5\%) over formal education indicates that classroom interventions alone cannot accelerate adoption.

\textbf{Economic factors} delay adoption through cost-benefit analysis periods. Grosse et al.'s ``Why do so?'' finding~\cite{grosse2023mlsecurity} reflects practitioners' need for demonstrated ROI before investment. Without clear business cases or regulatory mandates, defensive techniques remain unadopted indefinitely.

\textbf{Cultural inertia} creates identity-level resistance. The title of Mink et al.'s study—``Security is not my field, I'm a stats guy''~\cite{mink2023security}—captures practitioners' assumption that security concerns are irrelevant to their ML work. Changing these mental models requires sustained organizational culture shifts measured in years.

\subsection{Technology Adoption Measurement Methodologies}

While no prior work has quantitatively measured adversarial ML adoption lag, established methodologies from other domains provide both methodological foundations and comparative benchmarks.

\textbf{Citation lag analysis}~\cite{nakamura2011citation} measures knowledge diffusion speed through temporal analysis of citation patterns. Nakamura et al. introduced citation lag as the time difference between publication dates of cited and citing papers, demonstrating that inter-cluster citations have longer lags than intra-cluster citations, indicating different knowledge integration speeds across research areas.

\textbf{Translational research lag studies} provide critical benchmarks. Morris, Wooding \& Grant's systematic review~\cite{morris2011answer} synthesized 23 papers quantifying time lags in medical research, revealing substantial variation across different translation pathways. Publication to clinical guideline averaged 8--17 years (range: 0--49 years), drug discovery to commercialization averaged 12 years (range: 10--17 years), and first description to highly cited status averaged 24 years (range: 14--44 years). Three independent methodologies (Balas \& Boren 2000; Grant 2003; Wratschko 2010) converged on a \textbf{17-year average} for research evidence to reach clinical practice, with only 14\% of clinical research adopted into routine practice. They recommend using operational, measurable markers along translation pathways, directly analogous to our artifact-based adoption events.

\textbf{Cybersecurity patching timelines} provide more directly analogous benchmarks. Arora, Telang, and Xu~\cite{arora2008empirical} analyzed vendor patch release behavior and found that public vulnerability disclosure accelerates patch development: vendors release patches in an average of \textbf{28 days with immediate disclosure} versus 63 days without disclosure—a 2.5$\times$ speedup. However, user-side adoption lags further: Decan et al.'s~\cite{decan2018impact} study of npm vulnerabilities showed developers require \textbf{4--11 months} to respond to security threats, with 103 days average for public dependency vulnerabilities to be fixed. For large open-source projects with complex vulnerabilities (e.g., Chromium, OpenSSL), resolution timelines extend to \textbf{2--7 years}~\cite{liu2022vulnerabilities}.

\textbf{Software engineering practice adoption} demonstrates cultural adoption timelines. Agile methodology took approximately \textbf{10--12 years} from the 2001 Agile Manifesto publication to reach 50\%+ industry adoption (2012--2015), and \textbf{$\sim$20 years} to reach near-ubiquitous 80\%+ adoption~\cite{rigby2016agile}. DevOps followed similar patterns with 7--14 years from early adoption to mainstream practice. These timelines reflect the slow pace of organizational culture change independent of technical merit.

\textbf{Machine learning technique diffusion} occurs faster within technical communities but remains selective. AlexNet's 2012 breakthrough in deep learning reached widespread production deployment within \textbf{3--5 years}~\cite{krizhevsky2012imagenet}, though this rapid adoption focused on accuracy improvements rather than security properties. Trustworthiness practices (fairness, security, accountability) ``tend to be neglected'' even in mature ML organizations~\cite{lwakatare2020taxonomy}, suggesting that defensive techniques face longer adoption lags than offensive capabilities.

Table~\ref{tab:adoption_benchmarks} summarizes these adoption timelines across domains, providing context for interpreting adversarial ML adoption patterns.

\begin{table}[t]
\centering
\caption{Adoption lag benchmarks from related domains}
\label{tab:adoption_benchmarks}
\small
\begin{tabular}{@{}llp{5cm}@{}}
\toprule
\textbf{Domain} & \textbf{Typical Lag} & \textbf{Measurement Method} \\
\midrule
Medical research & 17 years & Bibliometric tracking~\cite{morris2011answer} \\
Security patches (vendor) & 28--63 days & CVE analysis~\cite{arora2008empirical} \\
Security patches (user) & 4--11 months & Repository analysis~\cite{decan2018impact} \\
Large OSS vulnerabilities & 2--7 years & Survival analysis~\cite{liu2022vulnerabilities} \\
SE practices (Agile/DevOps) & 10--20 years & Industry surveys~\cite{rigby2016agile} \\
ML techniques (deep learning) & 3--5 years & Deployment surveys \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Backward citation expansion}~\cite{chen2019visualizing} provides methodological justification for our reverse-engineering approach. Chen \& Song's cascading citation expansion methodology enables ``automatic expansion of an initial set by adding articles through citation links in forward, backward, or both directions.'' Backward expansion is particularly useful when ``a researcher may come across a recently published review article and would like to find previously published articles that lead to the state of knowledge summarized in the review.'' This describes precisely our use case, with authoritative tools serving as ``reviews'' of implemented research.

\textbf{Artifact-citation relationships} have been validated for measuring research impact. Frachtenberg's analysis of 2,439 systems papers~\cite{frachtenberg2022research} found that papers with shared artifacts received 75\% more citations than those without. GitHub-hosted artifacts showed 86.7\% availability compared to 77.8\% for university-hosted artifacts. Heumüller et al.'s study of 789 ICSE papers~\cite{heumuller2020publish} confirmed that ``making artifacts publicly available has made a difference in terms of citations as a measure of scientific impact.'' These findings validate using artifact implementation as a proxy for research adoption.

\subsection{Acceleration Factors Identified in Prior Work}

Prior research has identified several mechanisms that could potentially reduce adoption lag, though their effectiveness has not been quantitatively validated in the AML context.

\textbf{Tool ecosystems} reduce implementation barriers. The Adversarial Robustness Toolbox (ART)~\cite{nicolae2018adversarial}, now a Linux Foundation graduated project, supports all major ML frameworks and provides defenses across four attack categories. Practitioner guidance emphasizes tool availability as critical for adoption: without ``off-the-shelf'' solutions, busy ML teams cannot implement research findings~\cite{mink2023security}.

\textbf{Regulatory frameworks} create adoption pressure with specific timelines. The NIST AI Risk Management Framework 1.0 (January 2023) includes estimated adoption timelines of \textbf{3--6 months for foundational adoption} and \textbf{12--24 months for organization-wide integration}~\cite{nist2023ai}. The EU AI Act (2024) mandates that high-risk AI systems be ``resilient against attempts by unauthorised third parties to alter their use,'' with transparency rules binding August 2026. Such regulatory mandates can dramatically accelerate adoption by creating compliance deadlines.

\textbf{Code availability} theoretically accelerates adoption but remains rare. Pineau et al.'s~\cite{pineau2021improving} reproducibility analysis found that only one-third of researchers share data, with even fewer sharing code. NeurIPS reproducibility challenges showed 92\% participation growth from 2019--2021, but standardization remains incomplete. The correlation between code availability and adoption speed remains an empirical question our work addresses.

\textbf{Industry-academia collaboration} models can reduce lag. The Certus Centre research identifies ``industry champions'' as critical for successful technology transfer~\cite{solleiro2013technology}, with organizations having stronger ``absorptive capacity'' adopting research more readily. However, Apruzzese et al.~\cite{apruzzese2023realgradients} note that AML researchers face ``difficulty finding industry partners for case studies,'' suggesting structural barriers to collaboration.

\subsection{Positioning This Work}

Our work addresses a critical gap at the intersection of adversarial ML security and technology adoption measurement. While qualitative gap studies~\cite{kumar2020adversarial, grosse2023mlsecurity, mink2023security, apruzzese2023realgradients, boenisch2021security} have documented \textit{why} research doesn't translate into practice—organizational barriers, awareness deficits, threat model mismatches, computational costs, time constraints—\textit{no work has measured when and how fast adoption occurs}.

This temporal dimension is essential for evidence-based policy and research prioritization. Without quantitative lag measurements, we cannot:

\begin{itemize}
\item \textbf{Benchmark progress}: Is the research-practice gap improving over time, or are adoption delays consistent across the field's evolution?
\item \textbf{Identify domain bottlenecks}: Do certain domains (e.g., computer vision vs. LLMs) face systematically longer delays?
\item \textbf{Evaluate interventions}: Do code release requirements, industry partnerships, or standardized benchmarks actually accelerate adoption?
\item \textbf{Set realistic expectations}: Should funding agencies expect research impact within 3 years, 10 years, or longer?
\end{itemize}

The qualitative literature provides rich descriptions of barriers but cannot answer these quantitative questions. Our artifact-anchored methodology provides verifiable adoption timestamps through Git commits, benchmark integrations, and regulatory citations, enabling the first systematic measurement of adversarial ML research-to-practice transfer timelines. By comparing our findings to adoption benchmarks from medical research (17 years), security patching (28 days to 7 years), and software engineering practices (10--20 years), we can contextualize adversarial ML's translation speed and identify opportunities for acceleration.

\section{Research Questions}
\label{sec:rqs}

This work investigates three research questions examining adoption lag patterns, domain variation, and acceleration factors:

\textbf{RQ1: Adoption Lag Measurement.} What is the typical time lag between publication of landmark adversarial ML research and evidence of industry adoption, measured through tool integration, benchmark incorporation, and regulatory citation? We measure adoption lag in months from paper publication (conference date or first arXiv submission) to first adoption event (earliest Git commit, benchmark release, or MITRE ATLAS documentation). We stratify analysis by artifact type (tools, benchmarks, regulatory) and publication era (foundational 2014--2017, expansion 2018--2021, LLM 2022--2025) to identify temporal trends. Based on related domain benchmarks (Table~\ref{tab:adoption_benchmarks}), we hypothesize that adversarial ML adoption lags fall between security patching (months) and software engineering practices (years), likely in the 1--5 year range.

\textbf{RQ2: Domain Variation.} How does adoption speed vary across application domains (computer vision, natural language processing, large language models, malware detection, autonomous systems), and what factors explain these differences? We compare adoption lag distributions across 7 domains using pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05/21 = 0.0024$) and hypothesize that LLM research shows significantly shorter lags than computer vision due to heightened industry urgency around foundation model security and stronger regulatory pressure (e.g., EU AI Act, Executive Order 14110).

\textbf{RQ3: Acceleration Factors.} What mechanisms predict faster adoption? We examine code availability, publication venue, industry collaboration, and standardized benchmarks. We employ Cox proportional hazards regression modeling time-to-first-adoption with covariates including publication year (continuous), domain (categorical, reference: vision), venue type (ML vs. security, reference: ML conferences), code availability at publication (binary), and threat model assumptions (white-box, gray-box, or black-box; reference: white-box). Hazard ratios greater than 1 indicate faster adoption. We validate proportional hazards assumptions using Schoenfeld residuals. Based on prior work suggesting code availability improves research impact~\cite{frachtenberg2022research,heumuller2020publish}, we hypothesize that papers with publicly available code at publication show 1.5--2$\times$ faster adoption.
 
\section{Methodology}
\label{sec:methodology}

\subsection{Artifact-Anchored Approach}

Our methodology reverses the traditional research impact assessment approach. Rather than starting from papers and tracking forward citations to speculate about practical impact, we begin with concrete evidence of industry adoption: authoritative tools, benchmarks, and frameworks actively used by practitioners. We then trace backward to the research papers they cite and implement.

This \textit{artifact-anchored backward traceability} approach provides three key advantages. First, \textbf{verified adoption}: every paper in our sample has demonstrable evidence of industry use through tool implementation, benchmark integration, or regulatory citation. Second, \textbf{precise timestamps}: Git commit dates, benchmark publication dates, and framework documentation provide verifiable adoption event timing. Third, \textbf{multiple pathways}: we capture diverse adoption mechanisms including open-source tools, academic benchmarks, and regulatory frameworks, providing comprehensive coverage of research translation routes.

\subsection{Artifact Selection}

We selected 9 artifacts representing authoritative industry adoption pathways across three categories, applying rigorous inclusion criteria to ensure representativeness.

\subsubsection{Open-Source Tools (5 artifacts)}

Tool selection criteria: (1) $\geq$1,000 GitHub stars indicating substantial community adoption; (2) Active maintenance with commits in 2024--2025; (3) Focus on adversarial ML specifically (not general ML security); (4) Multiple framework support or domain coverage.

\begin{itemize}
\item \textbf{CleverHans} (6,401 stars): First major AML library, created October 2016 by Ian Goodfellow (Google Brain/OpenAI) and Nicolas Papernot. Maintained by CleverHans Lab at University of Toronto. Provides reference implementations for foundational attacks (FGSM, PGD, C\&W) across JAX, PyTorch, TensorFlow~\cite{papernot2018cleverhans}.

\item \textbf{IBM Adversarial Robustness Toolbox} (5,789 stars): Enterprise-focused library created July 2018, donated to Linux Foundation AI \& Data in 2020. Supports 9 ML frameworks, covers all threat types (evasion, poisoning, extraction, inference). Used in DARPA GARD program and DoD testing~\cite{nicolae2018adversarial}.

\item \textbf{TextAttack} (3,348 stars): Dominant NLP adversarial framework, published EMNLP 2020 by QData Lab (UVA). Implements 16 attack recipes with HuggingFace integration. 835+ citations~\cite{morris2020textattack}.

\item \textbf{PyRIT} (3,343 stars): Microsoft's LLM red-teaming framework, released February 2024. Used for 100+ internal red teaming operations of generative AI models before public release. Integrates with Azure AI evaluation~\cite{munoz2024pyrit}.

\item \textbf{Foolbox} (2,936 stars): Academic benchmark tool from Bethge Lab (Tübingen), dual peer-reviewed publications (JOSS 2020, ICML 2017). Emphasizes minimum perturbation measurement and scientific rigor~\cite{rauber2020foolbox}.
\end{itemize}

\subsubsection{Standardized Benchmarks (3 artifacts)}

Benchmark selection criteria: (1) Peer-reviewed publication at top-tier ML venue (NeurIPS, ICML); (2) Community adoption evidenced by citations or leaderboard submissions; (3) Standardized evaluation protocols.

\begin{itemize}
\item \textbf{RobustBench} (NeurIPS 2021): Standardized adversarial robustness leaderboard with 750+ citations, 120+ evaluated models. Uses AutoAttack for consistent evaluation across CIFAR-10, CIFAR-100, ImageNet~\cite{croce2020robustbench}.

\item \textbf{AutoAttack} (ICML 2020): Parameter-free attack ensemble (APGD-CE, APGD-DLR, FAB, Square Attack) with 1,987 citations. Revealed 13 of 50+ published defenses had robust accuracy overestimated by >10\%~\cite{croce2020reliable}.

\item \textbf{HarmBench} (ICML 2024): First standardized LLM jailbreak evaluation framework. Covers 510 harmful behaviors, 18 attack methods, 33 target LLMs. Backed by Center for AI Safety~\cite{mazeika2024harmbench}.
\end{itemize}

\subsubsection{Regulatory Framework (1 artifact)}

\begin{itemize}
\item \textbf{MITRE ATLAS}: Industry-standard adversarial ML threat taxonomy (15 tactics, 66 techniques, 33 case studies). Co-created with Microsoft in 2020, now with 16 member organizations. \$20M NIST partnership (December 2025). Explicitly referenced in EU AI Act alignment and CISA guidance.
\end{itemize}

\subsection{Paper Extraction and Selection}

We employed automated extraction followed by manual selection based on adoption evidence strength.

\subsubsection{Automated Extraction (277 papers)}

For each of the 9 artifacts, we:
\begin{enumerate}
\item Cloned the complete Git repository history
\item Scanned all files (code, documentation, README, citations) for arXiv identifiers using regex pattern \texttt{arxiv.org/abs/\textbackslash d+\textbackslash.\textbackslash d+}
\item Extracted academic citations from published benchmark papers (RobustBench, AutoAttack, HarmBench)
\item Retrieved MITRE ATLAS case study citations from framework documentation
\item Deduplicated across artifacts to create initial pool of 277 unique papers
\end{enumerate}

\subsubsection{Selection Criteria (71 papers)}

From the 277-paper pool, we applied two selection criteria prioritizing strongest adoption evidence:

\textbf{Criterion 1: Multi-artifact papers ($n=61$).} Papers cited by two or more artifacts demonstrate cross-validated adoption across different industry pathways. For example, a paper might be implemented in both CleverHans and IBM ART, or cited by both RobustBench and MITRE ATLAS. This criterion ensures a robust adoption signal.

\textbf{Criterion 2: MITRE ATLAS-only papers ($n=10$).} Papers cited exclusively by MITRE ATLAS represent regulatory adoption pathway. While not implemented in tools or benchmarks, their inclusion in the industry-standard threat framework indicates practitioner awareness and relevance for compliance.

Final sample: \textbf{71 papers} with verified adoption evidence spanning 2014--2025.

\subsection{Coding Framework}

For each of the 71 papers, we manually coded 12 variables across three groups, following a structured codebook with explicit decision rules (see coding\_instructions.pdf in reproducibility materials).

\subsubsection{Research Characteristics (G1--G6)}

\begin{itemize}
\item \textbf{G1 - Type}: Attack, Defense, or Evaluation (primary contribution)
\item \textbf{G2 - Threat}: Evasion, Poisoning, Privacy, or N/A (attack category per NIST taxonomy)
\item \textbf{G3 - Domain}: Vision, NLP, LLM, Malware, Audio, Tabular, or Cross-domain (primary evaluation domain)
\item \textbf{G4 - Venue}: ML conference, Security conference, Journal, or arXiv-only (publication type)
\item \textbf{G5 - Code available}: Yes or No (whether code link exists at time of coding)
\item \textbf{G6 - Code timing}: At-publication, Post-publication, or Never (when code was released; ``at-publication'' means within 1 month of paper date)
\end{itemize}

\subsubsection{Threat Model (T1--T2, Attack Papers Only)}

\begin{itemize}
\item \textbf{T1 - Access level}: White-box, Gray-box, or Black-box (model access assumptions: white-box means access to weights/gradients, gray-box means surrogate model, black-box means queries only)
\item \textbf{T2 - Gradient required}: Yes or No (whether gradients are used at any attack stage)
\end{itemize}

\subsubsection{Practical Evaluation (Q1)}

\begin{itemize}
\item \textbf{Q1 - Real-world evaluation}: Yes, Partial, or No. ``Yes'' means tested on production systems like Google API, Tesla, or ChatGPT. ``Partial'' means realistic simulation. ``No'' means evaluation only on standard datasets like CIFAR or ImageNet.
\end{itemize}

\subsubsection{Coding Procedure}

Two coders independently coded all 71 papers following the structured codebook. Initial coding was performed by GPT-4o with prompt-engineered instructions, then manually verified and corrected by human coders, resulting in 39 corrections documented in \texttt{coding\_corrections.csv}. Inter-rater reliability was assessed using Cohen's $\kappa$ across all 12 variables. Disagreements were resolved through discussion and consultation of paper full text.

\subsection{Adoption Event Definitions and Lag Calculation}

We define three types of adoption events with specific timestamp sources:

\textbf{Tool adoption}: Date of first Git commit that references the paper in code, documentation, or citations file. We extracted commit timestamps (UTC) using \texttt{git log --all --grep} for paper titles and arXiv IDs, verified through manual inspection.

\textbf{Benchmark adoption}: Publication date of the benchmark paper (conference proceedings date) that cites the research. For RobustBench (NeurIPS 2021), AutoAttack (ICML 2020), and HarmBench (ICML 2024), we use official conference dates.

\textbf{Regulatory adoption}: Date when MITRE ATLAS case study or technique description citing the paper was first published in framework documentation (extracted from GitHub repository history of \texttt{mitre/advmlthreatmatrix}).

For each paper, we record \textit{all} adoption events across the 9 artifacts, then identify the \textbf{first adoption} as the earliest event across all pathways. Adoption lag is calculated as:

\begin{equation}
\text{Adoption Lag (months)} = \text{Date}_{\text{first adoption}} - \text{Date}_{\text{publication}}
\end{equation}

Where $\text{Date}_{\text{publication}}$ is the earlier of: (1) conference/journal publication date, or (2) first arXiv submission date. All dates standardized to YYYY-MM-DD format, with lag calculated in months for consistency with translational research literature~\cite{morris2011answer}.

\subsection{Statistical Analysis Plan}

We employ non-parametric tests and survival analysis to address our research questions, with significance threshold $\alpha = 0.05$ (Bonferroni-corrected for multiple comparisons where applicable).

\subsubsection{RQ1: Adoption Lag Measurement}

\textbf{Descriptive statistics}: Median, interquartile range (IQR), range, and mean of adoption lags across full sample ($n=71$).

\textbf{Stratification by artifact type}: Compare adoption lags for papers adopted through tools-only, benchmarks-only, regulatory-only, and multi-pathway adoption using Kruskal-Wallis test with post-hoc Dunn tests (Bonferroni-corrected).

\textbf{Stratification by publication era}: Compare adoption lags across three eras—foundational (2014--2017), expansion (2018--2021), LLM (2022--2025)—using Kruskal-Wallis test to identify temporal trends.

\subsubsection{RQ2: Domain Variation}

\textbf{Pairwise domain comparison}: Compare adoption lag distributions across 7 domains (Vision, NLP, LLM, Malware, Audio, Tabular, Cross-domain) using Mann-Whitney U tests with Bonferroni correction for 21 pairwise comparisons ($\alpha = 0.05/21 = 0.0024$).

\textbf{Hypothesis test}: LLM papers ($n_{\text{LLM}}$) show significantly shorter adoption lags than computer vision papers ($n_{\text{CV}}$) due to heightened industry urgency. One-tailed Mann-Whitney U test.

\subsubsection{RQ3: Acceleration Factors}

\textbf{Cox proportional hazards regression}: Model time-to-first-adoption using Cox regression:

\begin{equation}
\lambda(t|X) = \lambda_0(t) \cdot \exp(\beta_1 X_{\text{year}} + \beta_2 X_{\text{domain}} + \beta_3 X_{\text{venue}} + \beta_4 X_{\text{code}} + \beta_5 X_{\text{threat}})
\end{equation}

where $\lambda(t|X)$ is the hazard rate (instantaneous adoption probability) at time $t$ given covariates $X$. The covariates include:
\begin{itemize}
\item $X_{\text{year}}$: Publication year (continuous variable)
\item $X_{\text{domain}}$: Domain (categorical: Vision, NLP, LLM, Malware, Audio, Tabular, Cross-domain; reference category is Vision)
\item $X_{\text{venue}}$: Venue type (ML conference vs. Security conference; reference category is ML)
\item $X_{\text{code}}$: Code available at publication (binary: Yes or No)
\item $X_{\text{threat}}$: Threat model (White-box, Gray-box, or Black-box; reference category is White-box)
\end{itemize}

We interpret hazard ratios as $\exp(\beta_i)$, where values greater than 1 indicate faster adoption and values less than 1 indicate slower adoption. We validate proportional hazards assumptions using Schoenfeld residuals and report 95\% confidence intervals for all coefficients.

\subsubsection{Software}

All analyses conducted in Python 3.11 using: \texttt{pandas} (1.5.3) for data manipulation, \texttt{scipy} (1.10.1) for Kruskal-Wallis and Mann-Whitney U tests, \texttt{lifelines} (0.27.4) for Cox regression, \texttt{matplotlib} (3.7.1) and \texttt{seaborn} (0.12.2) for visualization. Analysis scripts and data available at [anonymized GitHub repository].

\section{Results}
\label{sec:results}

% Placeholder for results - to be filled after coding and statistical analysis

\subsection{Sample Characteristics}

% Table 1: Descriptive statistics of coded papers
% - Distribution across research types (attack/defense/evaluation)
% - Distribution across domains (Vision/NLP/LLM/etc.)
% - Distribution across venues (ML/Security/Journal/arXiv)
% - Code availability statistics
% - Threat model distributions

\subsection{RQ1: Adoption Lag Patterns}

% Figure 1: Distribution of adoption lags (violin plot or box plot)
% Table 2: Adoption lag statistics by artifact type
% Table 3: Adoption lag statistics by publication era
% Key findings about median lag, IQR, temporal trends

\subsection{RQ2: Domain Variation}

% Figure 2: Adoption lag by domain (box plots with significance markers)
% Table 4: Pairwise Mann-Whitney U test results with Bonferroni correction
% Table 5: LLM vs. CV hypothesis test results

\subsection{RQ3: Acceleration Factors}

% Table 6: Cox proportional hazards regression results
% - Hazard ratios, 95% CIs, p-values for all covariates
% Figure 3: Forest plot of hazard ratios
% Schoenfeld residuals diagnostic plots in appendix

\section{Discussion}
\label{sec:discussion}

% Placeholder for discussion - to be filled after results analysis

\subsection{Key Findings Interpretation}

% Interpretation of adoption lag patterns, domain differences, acceleration factors
% Comparison to translational research benchmarks (Morris et al.'s 17 years, security patching 28 days - 7 years)
% Contextualization within adversarial ML field evolution
% Discussion of how findings compare to Table 1 benchmarks

\subsection{Implications for Researchers}

% How researchers can accelerate adoption of their work
% Importance of code release, industry collaboration, benchmark integration
% Publication venue considerations
% Concrete recommendations based on acceleration factors identified in Cox regression

\subsection{Implications for Practitioners}

% How practitioners can identify adoptable research
% Using adoption lag as signal for maturity
% Prioritization of defensive techniques based on adoption patterns
% Addressing time constraints and knowledge transfer barriers identified in Section 2.2

\subsection{Implications for Policy and Funding}

% Realistic timeline expectations for research translation (informed by Section 2.3 benchmarks)
% Funding mechanisms to accelerate adoption (e.g., tool development grants, industry partnerships)
% Regulatory intervention timing based on adoption lag findings

\subsection{Limitations}

% Selection bias: artifact selection may miss other adoption pathways
% Implementation ≠ full adoption: citation doesn't guarantee usage
% Missing informal adoption: proprietary implementations invisible
% Temporal resolution: Git commits may not reflect actual implementation timing
% Citation completeness: tools may implement without citing
% Survival bias: only successful tools analyzed

\subsection{Future Work}

% Extending analysis to broader set of artifacts
% Longitudinal tracking of ongoing adoptions
% Qualitative interviews to complement quantitative findings (e.g., validating barriers in Section 2.2)
% Cross-domain comparison with other security areas
% Investigating the 5 barrier categories quantitatively

\section{Conclusion}
\label{sec:conclusion}

% Placeholder for conclusion - to be filled after results

% Summary of contributions
% Restatement of key findings in context of related domain benchmarks
% Call to action for research community, funding agencies, industry
% Recommendations for accelerating adoption based on identified factors

\section{Acknowledgments}

This work was supported by [funding agency redacted for review]. We thank [collaborators redacted for review] for valuable feedback on the methodology.

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}