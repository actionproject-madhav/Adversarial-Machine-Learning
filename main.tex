\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{caption}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Compact lists
\setlist{nosep,leftmargin=*}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\placeholder}[1]{\textcolor{blue}{[PLACEHOLDER: #1]}}

\title{Tracing the Adoption of Adversarial Machine Learning Research into Industry Practice (2014--2025)}

\author{
    [Author Names] \\
    [Affiliations] \\
    \texttt{[emails]}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Despite a decade of adversarial machine learning (AML) research producing over 66 catalogued attack techniques and numerous defense mechanisms, industry adoption remains limited---surveys indicate only 5\% of practitioners have experienced AI-specific attacks, yet 86\% express security concerns. This systematic review measures the temporal lag between publication of AML research and evidence of industry adoption across tool integration, benchmark inclusion, regulatory citation, and vendor acknowledgment. Using an artifact-anchored methodology, we trace techniques implemented in major security tools (IBM ART, CleverHans, PyRIT), standardized benchmarks (RobustBench, HarmBench), and regulatory frameworks (MITRE ATLAS, OWASP Top 10) back to their originating academic papers. We code approximately 100 papers spanning 2014--2025 across foundational attacks, privacy threats, physical-world attacks, and LLM-specific vulnerabilities. We examine how adoption speed varies across domains and identify acceleration factors including standardized evaluation, regulatory mandates, and commercial investment. Our coding framework and adoption evidence database provide a foundation for future research-practice alignment studies.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Machine learning systems now influence decisions affecting millions daily---from facial recognition at border crossings~\citep{grother2019frvt} to fraud detection in financial services~\citep{dalpozzolo2015fraud} and content moderation on social platforms~\citep{gorwa2020algorithmic}. This widespread deployment has intensified scrutiny of adversarial vulnerabilities: inputs or interactions crafted to cause models to behave in unintended ways~\citep{biggio2018wild}.

The field of adversarial machine learning emerged with Szegedy et al.'s demonstration that imperceptible perturbations could fool state-of-the-art classifiers~\citep{szegedy2014intriguing}, followed by Goodfellow et al.'s Fast Gradient Sign Method establishing the dominant attack paradigm~\citep{goodfellow2015explaining}. Over the subsequent decade, researchers catalogued 66 attack techniques spanning evasion, poisoning, and privacy threats~\citep{mitreatlas2024}. MITRE ATLAS now documents 33 real-world case studies, and regulatory frameworks including the EU AI Act mandate adversarial robustness testing for high-risk systems~\citep{euaiact2024}.

Yet industry surveys reveal a persistent gap. Kumar et al.~\citep{kumar2020adversarial} found practitioners ``not equipped with tactical and strategic tools'' for ML-specific attacks. Grosse et al.~\citep{grosse2023mlsecurity} reported only 5\% of AI practitioners had experienced AI-specific attacks, despite 86\% expressing concern. Mink et al.~\citep{mink2023security} identified organizational barriers including lack of institutional motivation, inability to assess AML risk, and structures discouraging implementation. Apruzzese et al.~\citep{apruzzese2023real} crystallized these concerns, arguing that ``real attackers don't compute gradients''---academic threat models assume capabilities rarely available in practice.

This gap matters because real-world incidents demonstrate adversarial threats are not merely theoretical. In November 2025, Anthropic disclosed the first documented large-scale AI-orchestrated cyber campaign, with Chinese state-sponsored actors using Claude Code to execute 80--90\% of operational tasks autonomously~\citep{anthropic2025claude}. Tesla Autopilot was fooled by adversarial tape modifications~\citep{keenlab2019tesla,mcafee2020tesla}. Training data extraction from ChatGPT recovered megabytes of verbatim training data for under \$200~\citep{nasr2023extracting}. The OWASP Top 10 for LLM Applications lists prompt injection as the \#1 vulnerability across all versions~\citep{owasp2024llm}.

\subsection{Research Questions}

We address three questions about the research-to-practice transfer in adversarial ML:

\begin{enumerate}
    \item \textbf{RQ1 (Adoption Lag):} What is the typical time lag between publication of landmark AML research and evidence of industry adoption, measured through tool integration, commercial reference, regulatory citation, and production deployment?
    
    \item \textbf{RQ2 (Domain Variation):} How does adoption speed vary across application domains (computer vision, NLP, malware detection, autonomous systems, LLMs), and what factors explain these differences?
    
    \item \textbf{RQ3 (Acceleration Factors):} What mechanisms---regulatory frameworks, standardized benchmarks, industry consortiums, commercial tools---have accelerated adoption, particularly for foundation model security post-2022?
\end{enumerate}

\subsection{Contributions}

This review makes three contributions:

\begin{enumerate}
    \item \textbf{Artifact-anchored methodology:} We introduce a reverse-engineering approach that traces industry artifacts back to source papers, ensuring every paper in our sample has verified adoption evidence by construction.
    
    \item \textbf{Reproducible coding framework:} We release a 12-variable codebook (Table~\ref{tab:codebook}) with decision rules for coding paper characteristics and measuring adoption timelines.
    
    \item \textbf{Quantified adoption analysis:} We provide the first systematic measurement of research-to-industry lag across domains (vision, NLP, LLM, malware), artifact types, and publication eras.
\end{enumerate}

%==============================================================================
\section{Background}
\label{sec:background}
%==============================================================================

\subsection{The Emergence of Adversarial Vulnerabilities}

Modern adversarial ML research began with Szegedy et al.'s December 2013 demonstration that small perturbations could cause misclassification with high confidence~\citep{szegedy2014intriguing}. This work, which received ICLR's 2024 Test of Time Award, revealed that adversarial examples transfer across independently trained models---a finding with profound security implications.

Goodfellow et al.~\citep{goodfellow2015explaining} attributed these vulnerabilities to linear behavior in high-dimensional spaces and introduced the Fast Gradient Sign Method (FGSM), appearing on arXiv December 20, 2014 and published at ICLR 2015. FGSM established gradient-based perturbation as the dominant paradigm and introduced adversarial training as a defense. These foundational works established conventions that shaped subsequent research: white-box access assumptions, $L_p$ perturbation constraints, and optimization-based attack formulations.

Subsequent work refined attack formulations. The Carlini \& Wagner attack~\citep{carlini2017towards} (IEEE S\&P 2017) achieved state-of-the-art success across multiple norms. Madry et al.'s PGD attack~\citep{madry2018towards} (ICLR 2018) established the robust optimization framework:
\begin{equation}
\min_\theta \mathbb{E}_{(x,y)\sim D} \left[ \max_{\|\delta\| \leq \epsilon} L(f_\theta(x + \delta), y) \right]
\end{equation}
This min-max formulation remains the gold standard---10 of the top-10 models on RobustBench use PGD-based adversarial training~\citep{croce2021robustbench}.

\subsection{Expanding Threat Landscape}

Research expanded beyond test-time evasion to encompass the full ML pipeline. \textbf{Privacy attacks} demonstrated that models leak information: Shokri et al.~\citep{shokri2017membership} introduced membership inference at IEEE S\&P 2017; Tram\`{e}r et al.~\citep{tramer2016stealing} demonstrated model extraction from commercial APIs at USENIX Security 2016; Carlini et al.~\citep{carlini2021extracting} extracted verbatim training data from GPT-2 at USENIX Security 2021.

\textbf{Integrity attacks} compromise models during training. BadNets~\citep{gu2017badnets} (arXiv August 2017) demonstrated backdoor injection through poisoned training data. Subsequent work extended backdoors to federated learning~\citep{bagdasaryan2020backdoor} and self-supervised learning~\citep{jia2021badencoder}.

\textbf{Physical-world attacks} demonstrated that adversarial examples survive real-world conditions. Eykholt et al.~\citep{eykholt2018robust} (CVPR 2018) created adversarial stop signs robust to viewing angles and distances. DolphinAttack~\citep{zhang2017dolphinattack} (ACM CCS 2017, Best Paper) compromised voice assistants via ultrasonic commands inaudible to humans.

\subsection{The LLM Security Paradigm Shift}

Large language models introduced qualitatively different adversarial challenges. Unlike traditional attacks requiring gradient access and imperceptible perturbations, LLM attacks exploit semantic properties through natural language.

\textbf{Prompt injection} was first documented by Simon Willison in September 2022 and formalized by Perez \& Ribeiro~\citep{perez2022ignore} at the NeurIPS 2022 Workshop on ML Safety. Greshake et al.~\citep{greshake2023indirect} demonstrated indirect prompt injection against retrieval-augmented systems (arXiv February 2023, ACM AISec November 2023), showing attackers can embed malicious instructions in external content.

\textbf{Automated jailbreaking} emerged with Zou et al.'s GCG attack~\citep{zou2023universal} (arXiv July 2023, NeurIPS 2023 Spotlight), which optimizes adversarial suffixes achieving the first automated jailbreaks against aligned LLMs including ChatGPT, Bard, and Claude. Subsequent work developed more efficient black-box methods: PAIR~\citep{chao2024jailbreaking} achieves jailbreaks in approximately 20 queries; TAP~\citep{mehrotra2024tree} uses tree-of-thought reasoning for further efficiency.

\textbf{LLM defenses} include alignment techniques (RLHF~\citep{ouyang2022instructgpt}, Constitutional AI~\citep{bai2022constitutional}), specialized guardrails (LlamaGuard~\citep{inan2023llamaguard} released December 2023, NeMo Guardrails open-sourced April 2023), and input/output filtering. However, these defenses exhibit brittleness---the HackAPrompt competition~\citep{schulhoff2023hackaprompt} saw all 44 defenses eventually bypassed across 137,000+ adversarial interactions.

\subsection{The Theory-Practice Gap}

Apruzzese et al.~\citep{apruzzese2023real} synthesized concerns about research-practice disconnect through real-world case studies at IEEE SaTML 2023. Their core observation: academic threat models assume attackers possess white-box access, unlimited queries, and gradient computation capabilities rarely available in practice. Real incidents typically involve simpler tactics---basic input manipulation, system-level exploitation, social engineering.

Industry surveys quantify this gap. Kumar et al.~\citep{kumar2020adversarial} interviewed 28 organizations at IEEE S\&P Workshops 2020, finding widespread uncertainty about assessing adversarial risks. Grosse et al.~\citep{grosse2023mlsecurity} surveyed 139 practitioners (IEEE TIFS 2023), finding only 5\% had experienced AI-specific attacks despite 86\% expressing concern. Mink et al.~\citep{mink2023security} conducted 21 interviews at USENIX Security 2023, identifying three barriers: lack of institutional motivation, inability to assess AML risk, and organizational structures discouraging implementation.

However, prior work characterized this gap qualitatively. Our contribution is to measure adoption timelines quantitatively, identifying specific lag durations and acceleration factors. This work does not assess the severity of adversarial threats, but quantitatively measures when research techniques enter practitioner ecosystems.

%==============================================================================
\section{Methodology}
\label{sec:methodology}
%==============================================================================

\subsection{Study Design Overview}

This study measures the temporal lag between adversarial machine learning (AML) research and demonstrable industry adoption. Rather than starting from academic publications and speculating about downstream impact, we adopt an \textit{artifact-anchored backward traceability} methodology. We begin with concrete industry artifacts---open-source security tools, standardized benchmarks, regulatory frameworks, and vendor security documentation---and trace each adopted technique back to the academic paper that originally introduced it.

This design ensures that every paper in our dataset has verifiable evidence of adoption by construction, enabling precise measurement of research-to-practice timelines while avoiding subjective judgments about a paper's ``importance'' or speculative claims about industry relevance.

\subsection{Artifact Universe Definition}

Before extracting any papers, we define and freeze the set of industry artifacts examined in this study. Artifacts were selected according to objective inclusion criteria to minimize selection bias.

\subsubsection{Open-Source Security Tools}

We include open-source AML tools satisfying the criterion of $\geq$1,000 GitHub stars, ensuring broad community adoption and active maintenance. Based on this criterion, we analyze five tools (Table~\ref{tab:tools}):

\begin{table}[H]
\centering
\small
\caption{Selected open-source AML tools}
\label{tab:tools}
\begin{tabular}{lrl}
\toprule
\textbf{Tool} & \textbf{Stars} & \textbf{Description} \\
\midrule
CleverHans & 6,401 & First major AML library (University of Toronto) \\
IBM ART & 5,789 & Adversarial Robustness Toolbox (Linux Foundation) \\
TextAttack & 3,348 & NLP adversarial attacks \\
PyRIT & 3,343 & LLM red-teaming toolkit (Microsoft) \\
Foolbox & 2,936 & Adversarial attacks (Bethge Lab) \\
\bottomrule
\end{tabular}
\end{table}

For each tool, we clone the Git repository and programmatically extract arXiv references from source code, documentation, docstrings, and bibliography files. Adoption dates are determined via \texttt{git log --follow} to identify the first commit referencing each paper.

\subsubsection{Standardized Benchmarks}

We include benchmarks that are (1) published in peer-reviewed venues and (2) evaluate AML attacks or defenses as named techniques with explicit paper citations:

\begin{table}[H]
\centering
\small
\caption{Selected AML benchmarks}
\label{tab:benchmarks}
\begin{tabular}{llp{5.5cm}}
\toprule
\textbf{Benchmark} & \textbf{Venue} & \textbf{Description} \\
\midrule
RobustBench & NeurIPS 2021 & Adversarial robustness leaderboard (120+ models) \\
AutoAttack & ICML 2020 & Standardized attack ensemble evaluation \\
HarmBench & ICML 2024 & LLM jailbreak evaluation (33 LLMs, 18 methods) \\
\bottomrule
\end{tabular}
\end{table}

Only techniques explicitly evaluated and attributed to specific academic papers are included. Adoption dates are extracted from Git commit history of benchmark repositories.

\subsubsection{Regulatory and Threat Frameworks}

We include MITRE ATLAS~\citep{mitreatlas2024}, the industry-standard adversarial ML threat framework modeled after ATT\&CK. ATLAS catalogs 66 adversarial ML techniques with explicit academic references, providing structured machine-readable data via its public GitHub repository (\texttt{atlas-data}). We programmatically extract technique IDs, names, and cited papers from this repository.

\subsubsection{Artifact Summary}

Table~\ref{tab:artifacts-summary} summarizes the nine Git-searchable artifacts included in our automated extraction pipeline, along with their selection criteria.

\begin{table}[H]
\centering
\small
\caption{Summary of artifacts and selection criteria}
\label{tab:artifacts-summary}
\begin{tabular}{llp{5cm}}
\toprule
\textbf{Category} & \textbf{Criterion} & \textbf{Artifacts} \\
\midrule
Tools & $\geq$1,000 GitHub stars & CleverHans, IBM ART, TextAttack, PyRIT, Foolbox \\
Benchmarks & Peer-reviewed publication & RobustBench, AutoAttack, HarmBench \\
Regulatory & Industry threat framework & MITRE ATLAS \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Automated Paper Extraction}

For each of the nine Git-searchable artifacts, we clone the repository and programmatically extract arXiv references using comprehensive regular expression patterns that capture:
\begin{itemize}[noitemsep]
    \item Direct arXiv URLs (\texttt{arxiv.org/abs/}, \texttt{arxiv.org/pdf/})
    \item BibTeX \texttt{eprint} fields
    \item Inline citations (e.g., ``arXiv:2401.12345'')
\end{itemize}

We scan all relevant file types: Python source (\texttt{.py}), documentation (\texttt{.md}, \texttt{.rst}, \texttt{.txt}), configuration (\texttt{.json}, \texttt{.yaml}, \texttt{.yml}), and bibliography (\texttt{.bib}) files. Each extracted arXiv ID is resolved to a canonical paper record via the arXiv API, retrieving title, authors, and publication date.

\textbf{Exclusions:} We exclude non-AML papers (e.g., foundational ML papers like Adam, ResNet, VGGNet) based on a predefined exclusion list validated against paper abstracts.

\subsection{Paper Selection Criteria}

From the full set of 277 AML papers extracted from the nine artifacts, we select papers for manual coding using a tiered approach:

\begin{enumerate}[noitemsep]
    \item \textbf{Multi-artifact papers (61 papers):} Papers cited by $\geq$2 artifacts provide the strongest evidence of cross-ecosystem adoption.
    
    \item \textbf{MITRE ATLAS-only papers (10 papers):} Papers cited exclusively by MITRE ATLAS represent regulatory adoption without tool/benchmark integration, capturing a distinct adoption pathway.
\end{enumerate}

This yields a final sample of \textbf{71 papers} for manual coding, balancing methodological rigor (multi-artifact validation) with coverage of regulatory adoption patterns.

\subsection{Paper Validation}

Each extracted reference is validated to ensure: (1) the paper introduces the technique (not merely applies or evaluates it), (2) the technique name corresponds to the artifact reference, and (3) the paper is publicly accessible via arXiv or DOI.

\subsection{Adoption Event Definition}

We define three observable adoption events, corresponding to our artifact categories:

\begin{table}[H]
\centering
\small
\caption{Adoption event definitions}
\begin{tabular}{p{3cm}p{5.5cm}}
\toprule
\textbf{Adoption Event} & \textbf{Operational Definition} \\
\midrule
Tool adoption & First Git commit referencing the paper \\
Benchmark adoption & Paper cited in benchmark repository \\
Regulatory adoption & Paper referenced in MITRE ATLAS \\
\bottomrule
\end{tabular}
\end{table}

Adoption is defined strictly as acknowledgment or integration, not mitigation or successful defense.

\subsection{Adoption Lag Measurement}

For each (paper, adoption event) pair, we compute:

\begin{equation}
\text{Adoption Lag} = \text{Date}_{\text{artifact}} - \text{Date}_{\text{publication}}
\end{equation}

Artifact dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Tools:} First Git commit timestamp referencing the paper (UTC)
    \item \textbf{Benchmarks:} First Git commit timestamp referencing the paper (UTC)
    \item \textbf{Regulatory:} First Git commit timestamp in MITRE ATLAS repository (UTC)
\end{itemize}

Publication dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Peer-reviewed:} Conference/journal publication date
    \item \textbf{arXiv-first:} First arXiv submission date
\end{itemize}

For papers cited by multiple artifacts, we record all adoption events and use the \textit{earliest} adoption date for computing first adoption lag. This captures when a paper first entered the practitioner ecosystem, regardless of which artifact adopted it first.

\subsection{Paper Coding Scheme}

Each paper is coded along three dimensions using a fixed codebook (Table~\ref{tab:codebook}). All coding is performed by the authors based on the paper text only. Threat model attributes are coded using author-stated assumptions; if unstated, code as ``Not specified.''

\begin{table}[H]
\centering
\small
\caption{Codebook for paper coding (9 variables)}
\label{tab:codebook}
\begin{tabular}{p{2.2cm}p{3.2cm}p{4.3cm}}
\toprule
\textbf{Variable} & \textbf{Values} & \textbf{Coding Rule} \\
\midrule
\multicolumn{3}{l}{\textit{Research Characteristics (G1--G6)}} \\
\midrule
G1: Type & Attack / Defense / Evaluation & Primary contribution \\
G2: Threat category & Evasion / Poisoning / Privacy / N/A & Attack category; N/A for defenses \\
G3: Domain & Vision / NLP / Malware / Audio / Tabular / LLM / Cross-domain & Primary evaluation domain \\
G4: Venue & ML / Security / Journal / arXiv-only & See venue list below \\
G5: Code available & Yes / No & Code link exists at time of coding \\
G6: Code timing & At-pub / Post-pub / Never & At-pub = within 1 month of paper \\
\midrule
\multicolumn{3}{l}{\textit{Threat Model (T1--T2) -- Attack papers only}} \\
\midrule
T1: Access level & White / Gray / Black & White = full model access \\
T2: Gradient required & Yes / No & Gradients used at any stage \\
\midrule
\multicolumn{3}{l}{\textit{Practical Evaluation (Q1)}} \\
\midrule
Q1: Real-world eval & Yes / Partial / No & Yes = production system tested \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Venue classification:} ML = NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, ACL, EMNLP, NAACL. Security = IEEE S\&P, ACM CCS, USENIX Security, NDSS, IEEE SaTML. Journal = TPAMI, TIFS, TDSC, etc. arXiv-only = no peer-reviewed venue.

\subsubsection{Decision Rules}

\begin{enumerate}[noitemsep]
    \item \textbf{G1:} If paper proposes both attack and defense, code based on which receives more experimental evaluation.
    
    \item \textbf{G2:} For defense papers, code ``N/A.'' Evasion = test-time input perturbation. Poisoning = training-time data manipulation. Privacy = membership inference, model extraction, training data extraction.
    
    \item \textbf{G3:} Code ``Cross-domain'' only if paper evaluates on 2+ distinct domains with separate experiments.
    
    \item \textbf{G4:} Use venue of first peer-reviewed publication. If workshop paper later becomes full paper, use full paper venue.
    
    \item \textbf{T1/T2:} For defense papers, leave blank (these apply to attacks only). White-box = gradients from target model. Gray-box = surrogate model gradients transferred to target. Black-box = query access only, no gradients.
    
    \item \textbf{Q1:} ``Yes'' = tested on production system (commercial API, deployed vehicle). ``Partial'' = realistic simulation or industry dataset. ``No'' = standard benchmarks only (CIFAR, ImageNet, etc.).
\end{enumerate}

\textbf{Coding procedure:} For each paper: (1) read abstract and introduction to identify G1 (type) and G2 (threat category); (2) locate threat model section to code T1 and T2 (attack papers only); (3) review experiments to code G3 and Q1; (4) check code availability for G5, G6; (5) verify venue for G4. Estimated time: 15--20 minutes per paper.

\subsection{Statistical Analysis}

We address each research question through pre-specified analyses:

\textbf{RQ1 (Adoption Lag):} We report distributions of first-adoption lag, stratified by artifact type (tool, benchmark, regulatory). We compare lags across publication eras (2014--2017, 2018--2021, 2022--2025) using Kruskal-Wallis tests with post-hoc Dunn tests.

\textbf{RQ2 (Domain Variation):} We compare adoption lags across domains using pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05 / \binom{n}{2}$ for $n$ domains). Primary hypothesis: LLM security papers show significantly shorter lags than computer vision papers.

\textbf{RQ3 (Acceleration Factors):} We fit a Cox proportional hazards model predicting time-to-first-adoption, with covariates: publication year (continuous), domain (categorical), venue type (ML vs. Security), code availability (binary), and threat model (white-box vs. gray-box vs. black-box). Hazard ratios $>$1 indicate faster adoption. Model diagnostics include proportional hazards tests and residual analysis.

\subsection{Reliability, Reproducibility, and Limitations}

\textbf{Reliability:} We employ intra-rater reliability (15\% of papers re-coded after two weeks, targeting $\kappa \geq 0.80$) and inter-rater reliability for papers coded by multiple authors. Adoption dates are verified via \texttt{git log} and Wayback Machine archives.

\textbf{Data release:} All extracted metadata, coding decisions, and artifact URLs will be released as a structured CSV dataset with accompanying codebook.

\textbf{Limitations:} (1) By construction, our sample excludes papers never adopted---we measure adoption timelines, not adoption rates. (2) Our nine Git-searchable artifacts may miss proprietary implementations; adoption timelines represent lower bounds. (3) Git commit dates represent code-level adoption, which may lag behind internal awareness. (4) Observational design precludes causal claims. (5) We focus on English-language publications and arXiv-indexed papers.

%==============================================================================
\section{Artifact Timeline}
\label{sec:artifact-timeline}
%==============================================================================

Table~\ref{tab:artifact-timeline} documents the nine artifacts included in our automated extraction, with first public release dates verified from GitHub and official announcements. These dates establish the timeline against which we measure research adoption.

\begin{table}[H]
\centering
\small
\caption{Artifact release timeline}
\label{tab:artifact-timeline}
\begin{tabular}{llll}
\toprule
\textbf{Category} & \textbf{Artifact} & \textbf{First Release} & \textbf{Stars/Venue} \\
\midrule
\multirow{5}{*}{Tools} & CleverHans & Oct 2016 & 6,401 stars \\
& Foolbox & Jul 2017 & 2,936 stars \\
& IBM ART & Jul 2018 & 5,789 stars \\
& TextAttack & May 2020 & 3,348 stars \\
& PyRIT & Feb 2024 & 3,343 stars \\
\midrule
\multirow{3}{*}{Benchmarks} & AutoAttack & Mar 2020 & ICML 2020 \\
& RobustBench & Oct 2020 & NeurIPS 2021 \\
& HarmBench & Feb 2024 & ICML 2024 \\
\midrule
Regulatory & MITRE ATLAS & Jun 2021 & Industry framework \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\todo{Complete after paper coding. This section will report:
\begin{itemize}
    \item Sample construction summary (extraction counts by artifact source)
    \item RQ1: Adoption lag distributions (overall, by artifact type, by publication era)
    \item RQ2: Domain variation analysis (lag comparisons across Vision, NLP, LLM, Malware, Audio)
    \item RQ3: Cox regression results (predictors of adoption speed)
\end{itemize}
}

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\todo{Complete after results analysis. This section will include:
\begin{itemize}
    \item Summary of key findings (adoption lag compression, LLM paradigm shift, predictors)
    \item Why some research is never adopted (naming, tooling, threat model barriers)
    \item Implications for researchers and practitioners
    \item Limitations
\end{itemize}
}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

\todo{Complete after results. Will summarize:
\begin{itemize}
    \item Key quantitative findings on adoption lag
    \item Domain-specific patterns
    \item Acceleration factors
    \item Contribution of coding framework and dataset
\end{itemize}
}

%==============================================================================
\appendix
\section{Artifact Source URLs}
\label{sec:appendix}
%==============================================================================

\begin{itemize}[noitemsep]
    \item \textbf{IBM ART:} \url{https://github.com/Trusted-AI/adversarial-robustness-toolbox}
    \item \textbf{CleverHans:} \url{https://github.com/cleverhans-lab/cleverhans}
    \item \textbf{Foolbox:} \url{https://github.com/bethgelab/foolbox}
    \item \textbf{TextAttack:} \url{https://github.com/QData/TextAttack}
    \item \textbf{PyRIT:} \url{https://github.com/Azure/PyRIT}
    \item \textbf{RobustBench:} \url{https://robustbench.github.io/}
    \item \textbf{HarmBench:} \url{https://github.com/centerforaisafety/HarmBench}
    \item \textbf{MITRE ATLAS:} \url{https://atlas.mitre.org/techniques}
    \item \textbf{OWASP LLM Top 10:} \url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}
\end{itemize}

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}