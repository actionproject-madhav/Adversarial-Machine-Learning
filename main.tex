\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{float}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{caption}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Compact lists
\setlist{nosep,leftmargin=*}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\placeholder}[1]{\textcolor{blue}{[PLACEHOLDER: #1]}}

\title{Tracing the Adoption of Adversarial Machine Learning Research into Industry Practice (2014--2025)}

\author{
    [Author Names] \\
    [Affiliations] \\
    \texttt{[emails]}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Despite a decade of adversarial machine learning (AML) research producing over 66 catalogued attack techniques and numerous defense mechanisms, industry adoption remains limited---surveys indicate only 5\% of practitioners have experienced AI-specific attacks, yet 86\% express security concerns. This systematic review measures the temporal lag between publication of landmark AML research and evidence of industry adoption across tool integration, commercial deployment, regulatory citation, and production use. We analyze approximately 120 papers published between 2014--2025, spanning foundational attacks (FGSM, C\&W, PGD), privacy threats (membership inference, model extraction), physical-world attacks, and LLM-specific vulnerabilities (jailbreaking, prompt injection). Our findings reveal domain-dependent adoption lags ranging from 4--6 years for malware detection to 10+ years for financial systems, with LLM security exhibiting compressed 1--2 year cycles driven by direct user interaction and regulatory pressure. We identify key acceleration factors including standardized evaluation (AutoAttack, RobustBench, HarmBench), regulatory mandates (EU AI Act Article 15, NIST AI RMF), and significant commercial investment in AI security. Our coding framework and adoption evidence database provide a foundation for future research-practice alignment studies.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Machine learning systems now influence decisions affecting millions daily---from facial recognition at border crossings~\citep{grother2019frvt} to fraud detection in financial services~\citep{dalpozzolo2015fraud} and content moderation on social platforms~\citep{gorwa2020algorithmic}. This widespread deployment has intensified scrutiny of adversarial vulnerabilities: inputs or interactions crafted to cause models to behave in unintended ways~\citep{biggio2018wild}.

The field of adversarial machine learning emerged with Szegedy et al.'s demonstration that imperceptible perturbations could fool state-of-the-art classifiers~\citep{szegedy2014intriguing}, followed by Goodfellow et al.'s Fast Gradient Sign Method establishing the dominant attack paradigm~\citep{goodfellow2015explaining}. Over the subsequent decade, researchers catalogued 66 attack techniques spanning evasion, poisoning, and privacy threats~\citep{mitreatlas2024}. MITRE ATLAS now documents 33 real-world case studies, and regulatory frameworks including the EU AI Act mandate adversarial robustness testing for high-risk systems~\citep{euaiact2024}.

Yet industry surveys reveal a persistent gap. Kumar et al.~\citep{kumar2020adversarial} found practitioners ``not equipped with tactical and strategic tools'' for ML-specific attacks. Grosse et al.~\citep{grosse2023mlsecurity} reported only 5\% of AI practitioners had experienced AI-specific attacks, despite 86\% expressing concern. Mink et al.~\citep{mink2023security} identified organizational barriers including lack of institutional motivation, inability to assess AML risk, and structures discouraging implementation. Apruzzese et al.~\citep{apruzzese2023real} crystallized these concerns, arguing that ``real attackers don't compute gradients''---academic threat models assume capabilities rarely available in practice.

This gap matters because real-world incidents demonstrate adversarial threats are not merely theoretical. In November 2025, Anthropic disclosed the first documented large-scale AI-orchestrated cyber campaign, with Chinese state-sponsored actors using Claude Code to execute 80--90\% of operational tasks autonomously~\citep{anthropic2025claude}. Tesla Autopilot was fooled by adversarial tape modifications~\citep{keenlab2019tesla,mcafee2020tesla}. Training data extraction from ChatGPT recovered megabytes of verbatim training data for under \$200~\citep{nasr2023extracting}. The OWASP Top 10 for LLM Applications lists prompt injection as the \#1 vulnerability across all versions~\citep{owasp2024llm}.

\subsection{Research Questions}

We address three questions about the research-to-practice transfer in adversarial ML:

\begin{enumerate}
    \item \textbf{RQ1 (Adoption Lag):} What is the typical time lag between publication of landmark AML research and evidence of industry adoption, measured through tool integration, commercial reference, regulatory citation, and production deployment?
    
    \item \textbf{RQ2 (Domain Variation):} How does adoption speed vary across application domains (computer vision, NLP, malware detection, autonomous systems, LLMs), and what factors explain these differences?
    
    \item \textbf{RQ3 (Acceleration Factors):} What mechanisms---regulatory frameworks, standardized benchmarks, industry consortiums, commercial tools---have accelerated adoption, particularly for foundation model security post-2022?
\end{enumerate}

\subsection{Contributions}

This review makes four contributions:

\begin{enumerate}
    \item \textbf{Quantified adoption timelines:} We provide the first systematic measurement of research-to-industry lag across 120 landmark papers, documenting specific dates for tool integration, commercial reference, and regulatory citation.
    
    \item \textbf{Multi-indicator adoption framework:} We introduce a coding framework tracking six adoption indicators beyond citation counts, enabling comparison across domains and time periods.
    
    \item \textbf{Domain-stratified analysis:} We document domain-specific patterns ranging from 4--6 year lags (malware) to compressed 1--2 year cycles (LLMs), identifying factors driving variation.
    
    \item \textbf{Acceleration mechanism analysis:} We analyze how regulatory mandates, standardized evaluation, and market dynamics have compressed adoption timelines post-2020.
\end{enumerate}

%==============================================================================
\section{Background}
\label{sec:background}
%==============================================================================

\subsection{The Emergence of Adversarial Vulnerabilities}

Modern adversarial ML research began with Szegedy et al.'s December 2013 demonstration that small perturbations could cause misclassification with high confidence~\citep{szegedy2014intriguing}. This work, which received ICLR's 2024 Test of Time Award, revealed that adversarial examples transfer across independently trained models---a finding with profound security implications.

Goodfellow et al.~\citep{goodfellow2015explaining} attributed these vulnerabilities to linear behavior in high-dimensional spaces and introduced the Fast Gradient Sign Method (FGSM), appearing on arXiv December 20, 2014 and published at ICLR 2015. FGSM established gradient-based perturbation as the dominant paradigm and introduced adversarial training as a defense. These foundational works established conventions that shaped subsequent research: white-box access assumptions, $L_p$ perturbation constraints, and optimization-based attack formulations.

Subsequent work refined attack formulations. The Carlini \& Wagner attack~\citep{carlini2017towards} (IEEE S\&P 2017) achieved state-of-the-art success across multiple norms. Madry et al.'s PGD attack~\citep{madry2018towards} (ICLR 2018) established the robust optimization framework:
\begin{equation}
\min_\theta \mathbb{E}_{(x,y)\sim D} \left[ \max_{\|\delta\| \leq \epsilon} L(f_\theta(x + \delta), y) \right]
\end{equation}
This min-max formulation remains the gold standard---10 of the top-10 models on RobustBench use PGD-based adversarial training~\citep{croce2021robustbench}.

\subsection{Expanding Threat Landscape}

Research expanded beyond test-time evasion to encompass the full ML pipeline. \textbf{Privacy attacks} demonstrated that models leak information: Shokri et al.~\citep{shokri2017membership} introduced membership inference at IEEE S\&P 2017; Tram\`{e}r et al.~\citep{tramer2016stealing} demonstrated model extraction from commercial APIs at USENIX Security 2016; Carlini et al.~\citep{carlini2021extracting} extracted verbatim training data from GPT-2 at USENIX Security 2021.

\textbf{Integrity attacks} compromise models during training. BadNets~\citep{gu2017badnets} (arXiv August 2017) demonstrated backdoor injection through poisoned training data. Subsequent work extended backdoors to federated learning~\citep{bagdasaryan2020backdoor} and self-supervised learning~\citep{jia2021badencoder}.

\textbf{Physical-world attacks} demonstrated that adversarial examples survive real-world conditions. Eykholt et al.~\citep{eykholt2018robust} (CVPR 2018) created adversarial stop signs robust to viewing angles and distances. DolphinAttack~\citep{zhang2017dolphinattack} (ACM CCS 2017, Best Paper) compromised voice assistants via ultrasonic commands inaudible to humans.

\subsection{The LLM Security Paradigm Shift}

Large language models introduced qualitatively different adversarial challenges. Unlike traditional attacks requiring gradient access and imperceptible perturbations, LLM attacks exploit semantic properties through natural language.

\textbf{Prompt injection} was first documented by Simon Willison in September 2022 and formalized by Perez \& Ribeiro~\citep{perez2022ignore} at the NeurIPS 2022 Workshop on ML Safety. Greshake et al.~\citep{greshake2023indirect} demonstrated indirect prompt injection against retrieval-augmented systems (arXiv February 2023, ACM AISec November 2023), showing attackers can embed malicious instructions in external content.

\textbf{Automated jailbreaking} emerged with Zou et al.'s GCG attack~\citep{zou2023universal} (arXiv July 2023, NeurIPS 2023 Spotlight), which optimizes adversarial suffixes achieving the first automated jailbreaks against aligned LLMs including ChatGPT, Bard, and Claude. Subsequent work developed more efficient black-box methods: PAIR~\citep{chao2024jailbreaking} achieves jailbreaks in approximately 20 queries; TAP~\citep{mehrotra2024tree} uses tree-of-thought reasoning for further efficiency.

\textbf{LLM defenses} include alignment techniques (RLHF~\citep{ouyang2022instructgpt}, Constitutional AI~\citep{bai2022constitutional}), specialized guardrails (LlamaGuard~\citep{inan2023llamaguard} released December 2023, NeMo Guardrails open-sourced April 2023), and input/output filtering. However, these defenses exhibit brittleness---the HackAPrompt competition~\citep{schulhoff2023hackaprompt} saw all 44 defenses eventually bypassed across 137,000+ adversarial interactions.

\subsection{The Theory-Practice Gap}

Apruzzese et al.~\citep{apruzzese2023real} synthesized concerns about research-practice disconnect through real-world case studies at IEEE SaTML 2023. Their core observation: academic threat models assume attackers possess white-box access, unlimited queries, and gradient computation capabilities rarely available in practice. Real incidents typically involve simpler tactics---basic input manipulation, system-level exploitation, social engineering.

Industry surveys quantify this gap. Kumar et al.~\citep{kumar2020adversarial} interviewed 28 organizations at IEEE S\&P Workshops 2020, finding widespread uncertainty about assessing adversarial risks. Grosse et al.~\citep{grosse2023mlsecurity} surveyed 139 practitioners (IEEE TIFS 2023), finding only 5\% had experienced AI-specific attacks despite 86\% expressing concern. Mink et al.~\citep{mink2023security} conducted 21 interviews at USENIX Security 2023, identifying three barriers: lack of institutional motivation, inability to assess AML risk, and organizational structures discouraging implementation.

However, prior work characterized this gap qualitatively. Our contribution is to measure adoption timelines quantitatively, identifying specific lag durations and acceleration factors. This work does not assess the severity of adversarial threats, but quantitatively measures when research techniques enter practitioner ecosystems.

%==============================================================================
\section{Methodology}
\label{sec:methodology}
%==============================================================================

\subsection{Study Design Overview}

This study measures the temporal lag between adversarial machine learning (AML) research and demonstrable industry adoption. Rather than starting from academic publications and speculating about downstream impact, we adopt an \textit{artifact-anchored backward traceability} methodology. We begin with concrete industry artifacts---open-source security tools, standardized benchmarks, regulatory frameworks, and vendor security documentation---and trace each adopted technique back to the academic paper that originally introduced it.

This design ensures that every paper in our dataset has verifiable evidence of adoption by construction, enabling precise measurement of research-to-practice timelines while avoiding subjective judgments about a paper's ``importance'' or speculative claims about industry relevance.

\subsection{Artifact Universe Definition}

Before extracting any papers, we define and freeze the set of industry artifacts examined in this study. Artifacts were selected according to objective inclusion criteria to minimize selection bias.

\subsubsection{Open-Source Security Tools}

We include open-source AML tools satisfying at least one of the following criteria:

\begin{itemize}[noitemsep]
    \item $\geq$1,000 GitHub stars, or
    \item $\geq$100 academic citations (via Semantic Scholar)
\end{itemize}

Based on these criteria, we analyze five tools:

\begin{enumerate}[noitemsep]
    \item \textbf{IBM Adversarial Robustness Toolbox (ART)}~\citep{nicolae2018art}: 4,800+ stars, Linux Foundation AI project (graduated February 2022)
    
    \item \textbf{CleverHans}~\citep{papernot2016cleverhans}: 6,100+ stars, first major AML library (October 2016)
    
    \item \textbf{Foolbox}: 2,700+ stars, extensive attack implementations
    
    \item \textbf{TextAttack}: 2,900+ stars, NLP-specific attacks
    
    \item \textbf{Microsoft PyRIT}~\citep{pyrit2024}: LLM red-teaming toolkit (February 2024)
\end{enumerate}

For each tool, we extract: (a) technique name, (b) source paper citation from documentation/docstrings, (c) date of first Git commit implementing the technique (via \texttt{git log --follow}), and (d) release version containing the technique.

\subsubsection{Standardized Benchmarks}

We include benchmarks that evaluate AML attacks or defenses as named techniques and are used in peer-reviewed research or cited by industry:

\begin{enumerate}[noitemsep]
    \item \textbf{RobustBench}~\citep{croce2021robustbench}: Leaderboard tracking 120+ defense models with paper citations. We record when each defense was added to the leaderboard (via GitHub commit history).
    
    \item \textbf{AutoAttack}~\citep{croce2020autoattack}: Ensemble evaluation method with component attacks (APGD, FAB, Square Attack). We extract source papers for each component.
    
    \item \textbf{HarmBench}~\citep{mazeika2024harmbench}: Standardized jailbreak evaluation across 33 LLMs, evaluating 18 red-teaming methods with source paper citations.
\end{enumerate}

Only techniques explicitly evaluated and attributed to specific academic papers are included.

\subsubsection{Regulatory and Threat Frameworks}

We analyze the following frameworks:

\begin{enumerate}[noitemsep]
    \item \textbf{MITRE ATLAS}~\citep{mitreatlas2024}: 66 adversarial ML techniques with academic references. We extract technique ID, name, and cited papers. Technique page creation dates determined via Wayback Machine archives.
    
    \item \textbf{NIST AI RMF Adversarial ML Taxonomy} (AI 100-2e2025): Technique categorization with literature references. Publication date: January 2023 (v1.0), updated July 2024 (GenAI Profile).
    
    \item \textbf{OWASP Top 10 for LLM Applications}~\citep{owasp2024llm}: Vulnerability descriptions citing foundational research. Version 1.0 (August 2023) and Version 2.0 (November 2024) analyzed.
\end{enumerate}

We extract only techniques explicitly linked to academic references. Descriptive categories without citations are excluded.

\subsubsection{Vendor Security Documentation}

We analyze publicly available security documentation from major cloud and AI providers:

\begin{itemize}[noitemsep]
    \item AWS SageMaker security documentation
    \item Microsoft Azure AI security whitepapers and Responsible AI documentation  
    \item Google Cloud Vertex AI security guides
    \item OpenAI system cards and security disclosures
    \item Anthropic system cards and security disclosures
\end{itemize}

Only documents that explicitly reference AML techniques or academic work are included. For each reference, we record: (a) technique name, (b) document URL, (c) document publication date, and (d) cited academic paper (if any).

\subsection{Artifact-to-Paper Extraction}

For each artifact, we extract tuples of the form:

\begin{equation}
(\text{artifact\_name}, \text{artifact\_type}, \text{technique\_name}, \text{cited\_reference})
\end{equation}

Extraction follows these rules:

\begin{enumerate}[noitemsep]
    \item The technique must be named explicitly (e.g., ``FGSM,'' ``Carlini--Wagner,'' ``GCG'').
    
    \item The artifact must either cite a paper directly or attribute the technique to identifiable authors.
    
    \item Techniques lacking an academic anchor are excluded.
\end{enumerate}

This process yields a set of candidate academic papers that have demonstrably entered practitioner workflows.

\subsection{Paper Canonicalization and Validation}

Each extracted reference is resolved to a canonical paper record:

\begin{itemize}[noitemsep]
    \item \textbf{Identifier:} DOI where available, otherwise arXiv identifier
    
    \item \textbf{Publication date:} First arXiv submission date if it precedes peer-reviewed publication, otherwise conference/journal publication date. This reflects the earliest point at which the technique could plausibly be adopted.
\end{itemize}

Each paper is manually validated to ensure:

\begin{enumerate}[noitemsep]
    \item The paper introduces the technique (not merely applies or evaluates it)
    \item The technique name corresponds to the extracted artifact reference
    \item The paper is publicly accessible
\end{enumerate}

Papers failing any validation criterion are excluded.

\subsection{Adoption Event Definition}

We define four observable adoption events, each treated independently:

\begin{table}[H]
\centering
\small
\caption{Adoption event definitions}
\begin{tabular}{p{3cm}p{5.5cm}}
\toprule
\textbf{Adoption Event} & \textbf{Operational Definition} \\
\midrule
Tool adoption & First Git commit implementing the technique \\
Benchmark adoption & Technique evaluated as a named method \\
Regulatory adoption & Technique referenced in regulatory framework \\
Vendor acknowledgment & Technique referenced in vendor security documentation \\
\bottomrule
\end{tabular}
\end{table}

Adoption is defined strictly as acknowledgment or integration, not mitigation or successful defense.

\subsection{Adoption Lag Measurement}

For each (paper, adoption event) pair, we compute:

\begin{equation}
\text{Adoption Lag} = \text{Date}_{\text{artifact}} - \text{Date}_{\text{publication}}
\end{equation}

Artifact dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Tools:} Git commit timestamp (UTC)
    \item \textbf{Benchmarks:} Leaderboard inclusion date or paper publication date
    \item \textbf{Regulatory:} Framework publication or technique page creation date (Wayback Machine)
    \item \textbf{Vendor:} Document publication date (from document metadata or press release)
\end{itemize}

Publication dates are defined as:
\begin{itemize}[noitemsep]
    \item \textbf{Peer-reviewed:} Conference/journal publication date
    \item \textbf{arXiv-first:} First arXiv submission date
\end{itemize}

For papers appearing in multiple artifacts, we report: (a) \textit{first adoption lag} (minimum across all events), (b) \textit{median adoption lag}, and (c) \textit{regulatory adoption lag} (time to MITRE/NIST/OWASP inclusion, if any).

\subsection{Paper Coding Scheme}

Each paper is coded along three dimensions using a fixed codebook. All coding is performed by the authors based on the paper text. Threat model attributes are coded using author-stated assumptions only.

\subsubsection{Research Characteristics}

\begin{itemize}[noitemsep]
    \item \textbf{Technique type:} Attack / Defense / Evaluation Method
    \item \textbf{Threat category:} Evasion / Poisoning / Privacy / Availability / Multiple
    \item \textbf{Domain:} Vision / NLP / Malware / Audio / Tabular / LLM / Cross-domain
    \item \textbf{Venue type:} ML conference (NeurIPS, ICML, ICLR) / Security conference (S\&P, CCS, USENIX, NDSS) / Journal / arXiv-only
    \item \textbf{Code availability:} Yes / No
    \item \textbf{Code release timing:} At publication / Post-publication / Never
\end{itemize}

\subsubsection{Threat Model Assumptions}

\begin{itemize}[noitemsep]
    \item \textbf{Access level:} White-box (full model access) / Gray-box (partial knowledge or surrogate) / Black-box (query-only)
    \item \textbf{Gradient requirement:} Yes / No. If gradients are required at any stage, coded as Yes.
    \item \textbf{Query budget:} Unlimited / $>$1000 / 100--1000 / $<$100 / Zero (transfer-only)
    \item \textbf{Perturbation constraint:} $L_\infty$ / $L_2$ / $L_0$ / Semantic / Unconstrained
\end{itemize}

\subsubsection{Practical Evaluation Indicators}

\begin{itemize}[noitemsep]
    \item \textbf{Real-world evaluation:} Yes (deployed/commercial system) / Partial (realistic simulation) / No (benchmark only)
    \item \textbf{Computational cost reported:} Yes (explicit FLOPs, queries, or runtime) / Qualitative / No
    \item \textbf{Defense-aware evaluation:} Yes (adaptive evaluation) / Partial / No / N/A (for defenses)
\end{itemize}

\subsection{Statistical Analysis Plan}

We address each research question through pre-specified analyses:

\textbf{RQ1 (Adoption Lag):} We report distributions of first-adoption lag and median-adoption lag, stratified by artifact type (tool, benchmark, regulatory, vendor). We compare lags across publication eras (2014--2017, 2018--2021, 2022--2025) using Kruskal-Wallis tests with post-hoc Dunn tests.

\textbf{RQ2 (Domain Variation):} We compare adoption lags across domains using pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05 / \binom{n}{2}$ for $n$ domains). Primary hypothesis: LLM security papers show significantly shorter lags than computer vision papers.

\textbf{RQ3 (Acceleration Factors):} We fit a Cox proportional hazards model predicting time-to-first-adoption, with covariates: publication year (continuous), domain (categorical), venue type (ML vs. Security), code availability (binary), and threat model (white-box vs. gray-box vs. black-box). Hazard ratios $>$1 indicate faster adoption. Model diagnostics include proportional hazards tests and residual analysis.

\subsection{Reliability and Reproducibility}

To assess coding stability, 15\% of papers (randomly selected) are re-coded by the same coder after a two-week interval. Agreement is measured using Cohen's $\kappa$ for categorical variables and mean absolute deviation for adoption lag measurements. We report inter-rater reliability for papers coded by multiple authors.

All extracted metadata, coding decisions, and artifact links will be released in a structured dataset to enable replication and extension.

\subsection{Methodological Limitations}

This methodology has several limitations:

\begin{enumerate}[noitemsep]
    \item \textbf{Selection bias toward adopted work:} By construction, our sample excludes papers that were never adopted. We cannot estimate the ``adoption rate'' (fraction of papers eventually adopted), only the adoption timeline for papers that were adopted.
    
    \item \textbf{Tool coverage:} We analyze five major open-source tools, but techniques may be implemented in proprietary systems we cannot observe. Our timelines represent lower bounds on true first-adoption dates.
    
    \item \textbf{Artifact dating precision:} Git commit dates are reliable, but vendor documentation dates may reflect updates rather than first publication. We use Wayback Machine archives where available to verify initial publication dates.
    
    \item \textbf{Causal inference:} Observing that black-box attacks have shorter adoption lags does not prove that threat model choice \textit{causes} faster adoption; both may be driven by domain characteristics or other confounds.
    
    \item \textbf{Geographic and linguistic scope:} We focus on English-language publications and Western regulatory frameworks (US, EU). Adoption patterns in other regions may differ.
\end{enumerate}

%==============================================================================
\section{Industry Tools and Framework Timeline}
\label{sec:tools}
%==============================================================================

The development of industry tools provides a concrete timeline for measuring research-to-practice transfer. Table~\ref{tab:tools} documents major tools with release dates verified from GitHub releases, arXiv papers, and official announcements.

\begin{table}[H]
\centering
\small
\caption{Major AML tools and release timeline}
\label{tab:tools}
\begin{tabular}{lll}
\toprule
\textbf{Tool} & \textbf{Release} & \textbf{Key Milestone} \\
\midrule
CleverHans & Oct 2016 & arXiv:1610.00768 \\
Foolbox & Jul 2017 & ICML 2017 Workshop \\
IBM ART & Jul 2018 & LF AI Feb 2022 \\
TextAttack & May 2020 & EMNLP 2020 Demo \\
Counterfit & May 2021 & Microsoft Security \\
AutoAttack & Mar 2020 & ICML 2020 \\
RobustBench & Oct 2020 & NeurIPS 2021 D\&B \\
PyRIT & Feb 2024 & LLM red-teaming \\
HarmBench & Feb 2024 & ICML 2024 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{CleverHans}~\citep{papernot2016cleverhans} was created by Nicolas Papernot and Ian Goodfellow, implementing FGSM within two years of its publication. \textbf{IBM's Adversarial Robustness Toolbox}~\citep{nicolae2018art} released its first paper July 2018, was donated to Linux Foundation AI in July 2020, and graduated to full project status February 2022. \textbf{AutoAttack}~\citep{croce2020autoattack} became the de facto evaluation standard after demonstrating that reported robust accuracies dropped by $>$10\% for 13 published models when evaluated rigorously. \textbf{RobustBench}~\citep{croce2021robustbench} now tracks 120+ models with standardized evaluation.

The LLM era introduced specialized tools. \textbf{Microsoft PyRIT}~\citep{pyrit2024} (February 2024) focuses on LLM red-teaming. \textbf{HarmBench}~\citep{mazeika2024harmbench} (February 2024) provides standardized jailbreak evaluation across 33 LLMs. Commercial tools followed: Azure Prompt Shields entered preview March 2024 and GA September 2024; Google Model Armor launched February 2025.

Regulatory frameworks codified these practices. \textbf{MITRE ATLAS}~\citep{mitreatlas2024} launched June 2021, now cataloguing 66 techniques and 33 case studies. \textbf{NIST AI RMF 1.0}~\citep{nist2023airisk} was published January 26, 2023, with the Generative AI Profile following July 26, 2024. The \textbf{EU AI Act}~\citep{euaiact2024} entered force August 1, 2024, with high-risk adversarial testing requirements effective August 2, 2026. \textbf{OWASP Top 10 for LLM Applications}~\citep{owasp2024llm} released v1.0 August 2023 and v2.0 November 2024.

%==============================================================================
\section{Real-World Incidents}
\label{sec:incidents}
%==============================================================================

Documented incidents demonstrate that adversarial threats materialize in practice, though often through simpler mechanisms than academic threat models assume. We highlight three incidents with formal documentation in MITRE ATLAS, OWASP, or vendor disclosures:

\textbf{Training data extraction from ChatGPT:} Nasr, Carlini et al.~\citep{nasr2023extracting} demonstrated that prompting ChatGPT to ``repeat the word `poem' forever'' extracted several megabytes of training data for approximately \$200, with $>$5\% being verbatim 50-token copies. This attack exploited model memorization rather than adversarial optimization, requiring no gradient access or model knowledge.

\textbf{Prompt injection vulnerabilities:} CVE-2024-5184 (EmailGPT, CVSS 9.1) and CVE-2025-68664 (LangChain ``LangGrinch,'' CVSS 9.3) document production prompt injection vulnerabilities~\citep{owasp2024llm}. The Microsoft Bing Chat incident (February 2023) saw system prompt extraction within 24 hours of launch through simple conversational probing.

\textbf{Agentic AI campaign:} In November 2025, Anthropic disclosed the first documented large-scale AI-orchestrated cyber espionage operation~\citep{anthropic2025claude}. Chinese state-sponsored group GTG-1002 used Claude Code to execute 80--90\% of operational tasks autonomously against approximately 30 targets. This incident exploited agentic capabilities and tool integration rather than traditional adversarial perturbations.

These incidents share a pattern: attackers exploit system integration points, deployment assumptions, and operational gaps rather than computing optimal $L_p$-bounded perturbations. This validates Apruzzese et al.'s critique while demonstrating that adversarial vulnerabilities do manifest in practice.

%==============================================================================
\section{Results}
\label{sec:results}
%==============================================================================

\subsection{Sample Construction}

Applying the artifact-anchored methodology yielded the following extraction results:

\placeholder{
\textbf{Tool extraction:}
\begin{itemize}[noitemsep]
    \item IBM ART: X attacks, Y defenses $\rightarrow$ Z unique papers
    \item CleverHans: X techniques $\rightarrow$ Y unique papers
    \item Foolbox: X attacks $\rightarrow$ Y unique papers
    \item TextAttack: X NLP attacks $\rightarrow$ Y unique papers
    \item PyRIT: X LLM strategies $\rightarrow$ Y unique papers
\end{itemize}

\textbf{Benchmark extraction:}
\begin{itemize}[noitemsep]
    \item RobustBench: X defense papers (leaderboard entries)
    \item AutoAttack: Y component attacks $\rightarrow$ Z papers
    \item HarmBench: X attack methods $\rightarrow$ Y papers
\end{itemize}

\textbf{Regulatory extraction:}
\begin{itemize}[noitemsep]
    \item MITRE ATLAS: X techniques $\rightarrow$ Y unique papers
    \item NIST AI RMF: X technique categories $\rightarrow$ Y papers
    \item OWASP Top 10: X vulnerabilities $\rightarrow$ Y papers
\end{itemize}

\textbf{Vendor extraction:}
\begin{itemize}[noitemsep]
    \item AWS/Azure/GCP: X technique references $\rightarrow$ Y papers
    \item OpenAI/Anthropic: X technique references $\rightarrow$ Y papers
\end{itemize}

\textbf{Final sample:} After deduplication, N unique papers spanning 2014--2025. Table~\ref{tab:sample_breakdown} shows distribution by era and domain.
}

\subsection{RQ1: Adoption Lag Distributions}

\placeholder{
\textbf{Overall adoption timelines (N=X papers):}
\begin{itemize}[noitemsep]
    \item Median first-adoption lag: X months (IQR: Y--Z months)
    \item Mean first-adoption lag: X months (SD: Y months)
    \item Range: [minimum X months, maximum Y months]
\end{itemize}

\textbf{By artifact type:}
\begin{itemize}[noitemsep]
    \item Tool integration: Median X months (IQR: Y--Z, n=N)
    \item Benchmark inclusion: Median X months (IQR: Y--Z, n=N)
    \item Regulatory citation: Median X months (IQR: Y--Z, n=N)
    \item Vendor acknowledgment: Median X months (IQR: Y--Z, n=N)
\end{itemize}

\textbf{By publication era:}
\begin{itemize}[noitemsep]
    \item 2014--2017 papers: Median X months (IQR: Y--Z, n=N)
    \item 2018--2021 papers: Median X months (IQR: Y--Z, n=N)
    \item 2022--2025 papers: Median X months (IQR: Y--Z, n=N)
    \item Kruskal-Wallis test: $H$ = X.XX, $p$ = Y.YYY
    \item Post-hoc Dunn tests: [report pairwise comparisons]
\end{itemize}

Figure~\ref{fig:lag_distribution} shows the distribution of first-adoption lags. Figure~\ref{fig:lag_by_era} shows boxplots by publication era.
}

\subsection{RQ2: Domain Variation}

\placeholder{
\textbf{Adoption lag by domain:}
\begin{itemize}[noitemsep]
    \item Computer Vision: Median X months (IQR: Y--Z, n=N)
    \item NLP (pre-LLM): Median X months (IQR: Y--Z, n=N)
    \item Malware Detection: Median X months (IQR: Y--Z, n=N)
    \item LLM Security: Median X months (IQR: Y--Z, n=N)
    \item Audio: Median X months (IQR: Y--Z, n=N)
\end{itemize}

\textbf{Statistical comparison:} Pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05 / 10 = 0.005$ for 5 domains). 

\textbf{Key finding:} LLM security papers show significantly shorter adoption lags than computer vision papers ($U$ = X.XX, $p$ $<$ 0.001), with median lag of X months vs. Y months. This represents an X-fold acceleration.

Table~\ref{tab:first_adoption_by_domain} shows the first adoption artifact type by domain, revealing that LLM techniques predominantly enter via vendor acknowledgment, while vision techniques enter via tools.
}

\begin{table}[H]
\centering
\small
\caption{First adoption artifact by domain}
\label{tab:first_adoption_by_domain}
\placeholder{
\begin{tabular}{lrrrr}
\toprule
\textbf{Domain} & \textbf{Tool} & \textbf{Benchmark} & \textbf{Regulatory} & \textbf{Vendor} \\
\midrule
Vision & X\% & Y\% & Z\% & W\% \\
NLP (pre-LLM) & X\% & Y\% & Z\% & W\% \\
Malware & X\% & Y\% & Z\% & W\% \\
LLM & X\% & Y\% & Z\% & W\% \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{RQ3: Predictors of Adoption Speed}

\placeholder{
We fit a Cox proportional hazards model to identify factors independently predicting time-to-first-adoption. The model includes N papers with complete covariate data. Proportional hazards assumption validated via Schoenfeld residuals ($p$ = X.XX).

\begin{table}[H]
\centering
\small
\caption{Cox proportional hazards model: Predictors of time to first adoption}
\label{tab:cox_model}
\begin{tabular}{lrrr}
\toprule
\textbf{Covariate} & \textbf{HR} & \textbf{95\% CI} & \textbf{p-value} \\
\midrule
Publication year (per year) & X.XX & [X.XX, X.XX] & $<$0.001 \\
Code released (vs. not) & X.XX & [X.XX, X.XX] & X.XXX \\
Black-box (vs. white-box) & X.XX & [X.XX, X.XX] & X.XXX \\
Gray-box (vs. white-box) & X.XX & [X.XX, X.XX] & X.XXX \\
LLM domain (vs. vision) & X.XX & [X.XX, X.XX] & X.XXX \\
Security venue (vs. ML) & X.XX & [X.XX, X.XX] & X.XXX \\
Attack (vs. defense) & X.XX & [X.XX, X.XX] & X.XXX \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} Hazard ratio (HR) $>$1 indicates faster adoption. Each additional publication year is associated with X\% faster adoption (HR = X.XX, 95\% CI [X.XX, X.XX]), consistent with field maturation and improved infrastructure. Code release is associated with X\% faster adoption (HR = X.XX, $p$ = X.XXX). Black-box techniques show X\% faster adoption than white-box techniques (HR = X.XX, $p$ = X.XXX), supporting the hypothesis that realistic threat models accelerate industry uptake.

Model concordance: C-index = X.XX, indicating [good/moderate/poor] predictive discrimination.
}

%==============================================================================
\section{Factors Accelerating Adoption}
\label{sec:acceleration}
%==============================================================================

Our analysis identifies four mechanisms accelerating research-to-practice transfer post-2020.

\subsection{Standardized Evaluation}

AutoAttack~\citep{croce2020autoattack} and RobustBench~\citep{croce2021robustbench} transformed evaluation practices by providing parameter-free, reproducible assessment. Before AutoAttack, reported robust accuracies were often inflated due to weak evaluation; AutoAttack reduced claims by $>$10\% for 13 models. This standardization enabled practitioners to compare defenses on equal footing.

For LLMs, HarmBench~\citep{mazeika2024harmbench} provides comparable standardization, evaluating 18 red-teaming methods across 33 models. MLCommons AILuminate (December 2024) extends this with 24,000+ prompts across 12 hazard categories, providing the first industry-standard safety benchmark.

\subsection{Regulatory Mandates}

The EU AI Act~\citep{euaiact2024} Article 15 requires high-risk AI systems to achieve ``appropriate level of accuracy, robustness and cybersecurity,'' with explicit requirements for ``resilience regarding attempts by unauthorised third parties to alter their use.'' This creates compliance pressure driving adoption of adversarial testing.

NIST AI RMF~\citep{nist2023airisk} provides voluntary but widely-adopted guidance, with the Generative AI Profile~\citep{nist2024genai} specifying 200+ actions including adversarial evaluation. Executive Order 14110 (October 2023) directed NIST to develop these guidelines, accelerating timeline.

\subsection{Market Investment}

The AI security market matured rapidly through major acquisitions:
\begin{itemize}
    \item Cisco acquired Robust Intelligence for approximately \$400M (August 2024)
    \item Palo Alto Networks acquired Protect AI for \$500M+ (April 2025)
    \item F5 acquired CalypsoAI for \$180M (September 2025)
\end{itemize}

Total acquisition value exceeds \$1.1 billion in 2024--2025 alone. HiddenLayer raised \$50M Series A (September 2023), the largest for an AI security startup that year. This capital enables productization of research techniques.

\subsection{Direct User Adversarial Interaction}

LLM security exhibits uniquely compressed adoption cycles because users directly interact with models and discover vulnerabilities. The Bing Chat ``Sydney'' incident saw system prompt extraction within 24 hours of launch. Jailbreaking communities share techniques in real-time. This creates pressure for rapid defense deployment absent in traditional ML where adversaries are more distant from model interfaces.

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\subsection{Summary of Findings}

\placeholder{Synthesis of key findings from Section~\ref{sec:results}:
\begin{itemize}
    \item Overall adoption lag has decreased from 5--7 years (2014--2018 papers) to 1--3 years (2022--2024 papers)
    \item LLM security represents a paradigm shift with compressed cycles
    \item Regulatory pressure is primary accelerator for enterprise adoption
    \item Tool availability necessary but not sufficient for adoption
\end{itemize}}

\subsection{Why Some Research Is Never Adopted}

While our methodology focuses on adopted techniques, the artifact extraction process reveals structural barriers to adoption. Of X total techniques documented in academic papers but absent from our sample, we observe three patterns:

\begin{enumerate}[noitemsep]
    \item \textbf{Naming deficit:} Techniques without memorable names (e.g., ``Method A,'' ``Our approach'') are difficult to trace and reference in practitioner contexts. All adopted techniques in our sample have distinctive names (FGSM, C\&W, PGD, GCG).
    
    \item \textbf{Tooling affordances:} Techniques requiring custom architectures, specialized hardware, or non-standard data formats face implementation barriers. Adopted techniques tend to be architecture-agnostic or provide reference implementations.
    
    \item \textbf{Threat model compatibility:} White-box attacks requiring gradient access face adoption barriers in black-box deployment scenarios. Of X white-box techniques in our sample, Y\% were adapted to gray-box or black-box variants before tool integration.
\end{enumerate}

These observations suggest that adoption requires not only technical merit but also \textit{adoptability}---the extent to which a technique can be named, implemented, and applied in practitioner workflows.

\subsection{Implications for Researchers}

Our findings suggest research design choices affect adoption probability:
\begin{itemize}[noitemsep]
    \item \textbf{Threat model realism:} Black-box and query-efficient methods show faster adoption than white-box approaches requiring gradient access
    \item \textbf{Code availability:} Code release correlates with tool integration (necessary but not sufficient condition)
    \item \textbf{Naming conventions:} Memorable technique names facilitate traceability and practitioner adoption
    \item \textbf{Standardized evaluation:} Benchmark inclusion accelerates uptake by enabling direct comparison
    \item \textbf{Real-world validation:} Testing on deployed systems, though rare, dramatically increases practitioner interest
\end{itemize}

Researchers seeking practical impact should consider threat models reflecting actual deployment constraints rather than worst-case theoretical assumptions.

\subsection{Implications for Practitioners}

The gap between research availability and production readiness suggests practitioners should:
\begin{itemize}
    \item Monitor standardized benchmarks (RobustBench, HarmBench) for defense comparisons
    \item Leverage established toolkits (IBM ART, PyRIT) rather than implementing from papers
    \item Prioritize defenses validated under realistic threat models
    \item Anticipate regulatory requirements (EU AI Act compliance August 2026)
\end{itemize}

\subsection{Limitations}

Our analysis has several limitations. First, adoption evidence may be incomplete---commercial deployments often go undocumented. Second, our landmark paper selection, while systematic, involves judgment calls about impact thresholds. Third, we focus on English-language publications and Western regulatory frameworks. Fourth, the LLM era (2022--2025) provides limited longitudinal data for lag estimation.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This systematic review provides the first quantitative measurement of research-to-industry adoption lag in adversarial machine learning. Analyzing approximately 120 landmark papers from 2014--2025, we document adoption timelines across tool integration, commercial deployment, regulatory citation, and production use.

Our findings reveal domain-dependent patterns: traditional computer vision and malware detection exhibit 4--6 year lags; autonomous systems face 6--8 year timelines constrained by safety certification; LLM security shows compressed 1--2 year cycles driven by direct user interaction and competitive pressure. Overall, adoption has accelerated substantially post-2020, driven by standardized evaluation (AutoAttack, RobustBench, HarmBench), regulatory mandates (EU AI Act, NIST AI RMF), and market investment (\$1B+ in acquisitions).

The \$1.1 billion in AI security acquisitions during 2024--2025 signals enterprise recognition that adversarial ML has transitioned from research curiosity to business requirement. Yet the gap between 5\% attack experience and 86\% security concern~\citep{grosse2023mlsecurity} indicates the market is positioning for threats that remain largely prospective. Bridging research and practice requires continued investment in realistic threat models, standardized evaluation, and accessible tooling.

Our coding framework and adoption evidence database are available at \placeholder{[repository URL]} to support future research-practice alignment studies.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}