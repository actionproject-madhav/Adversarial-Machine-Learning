\documentclass[sigconf,review]{acmart}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath}

% Remove copyright for review
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}

% Paper metadata
\acmConference[CCS '26]{ACM Conference on Computer and Communications Security}{October 2026}{Salt Lake City, UT, USA}
\acmYear{2026}

\begin{document}

\title{From Research to Reality: Measuring the Adoption Lag of Adversarial Machine Learning Techniques in Industry Practice}

\author{Anonymous Authors}
\affiliation{%
  \institution{Paper \#XXX}
}
\email{redacted@institution.edu}

\begin{abstract}
The adversarial machine learning (AML) research community has produced over a decade of publications on attacks and defenses, yet practitioners report persistent gaps between academic advances and industry deployment. While qualitative studies have documented this research-practice divide through surveys and interviews, no work has quantitatively measured the time lag from paper publication to demonstrable industry adoption. We address this gap using a novel \textit{artifact-anchored backward traceability} methodology: starting from 9 authoritative industry artifacts—5 adversarial ML libraries (CleverHans, IBM ART, TextAttack, PyRIT, Foolbox), 3 standardized benchmarks (RobustBench, AutoAttack, HarmBench), and the MITRE ATLAS regulatory framework—we trace backward to extract and code 71 papers cited across these artifacts. Our approach provides verifiable adoption evidence through Git commit timestamps, benchmark integrations, and regulatory citations, enabling precise measurement of adoption lag (median: X.X years, IQR: X.X--X.X years). We find [key findings placeholder]. Our quantitative analysis reveals that [acceleration factors placeholder], with implications for research funding priorities, industry-academia collaboration models, and regulatory compliance timelines. This work provides the first systematic measurement of adversarial ML research-to-practice transfer, complementing prior qualitative gap studies with temporal adoption metrics.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003014</concept_id>
<concept_desc>Security and privacy~Malware and its mitigation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257</concept_id>
<concept_desc>Computing methodologies~Machine learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Malware and its mitigation}
\ccsdesc[500]{Computing methodologies~Machine learning}

\keywords{adversarial machine learning, technology adoption, research-to-practice gap, empirical measurement}

\maketitle

\section{Introduction}

Adversarial machine learning has emerged as a critical security concern, with over a decade of academic research demonstrating vulnerabilities in ML systems~\cite{szegedy2014intriguing, goodfellow2015explaining, carlini2017towards, madry2018towards}. The field has produced hundreds of attack techniques, defense mechanisms, and robustness evaluations across computer vision~\cite{carlini2017towards}, natural language processing~\cite{morris2020textattack}, and more recently, large language models~\cite{zou2023universal, mazeika2024harmbench}. Yet despite this substantial academic output, industry practitioners consistently report a persistent gap between research advances and operational deployment~\cite{kumar2020adversarial, mink2023security, apruzzese2023realgradients}.

This research-practice divide manifests in concrete ways. Kumar et al.'s interviews with 28 organizations revealed that most ML engineers and incident responders are ``not equipped with tactical and strategic tools to protect, detect and respond to attacks on their ML systems''~\cite{kumar2020adversarial}. Mink et al.'s qualitative analysis found that practitioners face barriers including ``lack of institutional motivation and educational resources,'' ``inability to adequately assess AML risk,'' and ``organizational structures that discourage implementation''~\cite{mink2023security}. Most tellingly, Apruzzese et al. observed that while 89\% of adversarial ML papers focus exclusively on deep learning and 63\% evaluate only image data, ``real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems''~\cite{apruzzese2023realgradients}.

These qualitative gap studies provide rich insights into \textit{why} adoption barriers exist, but they cannot answer fundamental questions about \textit{when} and \textit{how fast} research translates into practice. How long does it take for a published attack technique to be implemented in industry tools? Do defenses get adopted faster than attacks? Has adoption accelerated over the field's evolution from foundational work (2014--2017) through expansion (2018--2021) to the LLM era (2022--2025)? What factors predict faster adoption—code availability, venue prestige, industry collaboration, or domain? Without quantitative temporal measurements, we cannot benchmark progress, identify bottlenecks, or design evidence-based interventions to accelerate research translation.

\subsection{Our Approach: Artifact-Anchored Backward Traceability}

We introduce a novel methodology to measure adversarial ML adoption lag through \textit{reverse-engineering from authoritative industry artifacts}. Rather than starting from papers and speculating about impact through forward citation analysis, we begin with concrete evidence of industry adoption—widely-used tools, standardized benchmarks, and regulatory frameworks—and trace \textit{backward} to the research papers they cite and implement.

Our approach selects 9 artifacts representing different adoption pathways: (1)~\textbf{Tools}—CleverHans (6,401 GitHub stars), IBM Adversarial Robustness Toolbox (5,789 stars), TextAttack (3,348 stars), Microsoft PyRIT (3,343 stars), and Foolbox (2,936 stars); (2)~\textbf{Benchmarks}—RobustBench (NeurIPS 2021, 750+ citations), AutoAttack (ICML 2020, 1,987 citations), and HarmBench (ICML 2024); (3)~\textbf{Regulatory frameworks}—MITRE ATLAS (15 tactics, 66 techniques, 33 real-world case studies). These artifacts collectively represent the infrastructure through which adversarial ML research enters practice.

From these 9 artifacts, we automatically extracted 277 unique papers via Git repository scanning of all arXiv references, academic citations, and documentation. We then applied selection criteria prioritizing papers with strongest adoption evidence: 61 papers cited by 2+ artifacts (cross-validated adoption) and 10 papers cited only by MITRE ATLAS (regulatory adoption), yielding a final sample of \textbf{71 papers} for detailed coding. For each paper, we manually coded 12 variables capturing research characteristics (attack/defense/evaluation, threat type, domain, venue, code availability), threat model details (model access, gradient usage), and practical evaluation rigor.

Critically, our methodology provides \textit{verifiable timestamps} for adoption events: Git commit dates when tools first reference papers, benchmark publication dates incorporating research, and MITRE ATLAS case study documentation dates. This enables precise calculation of adoption lag as the time difference between paper publication (conference date or first arXiv submission) and first adoption event across all artifacts. Our dataset provides ground truth for which papers achieved demonstrable industry adoption, when this occurred, and through which pathways.

\subsection{Contributions}

This work makes the following contributions:

\begin{itemize}
\item \textbf{First quantitative measurement} of adversarial ML research-to-practice adoption lag, spanning 2014--2025 across computer vision, NLP, LLMs, and regulatory domains.

\item \textbf{Novel artifact-anchored methodology} using reverse citation tracing from 9 authoritative industry artifacts (5 tools, 3 benchmarks, 1 regulatory framework) to 71 research papers with verified adoption evidence.

\item \textbf{Systematic coding framework} capturing 12 variables per paper (research type, domain, venue, code availability, threat model, practical evaluation), enabling analysis of factors predicting adoption speed.

\item \textbf{Statistical analysis} using Kruskal-Wallis tests, Mann-Whitney U tests with Bonferroni correction, and Cox proportional hazards regression to identify adoption lag patterns by artifact type, publication era, and domain, plus acceleration factors including code availability, venue, and industry collaboration.

\item \textbf{Reproducible dataset and analysis code} providing complete adoption timestamps, coding decisions with inter-rater reliability metrics, and statistical analysis scripts for community validation and extension.
\end{itemize}

\subsection{Paper Organization}

Section~\ref{sec:background} provides background on adversarial ML threats and reviews qualitative gap studies. Section~\ref{sec:rqs} formalizes our research questions. Section~\ref{sec:methodology} details our artifact-anchored methodology, paper selection criteria, coding framework, and statistical analysis plan. Section~\ref{sec:results} presents adoption lag measurements and domain comparisons. Section~\ref{sec:discussion} interprets findings and discusses implications. Section~\ref{sec:conclusion} concludes with recommendations for accelerating research translation.

\section{Background and Related Work}
\label{sec:background}

\subsection{Adversarial Machine Learning Landscape}

Adversarial machine learning encompasses threats across three primary attack categories, formalized in frameworks including MITRE ATLAS and NIST AI 100-2~\cite{nist2025adversarial}.

\textbf{Evasion attacks} modify inputs at test time to cause misclassification while preserving semantic meaning. Foundational work includes Szegedy et al.'s discovery of adversarial examples~\cite{szegedy2014intriguing}, Goodfellow et al.'s Fast Gradient Sign Method (FGSM)~\cite{goodfellow2015explaining}, Carlini \& Wagner's optimization-based attacks~\cite{carlini2017towards}, and Madry et al.'s Projected Gradient Descent (PGD)~\cite{madry2018towards}. Physical-world evasion attacks have demonstrated real-world risks including traffic sign misclassification~\cite{eykholt2018robust} and autonomous vehicle manipulation.

\textbf{Poisoning attacks} corrupt training data or model parameters to degrade performance or insert backdoors. Key work includes backdoor attacks via training data manipulation~\cite{gu2017badnets} and federated learning poisoning. Recent concerns focus on supply chain vulnerabilities in foundation model training.

\textbf{Privacy attacks} extract sensitive information from trained models. Membership inference attacks~\cite{shokri2017membership} determine whether specific records were in training data, while model extraction attacks~\cite{tramer2016stealing} reconstruct model parameters through black-box queries.

For large language models, new attack surfaces have emerged including prompt injection~\cite{greshake2023not}, jailbreaking~\cite{zou2023universal}, and alignment failures~\cite{wei2023jailbroken}. OWASP's LLM Top 10 ranks prompt injection as the \#1 risk for LLM applications.

\subsection{The Research-Practice Gap: Qualitative Evidence}

Four seminal studies have documented the adversarial ML research-practice gap using qualitative methodologies, establishing the foundation that our quantitative work builds upon.

\textbf{Kumar et al. (2020)}~\cite{kumar2020adversarial} conducted interviews with 28 organizations across 11 industries, finding that practitioners ``are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their ML systems.'' Only 6 of 28 organizations were prepared to dedicate staff to building robust ML models. Their survey revealed concerning awareness gaps: most respondents lacked knowledge about even basic adversarial ML concepts.

\textbf{Grosse et al. (2023)}~\cite{grosse2023mlsecurity} provided the largest quantitative survey with 139 industrial practitioners, finding that approximately 5\% of AI practitioners had experienced AI-specific attacks—remarkably low given academic attention. Their statistical analysis revealed that defense implementation correlated with threat exposure or expected likelihood of attack, not company size or organizational area. When asked about implementing countermeasures, the general consensus was ``Why do so?''

\textbf{Mink et al. (2023)}~\cite{mink2023security} conducted 21 semi-structured interviews with data scientists and engineers, identifying three primary barriers: (1) lack of institutional motivation and educational resources for AML concepts, (2) inability to adequately assess AML risk, and (3) organizational structures discouraging implementation in favor of other objectives. Less than 25\% of surveyed developers had access to security experts.

\textbf{Apruzzese et al. (2023)}~\cite{apruzzese2023realgradients} provided the most comprehensive analysis, examining 88 papers from top security venues (CCS, USENIX Security, NDSS, S\&P) from 2019--2021 plus three real-world case studies. Their findings revealed stark research-practice misalignments: 89\% of papers consider only deep learning, 63\% evaluate only image data, only 5\% address malware/phishing/intrusion detection, and 27\% make no mention of computational costs. Their industry case studies demonstrated that real attackers use simple tactics (cropping, masking, stretching, blurring) rather than gradient-based optimization. They concluded: ``Real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems, and as a result security practitioners have not prioritized adversarial ML defenses.''

\subsection{Technology Adoption Measurement Methodologies}

While no prior work has quantitatively measured adversarial ML adoption lag, established methodologies from other domains provide methodological foundations for our approach.

\textbf{Citation lag analysis}~\cite{nakamura2011citation} measures knowledge diffusion speed through temporal analysis of citation patterns. Nakamura et al. introduced citation lag as the time difference between publication dates of cited and citing papers, demonstrating that inter-cluster citations have longer lags than intra-cluster citations, indicating different knowledge integration speeds across research areas.

\textbf{Translational research lag studies} provide critical benchmarks. Morris, Wooding \& Grant's systematic review~\cite{morris2011answer} synthesized 23 papers quantifying time lags in medical research, revealing substantial variation: publication to guideline (0--49 years, mean 8--17 years), drug discovery to commercialization (10--17 years, mean 12 years), and first description to highly cited (14--44 years, mean 24 years). They recommend using operational, measurable markers along translation pathways—directly analogous to our artifact-based adoption events.

\textbf{Backward citation expansion}~\cite{chen2019visualizing} provides methodological justification for our reverse-engineering approach. Chen \& Song's cascading citation expansion methodology enables ``automatic expansion of an initial set by adding articles through citation links in forward, backward, or both directions,'' with backward expansion particularly useful when ``a researcher may come across a recently published review article and would like to find previously published articles that lead to the state of knowledge summarized in the review''—precisely our use case with authoritative tools as ``reviews'' of implemented research.

\textbf{Artifact-citation relationships} have been validated for measuring research impact. Frachtenberg's analysis of 2,439 systems papers~\cite{frachtenberg2022research} found that papers with shared artifacts received 75\% more citations than those without, with GitHub-hosted artifacts showing 86.7\% availability vs. 77.8\% for university-hosted. Heumüller et al.'s study of 789 ICSE papers~\cite{heumuller2020publish} confirmed that ``making artifacts publicly available has made a difference in terms of citations as a measure of scientific impact.'' These findings validate using artifact implementation as a proxy for research adoption.

\subsection{Positioning This Work}

Our work addresses a critical gap at the intersection of adversarial ML security and technology adoption measurement. While qualitative gap studies~\cite{kumar2020adversarial, grosse2023mlsecurity, mink2023security, apruzzese2023realgradients} have documented \textit{why} research doesn't translate into practice—organizational barriers, awareness deficits, threat model mismatches, computational costs—no work has measured \textit{when} and \textit{how fast} adoption occurs.

This temporal dimension is essential for evidence-based policy. Without quantitative lag measurements, we cannot benchmark whether adoption is improving, identify which research domains face the longest delays, or evaluate whether interventions (code release requirements, industry partnerships, standardized benchmarks) accelerate translation. Our artifact-anchored methodology provides verifiable adoption timestamps through Git commits, benchmark integrations, and regulatory citations, enabling the first systematic measurement of adversarial ML research-to-practice transfer timelines.

\section{Research Questions}
\label{sec:rqs}

This work investigates three research questions examining adoption lag patterns, domain variation, and acceleration factors:

\textbf{RQ1: Adoption Lag Measurement.} What is the typical time lag between publication of landmark adversarial ML research and evidence of industry adoption, measured through tool integration, benchmark incorporation, and regulatory citation? We measure adoption lag in months from paper publication (conference date or first arXiv submission) to first adoption event (earliest Git commit, benchmark release, or MITRE ATLAS documentation). We stratify analysis by artifact type (tools, benchmarks, regulatory) and publication era (foundational 2014--2017, expansion 2018--2021, LLM 2022--2025) to identify temporal trends.

\textbf{RQ2: Domain Variation.} How does adoption speed vary across application domains (computer vision, natural language processing, large language models, malware detection, autonomous systems), and what factors explain these differences? We compare adoption lag distributions across 7 domains using pairwise Mann-Whitney U tests with Bonferroni correction ($\alpha = 0.05/21 = 0.0024$) and hypothesize that LLM research shows significantly shorter lags than computer vision due to heightened industry urgency around foundation model security.

\textbf{RQ3: Acceleration Factors.} What mechanisms—code availability, publication venue, industry collaboration, standardized benchmarks—predict faster adoption? We employ Cox proportional hazards regression modeling time-to-first-adoption with covariates including publication year (continuous), domain (categorical, reference: vision), venue type (ML vs. security, reference: ML conferences), code availability at publication (binary), and threat model assumptions (white/gray/black-box, reference: white-box). Hazard ratios >1 indicate faster adoption; we validate proportional hazards assumptions using Schoenfeld residuals.

\section{Methodology}
\label{sec:methodology}

\subsection{Artifact-Anchored Approach}

Our methodology reverses the traditional research impact assessment approach. Rather than starting from papers and tracking forward citations to speculate about practical impact, we begin with concrete evidence of industry adoption—authoritative tools, benchmarks, and frameworks actively used by practitioners—and trace backward to the research papers they cite and implement.

This \textit{artifact-anchored backward traceability} approach provides three key advantages: (1)~\textbf{Verified adoption}—every paper in our sample has demonstrable evidence of industry use through tool implementation, benchmark integration, or regulatory citation; (2)~\textbf{Precise timestamps}—Git commit dates, benchmark publication dates, and framework documentation provide verifiable adoption event timing; (3)~\textbf{Multiple pathways}—we capture diverse adoption mechanisms including open-source tools, academic benchmarks, and regulatory frameworks, providing comprehensive coverage of research translation routes.

\subsection{Artifact Selection}

We selected 9 artifacts representing authoritative industry adoption pathways across three categories, applying rigorous inclusion criteria to ensure representativeness.

\subsubsection{Open-Source Tools (5 artifacts)}

Tool selection criteria: (1) $\geq$1,000 GitHub stars indicating substantial community adoption; (2) Active maintenance with commits in 2024--2025; (3) Focus on adversarial ML specifically (not general ML security); (4) Multiple framework support or domain coverage.

\begin{itemize}
\item \textbf{CleverHans} (6,401 stars): First major AML library, created October 2016 by Ian Goodfellow (Google Brain/OpenAI) and Nicolas Papernot. Maintained by CleverHans Lab at University of Toronto. Provides reference implementations for foundational attacks (FGSM, PGD, C\&W) across JAX, PyTorch, TensorFlow~\cite{papernot2018cleverhans}.

\item \textbf{IBM Adversarial Robustness Toolbox} (5,789 stars): Enterprise-focused library created July 2018, donated to Linux Foundation AI \& Data in 2020. Supports 9 ML frameworks, covers all threat types (evasion, poisoning, extraction, inference). Used in DARPA GARD program and DoD testing~\cite{nicolae2018adversarial}.

\item \textbf{TextAttack} (3,348 stars): Dominant NLP adversarial framework, published EMNLP 2020 by QData Lab (UVA). Implements 16 attack recipes with HuggingFace integration. 835+ citations~\cite{morris2020textattack}.

\item \textbf{PyRIT} (3,343 stars): Microsoft's LLM red-teaming framework, released February 2024. Used for 100+ internal red teaming operations of generative AI models before public release. Integrates with Azure AI evaluation~\cite{munoz2024pyrit}.

\item \textbf{Foolbox} (2,936 stars): Academic benchmark tool from Bethge Lab (Tübingen), dual peer-reviewed publications (JOSS 2020, ICML 2017). Emphasizes minimum perturbation measurement and scientific rigor~\cite{rauber2020foolbox}.
\end{itemize}

\subsubsection{Standardized Benchmarks (3 artifacts)}

Benchmark selection criteria: (1) Peer-reviewed publication at top-tier ML venue (NeurIPS, ICML); (2) Community adoption evidenced by citations or leaderboard submissions; (3) Standardized evaluation protocols.

\begin{itemize}
\item \textbf{RobustBench} (NeurIPS 2021): Standardized adversarial robustness leaderboard with 750+ citations, 120+ evaluated models. Uses AutoAttack for consistent evaluation across CIFAR-10, CIFAR-100, ImageNet~\cite{croce2020robustbench}.

\item \textbf{AutoAttack} (ICML 2020): Parameter-free attack ensemble (APGD-CE, APGD-DLR, FAB, Square Attack) with 1,987 citations. Revealed 13 of 50+ published defenses had robust accuracy overestimated by >10\%~\cite{croce2020reliable}.

\item \textbf{HarmBench} (ICML 2024): First standardized LLM jailbreak evaluation framework. Covers 510 harmful behaviors, 18 attack methods, 33 target LLMs. Backed by Center for AI Safety~\cite{mazeika2024harmbench}.
\end{itemize}

\subsubsection{Regulatory Framework (1 artifact)}

\begin{itemize}
\item \textbf{MITRE ATLAS}: Industry-standard adversarial ML threat taxonomy (15 tactics, 66 techniques, 33 case studies). Co-created with Microsoft in 2020, now with 16 member organizations. \$20M NIST partnership (December 2025). Explicitly referenced in EU AI Act alignment and CISA guidance.
\end{itemize}

\subsection{Paper Extraction and Selection}

We employed automated extraction followed by manual selection based on adoption evidence strength.

\subsubsection{Automated Extraction (277 papers)}

For each of the 9 artifacts, we:
\begin{enumerate}
\item Cloned the complete Git repository history
\item Scanned all files (code, documentation, README, citations) for arXiv identifiers using regex pattern \texttt{arxiv.org/abs/\textbackslash d+\textbackslash.\textbackslash d+}
\item Extracted academic citations from published benchmark papers (RobustBench, AutoAttack, HarmBench)
\item Retrieved MITRE ATLAS case study citations from framework documentation
\item Deduplicated across artifacts to create initial pool of 277 unique papers
\end{enumerate}

\subsubsection{Selection Criteria (71 papers)}

From the 277-paper pool, we applied two selection criteria prioritizing strongest adoption evidence:

\textbf{Criterion 1: Multi-artifact papers ($n=61$).} Papers cited by $\geq$2 artifacts demonstrate cross-validated adoption across different industry pathways (e.g., implemented in both CleverHans and IBM ART, or cited by both RobustBench and MITRE ATLAS). This criterion ensures robust adoption signal.

\textbf{Criterion 2: MITRE ATLAS-only papers ($n=10$).} Papers cited exclusively by MITRE ATLAS represent regulatory adoption pathway. While not implemented in tools or benchmarks, their inclusion in the industry-standard threat framework indicates practitioner awareness and relevance for compliance.

Final sample: \textbf{71 papers} with verified adoption evidence spanning 2014--2025.

\subsection{Coding Framework}

For each of the 71 papers, we manually coded 12 variables across three groups, following a structured codebook with explicit decision rules (see coding\_instructions.pdf in reproducibility materials).

\subsubsection{Research Characteristics (G1--G6)}

\begin{itemize}
\item \textbf{G1 - Type}: Attack / Defense / Evaluation (primary contribution)
\item \textbf{G2 - Threat}: Evasion / Poisoning / Privacy / N/A (attack category per NIST taxonomy)
\item \textbf{G3 - Domain}: Vision / NLP / LLM / Malware / Audio / Tabular / Cross-domain (primary evaluation domain)
\item \textbf{G4 - Venue}: ML conference / Security conference / Journal / arXiv-only (publication type)
\item \textbf{G5 - Code available}: Yes / No (code link exists at time of coding)
\item \textbf{G6 - Code timing}: At-publication / Post-publication / Never (when code released; ``at-publication'' = within 1 month of paper date)
\end{itemize}

\subsubsection{Threat Model (T1--T2, Attack Papers Only)}

\begin{itemize}
\item \textbf{T1 - Access level}: White-box / Gray-box / Black-box (model access assumptions; white = weights/gradients, gray = surrogate, black = queries only)
\item \textbf{T2 - Gradient required}: Yes / No (whether gradients used at any attack stage)
\end{itemize}

\subsubsection{Practical Evaluation (Q1)}

\begin{itemize}
\item \textbf{Q1 - Real-world evaluation}: Yes / Partial / No (``Yes'' = tested on production systems like Google API, Tesla, ChatGPT; ``Partial'' = realistic simulation; ``No'' = CIFAR/ImageNet only)
\end{itemize}

\subsubsection{Coding Procedure}

Two coders independently coded all 71 papers following the structured codebook. Initial coding was performed by GPT-4o with prompt-engineered instructions, then manually verified and corrected by human coders, resulting in 39 corrections documented in \texttt{coding\_corrections.csv}. Inter-rater reliability was assessed using Cohen's $\kappa$ across all 12 variables. Disagreements were resolved through discussion and consultation of paper full text.

\subsection{Adoption Event Definitions and Lag Calculation}

We define three types of adoption events with specific timestamp sources:

\textbf{Tool adoption}: Date of first Git commit that references the paper in code, documentation, or citations file. We extracted commit timestamps (UTC) using \texttt{git log --all --grep} for paper titles and arXiv IDs, verified through manual inspection.

\textbf{Benchmark adoption}: Publication date of the benchmark paper (conference proceedings date) that cites the research. For RobustBench (NeurIPS 2021), AutoAttack (ICML 2020), and HarmBench (ICML 2024), we use official conference dates.

\textbf{Regulatory adoption}: Date when MITRE ATLAS case study or technique description citing the paper was first published in framework documentation (extracted from GitHub repository history of \texttt{mitre/advmlthreatmatrix}).

For each paper, we record \textit{all} adoption events across the 9 artifacts, then identify the \textbf{first adoption} as the earliest event across all pathways. Adoption lag is calculated as:

\begin{equation}
\text{Adoption Lag (months)} = \text{Date}_{\text{first adoption}} - \text{Date}_{\text{publication}}
\end{equation}

Where $\text{Date}_{\text{publication}}$ is the earlier of: (1) conference/journal publication date, or (2) first arXiv submission date. All dates standardized to YYYY-MM-DD format, with lag calculated in months for consistency with translational research literature~\cite{morris2011answer}.

\subsection{Statistical Analysis Plan}

We employ non-parametric tests and survival analysis to address our research questions, with significance threshold $\alpha = 0.05$ (Bonferroni-corrected for multiple comparisons where applicable).

\subsubsection{RQ1: Adoption Lag Measurement}

\textbf{Descriptive statistics}: Median, interquartile range (IQR), range, and mean of adoption lags across full sample ($n=71$).

\textbf{Stratification by artifact type}: Compare adoption lags for papers adopted through tools-only, benchmarks-only, regulatory-only, and multi-pathway adoption using Kruskal-Wallis test with post-hoc Dunn tests (Bonferroni-corrected).

\textbf{Stratification by publication era}: Compare adoption lags across three eras—foundational (2014--2017), expansion (2018--2021), LLM (2022--2025)—using Kruskal-Wallis test to identify temporal trends.

\subsubsection{RQ2: Domain Variation}

\textbf{Pairwise domain comparison}: Compare adoption lag distributions across 7 domains (Vision, NLP, LLM, Malware, Audio, Tabular, Cross-domain) using Mann-Whitney U tests with Bonferroni correction for 21 pairwise comparisons ($\alpha = 0.05/21 = 0.0024$).

\textbf{Hypothesis test}: LLM papers ($n_{\text{LLM}}$) show significantly shorter adoption lags than computer vision papers ($n_{\text{CV}}$) due to heightened industry urgency. One-tailed Mann-Whitney U test.

\subsubsection{RQ3: Acceleration Factors}

\textbf{Cox proportional hazards regression}: Model time-to-first-adoption using Cox regression:

\begin{equation}
\lambda(t|X) = \lambda_0(t) \cdot \exp(\beta_1 X_{\text{year}} + \beta_2 X_{\text{domain}} + \beta_3 X_{\text{venue}} + \beta_4 X_{\text{code}} + \beta_5 X_{\text{threat}})
\end{equation}

Where $\lambda(t|X)$ is the hazard rate (instantaneous adoption probability) at time $t$ given covariates $X$:
\begin{itemize}
\item $X_{\text{year}}$: Publication year (continuous)
\item $X_{\text{domain}}$: Domain (categorical: Vision, NLP, LLM, Malware, Audio, Tabular, Cross-domain; reference = Vision)
\item $X_{\text{venue}}$: Venue type (ML conference vs. Security conference; reference = ML)
\item $X_{\text{code}}$: Code available at publication (binary: Yes/No)
\item $X_{\text{threat}}$: Threat model (White-box / Gray-box / Black-box; reference = White-box)
\end{itemize}

Hazard ratios $\exp(\beta_i)$ interpret as: HR > 1 indicates faster adoption, HR < 1 indicates slower adoption. We validate proportional hazards assumptions using Schoenfeld residuals and report 95\% confidence intervals for all coefficients.

\subsubsection{Software}

All analyses conducted in Python 3.11 using: \texttt{pandas} (1.5.3) for data manipulation, \texttt{scipy} (1.10.1) for Kruskal-Wallis and Mann-Whitney U tests, \texttt{lifelines} (0.27.4) for Cox regression, \texttt{matplotlib} (3.7.1) and \texttt{seaborn} (0.12.2) for visualization. Analysis scripts and data available at [anonymized GitHub repository].

\section{Results}
\label{sec:results}

% Placeholder for results - to be filled after coding and statistical analysis

\subsection{Sample Characteristics}

% Table 1: Descriptive statistics of coded papers
% - Distribution across research types (attack/defense/evaluation)
% - Distribution across domains (Vision/NLP/LLM/etc.)
% - Distribution across venues (ML/Security/Journal/arXiv)
% - Code availability statistics
% - Threat model distributions

\subsection{RQ1: Adoption Lag Patterns}

% Figure 1: Distribution of adoption lags (violin plot or box plot)
% Table 2: Adoption lag statistics by artifact type
% Table 3: Adoption lag statistics by publication era
% Key findings about median lag, IQR, temporal trends

\subsection{RQ2: Domain Variation}

% Figure 2: Adoption lag by domain (box plots with significance markers)
% Table 4: Pairwise Mann-Whitney U test results with Bonferroni correction
% Table 5: LLM vs. CV hypothesis test results

\subsection{RQ3: Acceleration Factors}

% Table 6: Cox proportional hazards regression results
% - Hazard ratios, 95% CIs, p-values for all covariates
% Figure 3: Forest plot of hazard ratios
% Schoenfeld residuals diagnostic plots in appendix

\section{Discussion}
\label{sec:discussion}

% Placeholder for discussion - to be filled after results analysis

\subsection{Key Findings Interpretation}

% Interpretation of adoption lag patterns, domain differences, acceleration factors
% Comparison to translational research benchmarks (Morris et al.'s 17 years)
% Contextualization within adversarial ML field evolution

\subsection{Implications for Researchers}

% How researchers can accelerate adoption of their work
% Importance of code release, industry collaboration, benchmark integration
% Publication venue considerations

\subsection{Implications for Practitioners}

% How practitioners can identify adoptable research
% Using adoption lag as signal for maturity
% Prioritization of defensive techniques based on adoption patterns

\subsection{Limitations}

% Selection bias: artifact selection may miss other adoption pathways
% Implementation ≠ full adoption: citation doesn't guarantee usage
% Missing informal adoption: proprietary implementations invisible
% Temporal resolution: Git commits may not reflect actual implementation timing
% Citation completeness: tools may implement without citing
% Survival bias: only successful tools analyzed

\subsection{Future Work}

% Extending analysis to broader set of artifacts
% Longitudinal tracking of ongoing adoptions
% Qualitative interviews to complement quantitative findings
% Cross-domain comparison with other security areas

\section{Conclusion}
\label{sec:conclusion}

% Placeholder for conclusion - to be filled after results

% Summary of contributions
% Restatement of key findings
% Call to action for research community, funding agencies, industry

\section{Acknowledgments}

This work was supported by [funding agency redacted for review]. We thank [collaborators redacted for review] for valuable feedback on the methodology.

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}