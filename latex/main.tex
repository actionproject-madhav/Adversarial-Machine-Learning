\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{csvsimple}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{pdflscape}
\usepackage{ltablex}
\usepackage{xstring}
\usepackage{newunicodechar}
\usepackage{amssymb}

% Unicode character mappings for the imported dataset table
\newunicodechar{‚àó}{*}
\newunicodechar{‚ãÜ}{*}
\newunicodechar{‚Ä†}{\textdagger}
\newunicodechar{‚Ä°}{\textdaggerdbl}
\newunicodechar{Ô¨Å}{fi}
\newunicodechar{Ô¨Ç}{fl}
\newunicodechar{≈£}{\c{t}}
\newunicodechar{ƒá}{\'c}
\newunicodechar{‚Ä¢}{\textbullet}
\newunicodechar{‚Ñé}{h}
\newunicodechar{‚à•}{\ensuremath{\parallel}}
\newunicodechar{‚âÄ}{\ensuremath{\wr}}
\newunicodechar{¬ß}{\S}
\newunicodechar{¬∂}{\P}
\newunicodechar{√ü}{\ss}
\newunicodechar{√†}{\`a}
\newunicodechar{√§}{\"a}
\newunicodechar{√®}{\`e}
\newunicodechar{√∂}{\"o}
\newunicodechar{√º}{\"u}
\newunicodechar{‚á§}{<-}
\newunicodechar{‚Äã}{} % zero-width non-joiner
% Math bold/italic letters normalized to ASCII
\newunicodechar{ùêµ}{B}
\newunicodechar{ùêª}{H}
\newunicodechar{ùëÜ}{S}
\newunicodechar{ùëä}{W}
\newunicodechar{ùëå}{Y}
\newunicodechar{ùëç}{Z}
\newunicodechar{ùëé}{a}
\newunicodechar{ùëî}{g}
\newunicodechar{ùëñ}{i}
\newunicodechar{ùëõ}{n}
\newunicodechar{ùëú}{o}
\newunicodechar{ùë¢}{u}
\newunicodechar{ùë≥}{L}
\newunicodechar{ùëµ}{N}
\newunicodechar{ùë∫}{S}
\newunicodechar{ùíÇ}{a}
\newunicodechar{ùíÉ}{b}
\newunicodechar{ùíÖ}{d}
\newunicodechar{ùíÜ}{e}
\newunicodechar{ùíâ}{h}
\newunicodechar{ùíä}{i}
\newunicodechar{ùíå}{k}
\newunicodechar{ùíç}{l}
\newunicodechar{ùíê}{o}
\newunicodechar{ùíì}{r}
\newunicodechar{ùíî}{s}
\newunicodechar{ùíö}{y}
\newunicodechar{ùüè}{1}
\newcommand{\yn}[1]{\IfStrEq{#1}{YES}{\checkmark}{\IfStrEq{#1}{NO}{\(\times\)}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\threat}[1]{\IfStrEq{#1}{White-box}{W}{\IfStrEq{#1}{Black-box}{B}{\IfStrEq{#1}{Gray-box}{G}{\IfStrEq{#1}{Partial}{P}{#1}}}}}
\newcommand{\access}[1]{\IfStrEq{#1}{Full}{F}{\IfStrEq{#1}{Partial}{P}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\level}[1]{\IfStrEq{#1}{High}{H}{\IfStrEq{#1}{Low}{L}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}

% Narrow column helper for wide tables
\newcolumntype{Y}{>{\centering\arraybackslash}p{0.9cm}}
\keepXColumns

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Line spacing
\onehalfspacing

% Title
\title{\textbf{Bridging the Gap Between Theory and Practice in Adversarial Machine Learning: A Systematic Cross-Venue Analysis of 454 Papers (2022--2025)}}

\author{
  Madhav Khanal\\
  Rollins College\\
  \texttt{mkhanal@rollins.edu}
  \and
  JJ Jasser\\
  Rollins College\\
  \texttt{jjasser@rollins.edu}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Machine learning systems are increasingly employed in security-critical contexts. However, difference still exists between academic work in adversarial machine learning studies and the requirements of practical applications in the wild. This systematic review examines 454 articles in four leading security conferences, namely ACM CCS, IEEE S\&P, NDSS, and USENIX Security, between 2022 and 2025. Using a conceptual structure developed in ``Real Attackers Don't Compute Gradients'' by Apruzzese et al., we assess the extent to which current studies have bridged the gap between theory and practice.
    
We found that most papers don't test on deployment systems in 94.7\% of studies, 67.8\% require rare gradient access in real-world settings, 80.4\% query budgets are beyond real-world constraints, and 63.2\% require white-box models. We identified five regions that require more focus for real-world deployment: model specifications, model access to gradients, validation on real systems, diversity regions beyond image classification, and economics and human-centric focus. Our study, from 2022 to 2025, verifies the gap within the identified regions. Our goal is to demonstrate how real-world attacks deviate from research in general while also acknowledging the significant contributions that each paper has made. We conclude with directions for researchers, conferences, and funding bodies to build upon the substantial contribution of prior research and foster research that bridges theoretical rigour with real-world security needs.
    
\textbf{Keywords:} adversarial machine learning, theory-practice gap, security research, systematic review, threat modeling
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
%==============================================================================
\section{Introduction}

Machine learning has become an essential component of modern software systems. It is used in facial recognition at border crossings~\citep{grother2019frvt}, fraud detection in financial services~\citep{dalpozzo2014fraud}, navigation for autonomous vehicles~\citep{bojarski2016endtoend}, and content moderation on large social platforms~\citep{gillespie2018custodians}. As a result, ML models increasingly influence decisions that affect millions of people each day. This widespread deployment has intensified scrutiny of adversarial vulnerabilities, defined here as inputs or interactions crafted to cause models to behave in unintended and potentially harmful ways~\citep{biggio2018wild}.

Adversarial machine learning (AML) encompasses the study of attacks against ML systems and defenses to improve their security and robustness~\citep{papernot2016sok}. Over the past decade, researchers have demonstrated a range of attacks across the ML pipeline: evasion attacks that craft malicious inputs to cause misclassification at inference time~\citep{szegedy2014intriguing,goodfellow2015explaining}, poisoning attacks that inject corrupted training data to compromise model behavior~\citep{biggio2012poisoning,gu2017badnets}, and privacy attacks that extract sensitive information through model queries or inspections~\citep{shokri2017membership,carlini2021extracting}. In response, a substantial body of work has proposed defenses intended to improve robustness, privacy, and resistance to adversarial manipulation~\citep{madry2018towards,cohen2019certified}.

\textbf{Why This Gap Matters: Real-World Stakes} The consequences of adversarial vulnerabilities extend beyond academic benchmarks. In November 2025, Anthropic disclosed what it assessed with high confidence to be the first large-scale state-sponsored cyber espionage campaign executed predominantly by an AI agent~\citep{anthropic2025espionage}. Attackers manipulated Claude Code into autonomously conducting reconnaissance, exploit generation, and data exfiltration against approximately thirty global targets, and the system performed an estimated 80--90\% of the operational workload with minimal human oversight. Critically, this incident did not rely on gradient-based attacks or white-box model access; instead, it exploited deployment-level assumptions about agentic autonomy, tool access via the Model Context Protocol~\citep{anthropic2024mcp}, and safeguard circumvention through task decomposition~\citep{wei2024jailbroken}. This case exemplifies a broader pattern documented in Section 3: real adversaries exploit system integration points, human workflows, and economic leverage rather than computing optimal $L_p$-bounded perturbations~\citep{gilmer2018notdetection,kurakin2018physical}.

The economic and societal stakes are substantial. Adversarial attacks impose direct financial costs: deepfake-enabled fraud has caused documented losses reaching millions of dollars in individual incidents~\citep{guardian2024deepfake}, while data extraction attacks can expose proprietary training data worth substantial investment~\citep{carlini2021extracting}. Organizations face barriers to deploying adversarial defenses due to unclear ownership of ML security risks, insufficient tooling for adversarial testing, and limited understanding of realistic threat models~\citep{mink2023barriers,kumar2020adversarial}. Industry surveys reveal widespread uncertainty about how to assess adversarial risks in production systems~\citep{grosse2024threatmodels}. Meanwhile, regulatory frameworks increasingly mandate robustness guarantees for high-stakes ML deployments~\citep{eu2021aiact}, creating compliance costs and potential penalties, yet practitioners report that academic defenses often prove impractical under operational constraints~\citep{arp2022dosdonts}.

\textbf{The Apruzzese Critique} In 2022, Apruzzese and colleagues published ``Real Attackers Don't Compute Gradients''~\citep{apruzzese2022real}, a critical analysis highlighting a persistent gap between academic AML research and practical security concerns. Their core observation was that much of the literature assumes attackers possess capabilities rarely available in real-world settings: white-box access (complete knowledge of model architectures, parameters, and training procedures~\citep{carlini2019evaluating}), the ability to compute gradients through target models~\citep{papernot2017practical}, effectively unlimited query budgets~\citep{chen2017zoo}, and an absence of monitoring or rate limiting~\citep{juuti2019prada}. Through case studies of actual ML attacks, they demonstrated that real adversaries more often rely on simpler strategies, including basic input manipulation, exploitation of system-level weaknesses, or social engineering, when these are sufficient to achieve their objectives. This critique raises a fundamental question about whether academic research priorities align with operational security needs.

\textbf{Research Questions} Based on the Apruzzese critique, we conduct this systematic review to answer the following questions:
\begin{itemize}[noitemsep]
    \item \textbf{RQ1:} To what extent has adversarial ML research addressed practical deployment constraints since the 2022 Apruzzese critique?
    \item \textbf{RQ2:} What patterns distinguish research that bridges the theory--practice gap from work that remains laboratory-focused?
    \item \textbf{RQ3:} How do emerging threats, including large language models and foundation models, affect the alignment between academic research and operational security needs?
\end{itemize}

\textbf{Positioning in Prior Work} This review builds on a growing body of meta-analyses examining the practical relevance of security research. Kumar et al.~\citep{kumar2020adversarial} conducted interviews with 28 organizations and found widespread uncertainty about deploying adversarial defenses. Grosse et al.~\citep{grosse2024threatmodels} surveyed 271 industry practitioners, revealing systematic overestimation of realistic attacker capabilities in academic work. Arp et al.~\citep{arp2022dosdonts} performed a meta-analysis of security research, documenting reliance on simplified laboratory setups that inadequately account for adaptive adversaries. Apruzzese et al.~\citep{apruzzese2022real} synthesized these concerns through case studies demonstrating that real ML attacks rarely resemble academic threat models.

Our work extends this line of inquiry in four ways. First, we provide the first systematic cross-venue analysis of the post-Apruzzese period (2022--2025), examining whether the critique catalyzed methodological changes. Second, we introduce a quantitative Gap Score framework applied uniformly across 454 papers, enabling statistical comparison of research practices across venues and time. Third, we document the emergence of new threat landscapes, particularly agentic LLM misuse and multimodal attacks, that introduce adversarial challenges distinct from traditional computer vision settings. Fourth, we provide actionable recommendations tailored to four stakeholder groups rather than general observations.

\textbf{Scope and Boundaries} To address these questions, we analyze 454 adversarial ML papers published between 2022 and 2025 at four leading security venues: ACM CCS (118 papers), IEEE S\&P (79 papers), NDSS (49 papers), and USENIX Security (208 papers). We selected these venues because they represent top-tier security research outlets that explicitly emphasize real-world impact and practical relevance in their calls for papers and review criteria. The 2022--2025 timeframe captures the research community's response in the post-critique period, with sufficient temporal coverage to identify trends while maintaining methodological tractability.

Our analysis excludes papers from general ML conferences (NeurIPS, ICML, ICLR), workshop-only publications, and preprints. This exclusion is deliberate. Security-focused venues face different review criteria and author incentives than ML theory venues, where advancing state-of-the-art benchmark performance may supersede deployment considerations~\citep{sculley2015debt,bender2021stochasticparrots}. Security conference review criteria explicitly emphasize threat model realism, evaluation on operational systems, and consideration of attacker incentives~\citep{herley2014science}. By focusing on security conferences, we examine research that ostensibly prioritizes practical security impact, making the theory--practice gap more salient when present.

Each paper is evaluated along dimensions adapted from Apruzzese et al.~\citep{apruzzese2022real} and extended to capture deployment realism: threat model assumptions (white-box, gray-box, or black-box adversary knowledge~\citep{carlini2019evaluating}), reliance on gradient access (whether attacks require backpropagation through the target model), query budget requirements (number of model inferences needed), computational cost (GPU requirements and training time), validation setting (evaluation on real production systems versus static benchmarks), and consideration of economic or organizational constraints (costs, incentives, and operational barriers). These dimensions are operationalized into a Gap Score (0--6), where 0 indicates full alignment with practical constraints and 6 indicates reliance on all six idealized assumptions (detailed in Section 4).

\textbf{Preview of Key Findings} Our analysis reveals that the theory--practice gap identified in 2022 remains substantial. Key findings include:
\begin{itemize}[noitemsep]
    \item \textit{Limited real-world evaluation:} Only 5.3\% of papers (24 of 454) evaluate attacks or defenses on deployed systems (operational production environments with real users, monitoring, and operational constraints), while the remainder rely on offline datasets and simulated laboratory environments.
    \item \textit{Persistent gradient dependence:} 67.8\% of papers require gradient information from target models (access to model internals enabling backpropagation-based attacks~\citep{carlini2017towards}), with minimal decline from 2022 (68.2\%) to early 2025 (65.9\%).
    \item \textit{White-box dominance:} 63.2\% assume white-box adversaries with complete model knowledge, despite industry surveys indicating such access is rare in practice~\citep{grosse2024threatmodels}. In contrast, black-box settings (query-only access without model internals~\citep{ilyas2018blackbox}) better reflect most deployment scenarios.
    \item \textit{High query budgets:} Among papers involving model queries, 80.4\% assume budgets exceeding 1000 queries, often ignoring cost constraints (commercial API pricing~\citep{openai2023pricing}), rate limiting~\citep{juuti2019prada}, and anomaly detection systems~\citep{li2022blacklight}.
    \item \textit{Average Gap Score of 3.17:} Most papers incorporate approximately half of the idealized assumptions we track, with only 10.5\% scoring 0--1 (closely aligned with deployment constraints).
\end{itemize}

Despite these gaps, we observe encouraging trends. Code release rates have increased, improving reproducibility~\citep{pineau2021reproducibility}. Emerging work explores query-efficient and gradient-free methods~\citep{tao2023patchattack,eykholt2023uret}. There is growing attention to LLM-specific threats including jailbreaking~\citep{zou2023universal} and prompt injection~\citep{liu2024promptinjection}, along with modest diversification beyond computer vision domains into text~\citep{wallace2019triggers}, audio~\citep{carlini2018audio}, and malware detection~\citep{chen2023obsan}.

\textbf{Contributions} This review makes the following contributions:
\begin{itemize}[noitemsep]
    \item \textit{First post-Apruzzese cross-venue systematic analysis:} We provide the first comprehensive assessment of how 454 papers published across four top-tier security venues (ACM CCS, IEEE S\&P, NDSS, USENIX Security, 2022--2025) address the theory--practice gap following the influential Apruzzese critique~\citep{apruzzese2022real}.
    \item \textit{Quantitative Gap Score framework:} We introduce and apply a systematic coding framework with a six-dimensional Gap Score, enabling statistical comparison of practical relevance across venues, years, and research focus (attack versus defense), extending prior qualitative meta-analyses~\citep{kumar2020adversarial,arp2022dosdonts}.
    \item \textit{Documentation of real-world adversarial incidents:} We synthesize documented cases of ML attacks in deployed systems (Section 3), including the first large-scale agentic AI cyber campaign~\citep{anthropic2025espionage}, prompt injection vulnerabilities~\citep{liu2024promptinjection}, and deepfake fraud~\citep{guardian2024deepfake}, illustrating the sociotechnical nature of adversarial threats.
    \item \textit{Emergence analysis of the LLM threat landscape:} We characterize how foundation models (large-scale pre-trained models adapted to diverse tasks~\citep{bommasani2021foundation}) and agentic systems (autonomous AI systems with tool access and iterative decision-making~\citep{wang2024agents}) introduce adversarial challenges---including jailbreaking~\citep{zou2023universal}, prompt injection~\citep{liu2024promptinjection}, and multimodal attacks~\citep{qi2024visual}---that diverge from traditional threat models (Section 7).
    \item \textit{Actionable stakeholder-specific recommendations:} We provide targeted guidance for researchers (evaluation methodology), conference organizers (review criteria), practitioners (deployment considerations), and funders (bridging infrastructure), grounded in our empirical findings.
\end{itemize}

\textbf{Who This Review Serves} This work is intended for multiple stakeholder groups. Researchers designing new attacks or defenses can use our findings to align evaluation methodologies with deployment realities and identify underexplored but practically relevant research directions. Conference program committees and reviewers can reference our quantitative analysis when assessing the practical relevance of submissions and designing review criteria that reward realistic threat modeling. Industry practitioners can use our gap analysis to critically evaluate whether proposed academic defenses address their operational threat models and constraints, avoiding adoption of methods validated only under idealized assumptions. Funding agencies can use our recommendations to structure programs encouraging academia--industry collaboration, realistic benchmarks, and bridging infrastructure. Collectively, we hope this review catalyzes methodological reflection and incremental alignment between adversarial ML research and operational security needs.


%==============================================================================
% 2. BACKGROUND
%==============================================================================
\section{Background: Foundations of Adversarial Machine Learning}

\subsection{The Discovery of Adversarial Vulnerabilities}

The modern study of adversarial machine learning began with the important work of Szegedy et al.~\citep{szegedy2013intriguing}. They showed that small, barely noticeable changes to input data could lead state-of-the-art deep learning models to produce wrong outputs with high confidence. These changes were not just random mistakes; they were systematic vulnerabilities that could transfer between different models. Adversarial examples created for one model often worked against other models trained separately. This ability to transfer highlighted the nature of the vulnerability and sparked further research.

Goodfellow et al.~\citep{goodfellow2014explaining} suggested that the linear behavior of neural networks, instead of their nonlinearity or overfitting, was the main cause of their vulnerability to adversarial attacks. They introduced the Fast Gradient Sign Method (FGSM), which made it easier to generate adversarial inputs using a single gradient step. The speed and effectiveness of FGSM set a standard for later research, as gradient-based attacks became the common approach.

These foundational studies shaped the direction of adversarial ML research in three significant ways. First, they assumed that adversaries had white-box access to models, including details about their architecture and gradient information. Second, they focused on imperceptible changes measured by specific distance metrics. Third, they highlighted optimization-based methods as the best way to generate adversarial inputs. While these frameworks allowed for thorough analysis, they also shifted research away from the practical realities of real-world attacks.

\subsection{The Arms Race: Attacks and Defenses}

After these initial findings, researchers came up with more advanced gradient-based attacks. Moosavi-Dezfooli et al.~\citep{moosavi2016deepfool} introduced DeepFool, which calculated minimal changes iteratively to cross decision boundaries with smaller, more accurate modifications than FGSM. Carlini and Wagner~\citep{carlini2017towards} approached the generation of adversarial examples as a complex optimization problem tailored to various distance metrics ($L_0$, $L_2$, and $L_\infty$), achieving high success rates against certain defenses, including defensive distillation~\citep{papernot2016distillation}.

This led to an arms race between attack and defense research. Defenses were proposed and tested against known attacks, only to be later bypassed by adaptive gradient-based methods. Carlini and Wagner~\citep{carlini2017detecting} systematically defeated ten detection-based defenses, showing that evaluations limited to known attacks fell short when adversaries adapted their strategies to account for defense mechanisms.

Adversarial training became a significant defense approach. Madry et al.~\citep{madry2018towards} redefined adversarial robustness through a min-max optimization framework, training models to perform well against the worst-case changes within set limits. Tramer et al.~\citep{tramer2017ensemble} introduced ensemble adversarial training to enhance black-box robustness. Research on certified robustness~\citep{li2020sok} aimed to provide formal guarantees that models would remain accurate within certain perturbation limits, although these methods often involved trade-offs between accuracy and robustness and required considerable computing power.

\subsection{Expanding Threat Landscape}

The field then expanded beyond evasion attacks to include privacy violations and threats during training. Membership inference attacks~\citep{shokri2017membership, ye2021enhanced} determine whether specific data points were part of model training. Carlini et al.~\citep{carlini2022membership} argued for assessing such attacks at low false positive rates, which reflect the serious nature of privacy issues.

Training-time attacks interfere with the learning process itself. Backdoor attacks introduce hidden triggers that cause specific misbehaviors when activated, with recent studies showing attacks on self-supervised learning~\citep{jia2021badencoder}, along with distribution-preserving methods that avoid detection~\citep{tao2024distribution}. Poisoning attacks that corrupt training data demonstrate that even small amounts of malicious samples can significantly harm model performance~\citep{tramer2022truth}.

Physical-world attacks took gradient-based techniques beyond digital settings. Kurakin et al.~\citep{kurakin2016adversarial} showed that adversarial examples remained effective when printed and photographed. Later work~\citep{jia2022physical} developed robust physical adversarial examples against traffic sign recognition systems in autonomous vehicles, raising questions about whether real-world attackers would use such complex methods when simpler options might work.

\subsection{The Theory-Practice Gap}

As the theoretical level of adversarial ML research increased, worries grew about whether this work addressed actual security needs in the real world. Kumar et al.~\citep{kumar2020adversarial} interviewed practitioners from 28 organizations, discovering that the industry lacked practical tools for dealing with adversarial ML threats. Grosse et al.~\citep{grosse2024practical} surveyed 271 industrial practitioners, revealing that while academic threat models were theoretically valid, research often overestimated what attackers could do, especially regarding access to training data and query limits.

Mink et al.~\citep{mink2023barriers} identified organizational barriers to deploying defenses through interviews with ML practitioners. These barriers included a lack of motivation from institutions, difficulty assessing AML risk, and competing business priorities. Practitioners often saw security as outside their expertise, revealing mismatches between job definitions in ML and security duties.

Apruzzese et al.~\citep{apruzzese2022real} crystallized these issues through real-world case studies showing that actual ML system breaches often involved simple tactics rather than complex gradient-based attacks: input manipulation without gradients, exploiting system weaknesses instead of model-specific faults, operational gaps, and social engineering. This historical trend encourages a systematic analysis of recent publications to see if current research is addressing the theory-practice gap or continuing past patterns.

\subsection{Types of Adversarial Attacks}

Adversarial attacks generally fall into three categories based on their goals:

\textbf{Evasion attacks} manipulate inputs during inference to cause misclassification. The attacker changes an input---such as an image, malware sample, or network packet---so that the model gives an incorrect prediction. These attacks focus on the inference phase and represent the most studied threat type.

\textbf{Poisoning attacks} corrupt the training process. By adding malicious examples to training data, attackers can lead models to learn wrong behaviors or establish backdoors---hidden triggers that cause specific failures when activated. A compromised model might classify most inputs correctly but struggle with ones containing a particular pattern.

\textbf{Privacy attacks} extract sensitive information. Membership inference attacks find out if specific persons were included in the training data. Model extraction attacks aim to replicate model functions through repeated queries. Data reconstruction attacks try to retrieve actual training examples.

\subsection{Threat Models and Their Implications}

A threat model outlines what an adversary can do and what they know. This aspect is a major area where academic research diverges from real-world usage.

\textbf{White-box access} suggests the attacker knows everything about the model: its architecture, parameters (weights), and possibly its training data. With white-box access, attackers can compute gradients---the mathematical changes needed to alter inputs and impact model outputs. Many academic attacks assume white-box access because it simplifies the process of optimizing attacks.

\textbf{Black-box access} means the attacker can only query the model and observe outputs. This situation is closer to real-world uses where models operate as web services or are embedded in applications. The attacker cannot see internal model details; they can only submit inputs and get predictions.

\textbf{Gray-box access} represents middle-ground scenarios. The attacker might know the model architecture but not the specific parameters or might have access to a similar training dataset.

Apruzzese et al. stress that real-world attackers rarely have white-box access~\citep{apruzzese2022real}. Production models usually have security measures in place. Yet, as our analysis shows, 63.2\% of papers in our dataset assume white-box access, which may not reflect what actually happens in deployment.

%==============================================================================
% 3. METHODOLOGY
%==============================================================================
\section{Methodology}

\subsection{Data Collection}

We systematically reviewed all papers with adversarial ML focus published at four top-tier security venues from 2022 through 2025. These venues were selected because they represent primary publication outlets for security-focused ML research and have historically shaped the field's direction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig01_dataset_overview.png}
    \caption{Dataset overview showing the distribution of 454 papers across four security conferences (ACM CCS, IEEE S\&P, NDSS, USENIX Security) from 2022 to 2025.}
    \label{fig:dataset}
\end{figure}

Our dataset comprises 454 papers: ACM CCS contributed 118 papers (26.0\%), IEEE S\&P contributed 79 papers (17.4\%), NDSS contributed 49 papers (10.8\%), and USENIX Security contributed 208 papers (45.8\%). The temporal distribution includes 89 papers from 2022, 136 from 2023, 198 from 2024, and 31 from 2025 (partial year at time of analysis).

\subsection{Coding Framework}

Each paper was evaluated across multiple dimensions designed to capture practical relevance:

\begin{itemize}[noitemsep]
    \item \textbf{Research Focus (G1):} Whether the paper primarily proposes attacks (60\%), defenses (39\%), or both (2\%).
    \item \textbf{Attack Type (G2):} Classification as evasion (48\%), poisoning (20\%), privacy (23\%), or multiple types (9\%).
    \item \textbf{Data Domain (G4):} The input modality: images (65\%), text (11\%), audio (7\%), malware (6\%), or other (12\%).
    \item \textbf{Threat Model (T1):} Assumed adversary access: white-box (63.2\%), black-box (34.1\%), or gray-box (2.6\%).
    \item \textbf{Gradient Requirements (Q1):} Whether the approach requires gradient access.
    \item \textbf{Query Budget (Q2):} High ($>$1000 queries), low, or none.
    \item \textbf{Real System Testing (G7):} Whether validation occurred on deployed systems.
    \item \textbf{Code Release (G6):} Whether code was released publicly.
\end{itemize}

\subsection{Gap Score Framework}

To quantify the theory-practice gap, we developed a 6-point ``Gap Score'' summing binary indicators of assumptions that may limit practical applicability:

\begin{enumerate}[noitemsep]
    \item Requires white-box access (vs. black/gray-box)
    \item Requires gradient computation
    \item Assumes high query budget ($>$1000 queries)
    \item Requires substantial computation (GPU-level resources)
    \item No testing on deployed systems
    \item No consideration of economic factors
\end{enumerate}

Higher scores indicate greater distance from practical deployment considerations. A paper scoring 0 would employ realistic threat models, require minimal resources, validate on production systems, and consider economic constraints. A paper scoring 6 would represent a primarily theoretical contribution with limited immediate applicability to deployed systems.

%==============================================================================
% 4. QUANTITATIVE FINDINGS
%==============================================================================
\section{Quantitative Findings}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig02_theory_practice_gap.png}
    \caption{Overview of the theory-practice gap showing the percentage of papers exhibiting each characteristic: no real-world testing (94.7\%), high query budget (80.4\%), gradient dependency (67.8\%), white-box access (63.2\%), and no code release (10.4\%).}
    \label{fig:gap_overview}
\end{figure}

Our analysis identifies several areas where research practices diverge from deployment realities. Figure~\ref{fig:gap_overview} summarizes the key indicators across all 454 papers.

\subsection{Real-World Validation}

The most pronounced finding concerns real-world validation: \textbf{only 5.3\% of papers (24 of 454) evaluate on actual deployed systems}. The remaining 94.7\% validate exclusively on research benchmarks, simulated environments, or research prototypes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig05_real_world_testing.png}
    \caption{Real-world testing rate: 5.3\% of papers validate on deployed systems.}
    \label{fig:real_world}
\end{figure}

This pattern has implications because deployment introduces constraints absent from controlled experiments. Production systems employ proprietary model formats and encryption. They integrate ML components into complex security pipelines where multiple components interact. They face hardware constraints, latency requirements, and regulatory compliance obligations. Research validated only in laboratory settings may require substantial adaptation for deployment.

The few papers that do validate on real systems often reveal differences between laboratory and field performance. Nayan et al.~\citep{nayan2024sok} conducted a systematic review of on-device ML model extraction attacks, finding that many proposed academic attacks proved difficult to reproduce, performed less effectively on production models, or introduced unacceptable computational and energy costs. Similarly, Layton et al.~\citep{layton2024sok} demonstrated that deepfake detection research employs metrics and dataset distributions that may lead to overestimation of detector efficacy.

Duan et al.~\citep{duan2022perception} validated perception-aware attacks against YouTube's copyright detection system, representing one of the rare examples of testing against commercial infrastructure. Their work demonstrated that attacks optimized in simulation required adaptation to succeed in practice.

\subsection{Gradient Dependency}

Apruzzese et al.'s critique centered on the observation that ``real attackers don't compute gradients.'' Our analysis indicates this pattern persists: \textbf{67.8\% of papers require gradient access}, despite this capability being unavailable against most production systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig04_gradient_dependency.png}
    \caption{Gradient dependency analysis showing limited change over time (68.2\% in 2022 to 65.9\% in 2025) and variation by attack type.}
    \label{fig:gradient}
\end{figure}

Temporal analysis reveals limited change in this dimension. Gradient dependency has remained relatively stable: 68.2\% in 2022, 69.1\% in 2023, 66.8\% in 2024, and 65.9\% in 2025. While individual papers have explored gradient-free approaches, the overall distribution has not shifted substantially.

Some research has pioneered gradient-free methods in specific domains. The Universal Robustness Evaluation Toolkit (URET) from Eykholt et al.~\citep{eykholt2023uret} formulates adversarial generation as a graph exploration problem, seeking sequences of domain-specific, functionality-preserving transformations rather than relying on differentiable feature spaces. This framework enables evaluation of systems processing inputs like malware binaries or tabular data where semantic and functional correctness must be maintained during perturbation.

Similarly, practical LLM jailbreak attacks demonstrate that effective attacks need not be gradient-based. Liu et al.~\citep{liu2024jailbreaking} and Yu et al.~\citep{yu2024jailbreak} showed that jailbreaking large language models can be effective even when executed via strategically crafted natural language prompts, illustrating the potential of accessible black-box attacks compared to complex gradient optimization.

\subsection{Threat Model Assumptions}

White-box access remains the predominant assumption: \textbf{63.2\% of papers assume adversaries have complete knowledge of model architecture and parameters}. Only 34.1\% consider black-box scenarios more closely matching deployment conditions, and 2.6\% examine gray-box settings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig03_threat_model.png}
    \caption{Distribution of threat model assumptions across all papers, showing prevalence of white-box assumptions.}
    \label{fig:threat_model}
\end{figure}

This pattern contrasts with industrial practice. As Grosse et al.~\citep{grosse2024practical} document, academic studies often operate under assumptions of attacker access---such as extensive access to internal models, parameters, or training data---that do not reflect the security controls present in production environments.

The foundational meta-analysis by Arp et al.~\citep{arp2022dos} identified methodological patterns in security research, including reliance on laboratory-only evaluation and deployment of threat models that may not account for adaptive adversaries. Our analysis suggests these patterns persist in the 2022--2025 literature.

\subsection{Query Budget Assumptions}

Even papers employing black-box threat models often assume substantial query access. \textbf{80.4\% of papers assume high query budgets ($>$1000 queries)}, which may be impractical when commercial APIs implement rate limiting, repeated queries trigger anomaly detection, and real-time constraints limit iterative optimization.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig06_query_budget.png}
    \caption{Query budget assumptions showing 80.4\% of papers assume high query budgets.}
    \label{fig:query}
\end{figure}

Some recent work has addressed query efficiency. HARDBEAT from Tao et al.~\citep{tao2023hardbeat} generates triggers requiring knowledge only of the final predicted label (hard-label) and minimal queries, addressing restrictions imposed by commercial services. BounceAttack from Wan et al.~\citep{wan2024bounceattack} demonstrates query-efficient decision-based attacks. However, these remain exceptions rather than the predominant approach.

\subsection{Domain Distribution}

Adversarial ML research exhibits notable domain concentration: \textbf{65\% of papers focus exclusively on image data}. Text receives 11\% of attention, audio 7\%, malware 6\%, with all other domains combined representing 12\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig08_data_domains.png}
    \caption{Data domain distribution revealing concentration on image classification in adversarial ML research.}
    \label{fig:domains}
\end{figure}

This concentration creates potential blind spots. Financial systems process tabular data where adversarial perturbations cannot be measured by pixel distances. As Kireev et al.~\citep{kireev2023cost} observe for fraud detection, the meaningful constraint is not visual imperceptibility but rather the quantifiable financial cost or utility an adversary must expend. $L_p$ norms may be less informative for tabular financial data; what matters is whether fraudulent transactions remain economically viable.

\subsection{Code Availability}

One dimension shows encouraging results: \textbf{89.6\% of papers release their code}. This represents substantial commitment to reproducibility and exceeds code release rates in many other fields.

However, this introduces a nuanced consideration: code designed for research datasets may not transfer directly to production environments without substantial re-engineering. High code release rates support reproducibility while not necessarily indicating deployment readiness.

\subsection{Gap Score Distribution}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig12_gap_score.png}
    \caption{Distribution of Gap Scores showing the typical paper (mean 3.17/6) incorporates roughly half of the measured assumptions.}
    \label{fig:gap_score}
\end{figure}

The mean Gap Score across all 454 papers is \textbf{3.17 out of 6}, indicating the typical paper incorporates approximately half of the assumptions we measured. The distribution peaks at scores of 3 to 4, with approximately 10.5\% of papers achieving scores of 0 to 1 that would indicate closer alignment with deployment considerations.

Conference-level analysis reveals similar patterns across venues: ACM CCS averages 3.04, NDSS 3.12, IEEE S\&P 3.18, and USENIX 3.25. No venue has established itself as substantially more practice-focused than others; the patterns we observe appear consistent across the security research community.

%==============================================================================
% 5. THEMATIC FINDINGS
%==============================================================================
\section{Thematic Findings}

Beyond quantitative metrics, thematic analysis across venues reveals structural patterns in how adversarial ML research is conducted and evaluated.

\subsection{The Utility-Robustness Trade-off}

A significant consideration for deployment is the relationship between security guarantees and system performance. Research from USENIX Security (2022--2025) particularly illuminates this tension.

Xiang et al.~\citep{xiang2022patchcleanser} found that defensive proposals achieving certifiable robustness against adversarial patches frequently yielded reduced clean classification accuracy, which may discourage real-world deployment. By 2024, Xiang et al.~\citep{xiang2024patchcure} documented that certifiably robust defenses in computer vision require 10 to 100 times more inference-time computation than undefended models, presenting computational challenges for practical deployment.

This pattern extends beyond vision systems. Ahmed et al.~\citep{ahmed2022keyword} found that defenses against malicious activations in voice assistants typically affect natural accuracy. Similarly, widely used privacy-preserving techniques like DP-SGD may compromise model utility to achieve privacy guarantees~\citep{liu2022mldoctor, tang2022membership}.

Some recent work demonstrates progress in addressing this trade-off. PatchCleanser introduced a double-masking approach compatible with any image classifier, achieving high certified robustness while preserving state-of-the-art clean accuracy~\citep{xiang2022patchcleanser}. MIST offers a pathway toward robust security by strategically limiting overfitting only to the most membership-vulnerable training instances~\citep{li2024mist}. CAMP training demonstrated that provable adversarial robustness in deep reinforcement learning need not sacrifice certified expected return, which may be valuable for safety-critical robotics applications~\citep{wang2025camp}.

\subsection{Evaluation Methodology}

Research across venues reveals considerations regarding how attacks and defenses are evaluated.

\subsubsection{Distance Metrics}

Standard evaluation measures adversarial perturbation size using $L_p$ norms---mathematical measures of distance between original and perturbed inputs. However, as Carlini et al.~\citep{carlini2022membership} document, conventional distance metrics like $L_p$ norms, which measure pixel-level differences, do not reliably predict whether humans perceive adversarial perturbations.

This consideration was empirically examined by the Avara framework~\citep{ma2024avara}, which used VR environments and eye-tracking to study whether drivers notice adversarial traffic signs. They found that $L_p$ norms do not reliably predict whether human drivers notice adversarial perturbations: attacks deemed ``imperceptible'' by mathematical standards may be immediately obvious to a driver, while attacks violating $L_p$ constraints might go unnoticed in realistic driving conditions.

\subsubsection{Privacy Evaluation}

Privacy attacks present particular evaluation challenges. Carlini et al.~\citep{carlini2022membership} observe that privacy is fundamentally a worst-case concern: a defense succeeds only if it protects all individuals, not just the majority. Yet membership inference attacks are typically evaluated using average-case metrics (overall accuracy, AUC) that may mask severe privacy leakage for specific individuals.

\subsection{Physical Deployment Considerations}

A consideration for conventional AML research concerns the gap between digital adversarial examples and the complex physical conditions governing real-world perception systems.

\subsubsection{Physical Attack Challenges}

Digital perturbations optimized in simulation may perform differently when deployed physically due to environmental factors: distance and viewing angle variations, illumination changes, sensor noise, and compression artifacts. NDSS research has particularly emphasized this consideration.

Jia et al.~\citep{jia2022physical} developed robust physical adversarial example pipelines tested against production autonomous vehicles running YOLO v5 traffic sign recognition. Their work required accounting for real-road conditions that simulation may not capture.

Physical attack research at USENIX has expanded beyond vision systems. Liu et al.~\citep{liu2023xadv} designed physically realizable 3D adversarial objects capable of deceiving X-ray prohibited item detection, requiring optimization for shape rather than color or texture and accounting for complex object overlap in luggage. Cao et al.~\citep{cao2023lidar} demonstrated Physical Removal Attacks using focused laser spoofing to selectively remove LiDAR point cloud data on autonomous vehicles. The ``Tubes Among Us'' research~\citep{ahmed2023tubes} demonstrated analog adversarial attacks where adversaries manipulate voice signals using simple tubes to bypass speaker recognition, effectively bypassing established digital artifact detection methods.

By 2025, physical attack research has embraced increasingly practical scenarios. ``Shadow Hack''~\citep{kobayashi2025shadow} exploits LiDAR weaknesses using ordinary non-reflective materials placed on roads, requiring no specialized equipment. ATKSCOPES~\citep{zhang2025atkscopes} demonstrates rapid evasion against real-world perceptual hashing algorithms by dynamically adapting to victim systems.

\subsubsection{System Integration}

Research increasingly recognizes that integrating ML models into complex systems introduces considerations that isolated model analysis may miss. Debenedetti et al.~\citep{debenedetti2024privacy} reveal that system-level components such as training data filters or output monitoring may introduce privacy side channels exploitable by adaptive adversaries, potentially affecting provable differential privacy guarantees.

Nasr et al.~\citep{nasr2025magika} demonstrated this through examining how exploiting Google's Magika file-type classifier affects Gmail's malware detection pipeline---a single non-robust component can affect an entire security pipeline.

Chen et al.~\citep{chen2023obsan} document that security mechanisms integrated solely at the framework level may not persist once models are compiled into optimized executables for deployment. Defenses may need to be embedded within the DL compiler pipeline to persist through deployment, a requirement often absent from academic work.

\subsection{Attack Evolution}

Despite methodological considerations about assumptions, research has progressively explored more practical attack vectors.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig07_attack_types.png}
    \caption{Attack type distribution showing evasion attacks (48\%) predominate, with trends over time.}
    \label{fig:attack_types}
\end{figure}

Early practical attacks (2022) demonstrated physically realizable approaches like the ``frustum attack,'' which leverages environmental context to compromise automotive sensor fusion in black-box settings~\citep{hallyburton2022frustum}.

By 2023--2024, attacks increasingly emphasized simplicity and accessibility. The effectiveness of jailbreaking LLMs through natural language prompts, even by users without ML expertise, illustrated the potential of accessible black-box attacks compared to complex gradient optimization.

The 2025 landscape shows attacks prioritizing stealth and persistence. MergeBackdoor~\citep{wang2025mergebackdoor} reveals supply chain considerations where seemingly benign upstream models pass security checks but activate malicious backdoors upon merging with other components. Guo et al.~\citep{guo2025persistent} describe persistent backdoor strategies targeting stable neuronal components, ensuring exploits survive continuous parameter updates in continual learning systems.

\subsection{Defense Evolution}

Defensive research has evolved from generic solutions toward specialized, interpretable, and context-aware mechanisms.

Blacklight~\citep{li2022blacklight} detected and mitigated black-box query-based attacks against MLaaS by leveraging the observation that iterative optimization produces highly similar queries, providing defense against persistent attackers that bypass account-based security measures.

Recent defenses demonstrate increased sophistication. JBShield~\citep{zhang2025jbshield} moves beyond heuristics for LLM protection by using the Linear Representation Hypothesis to identify and manipulate ``toxic'' and ``jailbreak'' concepts within hidden states. SafeSpeech~\citep{zhang2025safespeech} proactively affects voice data during training to make synthesized audio less usable, achieving robustness against voice cloning. DeBackdoor~\citep{popovic2025debackdoor} addresses deployment constraints by providing backdoor detection effective under black-box access, data scarcity, and pre-deployment inspection limitations.

\subsection{Economic and Organizational Factors}

Academic research has given limited attention to the economic and organizational dimensions of adversarial ML.

\subsubsection{Economic Considerations}

ACM CCS research has highlighted how commercial ML services create incentives for IP theft that may affect protective mechanisms. Cong et al.~\citep{cong2022sslguard} document that extracting pre-trained encoders may cost substantially less than training from scratch. Lu et al.~\citep{lu2024neural} show that Neural Dehydration can remove watermarks using less than 2\% of training data. The economic asymmetry---where model extraction attacks may cost orders of magnitude less than defenses or the assets they protect---remains an area requiring additional attention.

\subsubsection{Organizational Considerations}

Beyond technical considerations, qualitative research reveals organizational factors affecting industry adoption. Mink et al.~\citep{mink2023barriers} found that ML practitioners often lack institutional motivation and resources to understand and mitigate adversarial threats, frequently viewing security and machine learning as disconnected fields.

This organizational separation results in lower prioritization of adversarial evaluations before deployment and limited visibility into monitoring for active attacks. Defenses may remain unimplemented due to isolation between ML and security teams or because competing business priorities outweigh the cost and time needed for robust implementation.

%==============================================================================
% 6. TEMPORAL TRENDS
%==============================================================================
\section{Temporal Trends}

A central question motivates this review: has the research community responded to calls for greater practical relevance? Our temporal analysis provides insight into this question.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig11_yearly_trends.png}
    \caption{Temporal trends showing the theory-practice gap from 2022 to 2025, with limited change in key metrics.}
    \label{fig:trends}
\end{figure}

\subsection{Stable Patterns}

\textbf{Real-world testing} remains at approximately 5\% across all years. Despite calls for deployment validation, the research community has not substantially shifted toward testing on production systems.

\textbf{Gradient dependency} has remained relatively stable at 67--69\%. There has been limited movement toward gradient-free approaches at the aggregate level.

\textbf{White-box assumptions} show no substantial reduction (63\% in 2025 vs. 64\% in 2022). Threat model distributions have remained consistent.

\textbf{Domain distribution} persists with image data representing 65\% of papers throughout the study period.

\textbf{Economic analysis} remains below 11\% in all years.

\subsection{Areas of Modest Progress}

Query efficiency shows some progress, with high-budget assumptions declining from 80.4\% to approximately 76\% by 2025. Some researchers have developed query-efficient attacks, though these represent a minority of approaches.

Awareness of attacker knowledge limitations has increased, with growing acknowledgment that adversaries often lack full system knowledge. However, this awareness has not yet translated into substantial shifts in threat modeling practices.

\subsection{Emerging Research Areas}

The 2024--2025 period has seen growth in LLM security research, introducing new considerations. Jailbreak attacks evolve rapidly relative to RLHF defenses~\citep{shen2024llm}. Prompt injection against commercial LLM services poses practical concerns. Adversarial training, the standard defense approach, remains computationally intensive for broad deployment.

%==============================================================================
% 7. CROSS-VENUE ANALYSIS
%==============================================================================
\section{Cross-Venue Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig10_conference_comparison.png}
    \caption{Conference comparison heatmap showing similar patterns across all venues.}
    \label{fig:heatmap}
\end{figure}

While all venues share similar theory-practice gap patterns, each has developed distinctive emphases that collectively illuminate different facets of the research landscape.

\subsection{ACM CCS: Economics and System Integration}

ACM CCS research uniquely emphasizes business logic, usability constraints, and intellectual property protection. Contributions include demonstrating how economic incentives shape the threat landscape~\citep{cong2022sslguard, lu2024neural}, documenting relationships between mathematical metrics and human perception~\citep{ma2024avara}, analyzing system-level integration considerations~\citep{nasr2025magika}, and examining architectural constraints in emerging deployment patterns.

\subsection{IEEE S\&P: Efficiency and Formal Guarantees}

IEEE S\&P research emphasizes computational efficiency, privacy-utility trade-offs, and certified scalability. The venue has advanced understanding of threat modeling considerations and resource assumptions~\citep{carlini2022membership}, documented accuracy-privacy trade-offs~\citep{rezaei2023accuracy}, and examined scalability considerations for certified robustness deployment~\citep{li2023sok}.

\subsection{NDSS: Physical Constraints and Operational Realities}

NDSS research emphasizes system-level constraints, distributed training considerations, and physical deployment realities. Contributions include documenting gaps between digital perturbations and physical deployment~\citep{jia2022physical}, developing metrics for non-image domains~\citep{kireev2023cost}, analyzing distributed training heterogeneity~\citep{rieger2022deepsight}, and studying adversarial dynamics where defenses create exploitable side channels.

\subsection{USENIX Security: Real-World Validation and Domain Diversity}

USENIX Security research emphasizes testing on deployed systems and domain-specific constraints. The venue has documented tensions between theoretical rigor and deployment constraints~\citep{xiang2022patchcleanser, xiang2024patchcure}, analyzed trade-offs involving computational cost, regulatory compliance, and usability~\citep{ahmed2022keyword}, studied adversarial dynamics when defenses encounter adaptive attackers in production, and examined domain-specific requirements~\citep{eykholt2023uret}.

\subsection{Attack versus Defense Papers}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig09_attack_vs_defense.png}
    \caption{Comparison of practicality metrics between attack-focused and defense-focused papers.}
    \label{fig:atk_def}
\end{figure}

Our analysis reveals that defense papers demonstrate somewhat higher alignment with practical considerations than attack papers on average. Defense papers show 6\% real-system testing versus 5\% for attacks, 41\% gradient-free approaches versus 27\%, and 38\% black-box focus versus 31\%. However, both categories remain distant from deployment-ready research.

%==============================================================================
% 8. RECOMMENDATIONS
%==============================================================================
\section{Recommendations}

Bridging the theory-practice gap requires coordinated effort across the research ecosystem. We offer recommendations for multiple stakeholder groups.

\subsection{For Researchers}

\subsubsection{Methodological Considerations}

\textbf{Validate on real systems when feasible.} Partner with industry to evaluate against production deployments. Even limited real-world testing can reveal constraints not apparent in laboratory settings.

\textbf{Employ realistic threat models.} Consider black-box access as a default assumption. When white-box assumptions are necessary, justify them explicitly and acknowledge their implications.

\textbf{Expand domain coverage.} Extend research beyond image classification to tabular data, graphs, time-series, and multimodal systems where practical deployment is occurring.

\textbf{Consider economic factors.} Analyze attack costs, defender resources, and return-on-investment trade-offs. Security decisions involve economic considerations.

\textbf{Include human evaluation.} Test perceptibility against actual humans rather than relying solely on mathematical metrics.

\subsubsection{Evaluation Practices}

Evaluate privacy using true-positive rate at low false-positive rates, not only average accuracy or AUC. For non-image domains, define domain-appropriate perturbation constraints. Report query budgets realistically based on what commercial APIs actually permit. Measure computational requirements in practical units such as wall-clock time and computational cost.

\subsection{For Venues and Program Committees}

\textbf{Encourage real-world validation.} At minimum, request authors to justify why real-system testing is infeasible if omitted.

\textbf{Consider artifact badges for deployability.} Recognize work tested on production systems with explicit acknowledgment.

\textbf{Develop evaluation guidelines.} Request papers to explicitly address threat model realism, query budget constraints, gradient requirements, domain-appropriate metrics, economic considerations, and adaptive adversary testing.

\textbf{Value negative results.} Papers demonstrating that attacks fail under realistic constraints provide valuable information that positive-result publication preferences may suppress.

\subsection{For Industry Practitioners}

\textbf{Interpret academic results in deployment context.} Adjust risk assessments for deployment realities rather than accepting laboratory success rates directly.

\textbf{Anticipate adaptation requirements.} Budget for re-engineering before research prototypes become production-ready.

\textbf{Prioritize defenses against realistic threats:} black-box attacks, low query budget scenarios, economically motivated adversaries, and domain-specific constraints.

\textbf{Consider contributing data.} Anonymized information about real attack attempts could help ground academic research in deployment reality.

\subsection{For Funding Agencies}

Support industry-academic partnerships with real-world validation components. Fund deployment studies rather than only paper publication. Incentivize replication studies that validate academic claims against production systems. Consider red team / blue team programs with realistic constraints.

%==============================================================================
% 9. LIMITATIONS
%==============================================================================
\section{Limitations and Threats to Validity}

\subsection{Scope Limitations}

This review focuses on four security venues and may not capture patterns at ML conferences (NeurIPS, ICML, ICLR) where different norms may apply. Publication preferences may suppress negative results; attacks that fail under realistic constraints or defenses that prove impractical are less likely to be published. Our 2022--2025 window may not capture longer-term trends.

\subsection{Coding Limitations}

The Gap Score reduces complex trade-offs to binary decisions, potentially oversimplifying nuanced situations. Manual coding introduces subjectivity, particularly for papers spanning multiple categories. The definition of ``real system testing'' may vary; some papers test on emulated production environments that share some but not all deployment constraints.

\subsection{Generalizability}

Security venues may actually be more practice-focused than typical computer science venues given their traditional emphasis on real-world threats. Our findings may therefore underestimate the theory-practice gap in the broader ML research community.

%==============================================================================
% 10. CONCLUSION
%==============================================================================
\section{Conclusion}

This systematic review involving 454 research papers published from 2022--2025 at four prestigious security conferences bears evidence of some beneficial contributions by the research community of adversarial machine learning studies in spite of the theory-practice gap noticed by Apruzzese et al.

The quantitative trends are striking:
\begin{itemize}[noitemsep]
    \item 94.7\% of the papers don't test on real systems
    \item 67.8\% need gradients which might not be accessible in real life
    \item 80.4\% assume query budgets that could exceed deployment limits
    \item 63.2\% assume white-box access that could not be provided in deployment situations
\end{itemize}

Thematic analysis shows other considerations emerging. $L_p$ norms may not model human perception well. Robustness may not generalize to production-quality settings. Defense systems do not necessarily take into consideration economic losses and use ease-of-use constraints. Assessment metrics are often averages for the worst-case scenario characteristics.

Among the many contributions of the research community is the theoretical foundation and pioneering research in adversarial phenomena. However, to move forward, accomplishing the goal of closing the theory-practice gap will necessitate structural changes in conferences that reward verification of effectiveness in the real world, assumptions of reasonable threats in research, industry inputs of deployment results, and distribution of funding that emphasizes effectiveness along with innovation.

However, the efficacy of ever more widespread ML-dependent systems is contingent on the ongoing involvements of the community of ML researchers with both the theory and the practicalities. Our observations indicate that those practicalities can, through shifts within publishing, evaluation, and collaboration, help ensure that community-driven ML adversarial research and development work increasingly serves a practical, security context.

%==============================================================================
% REFERENCES
%==============================================================================
\newpage
\bibliographystyle{unsrtnat}
\bibliography{ref}

%==============================================================================
% APPENDIX
%==============================================================================
\newpage
\appendix
\section{Complete Paper Analysis Dataset}

The complete analysis of all 454 papers is available in the supplementary CSV file: 
\texttt{all\_conferences\_analysis\_results\_2022\_2025.csv}

The dataset includes the following columns for each paper:
\begin{itemize}[noitemsep]
    \item Year, Conference, Filename, Title, Authors
    \item G1 (Focus), G2 (Attack Type), G3 (ML Type), G4 (Data Domain)
    \item G5 (Economics), G6 (Code Release), G7 (Real System Testing)
    \item T1 (Threat Model), T2 (Training Data Access)
    \item Q1 (Gradient Requirements), Q2 (Query Budget), Q3 (Computation)
    \item Gap indicator flags and Traditional Score
\end{itemize}

\noindent Benchmark meanings (aligned with the CSV fields used in the table):
\begin{itemize}[noitemsep]
    \item G1 Focus: atk (attack), def (defense), both.
    \item G2 Attack Type: Evasion, Poisoning, Privacy, Multiple.
    \item G3 ML Type: DL, Traditional, Both.
    \item G4 Data Domain: Images, Text, Audio, Malware, Other.
    \item G5 Economics mentioned: YES/NO.
    \item G6 Code released: YES/NO.
    \item G7 Real system testing: YES/NO.
    \item T1 Threat model: White-box, Gray-box, Black-box.
    \item T2 Training data access: Full, Partial, None.
    \item Q1 Requires gradients: YES/NO.
    \item Q2 Query budget: High ($>$1000), Low ($<$1000), None.
    \item Q3 Computation: High (GPU), Low (CPU).
    \item Traditional\_Score (Gap Score 0--6): sum of six impractical-assumption flags (higher = less practical).
\end{itemize}

For full per-paper details (all 24 columns), please see the CSV file. Below is a compact, bordered summary table focused on the most critical taxonomy fields. Binary fields are rendered as checkmarks (yes) or blanks (no); threat/access levels are abbreviated (W/B/G/P, F/P, H/L).

\begin{landscape}
\begin{tiny}
\setlength{\tabcolsep}{0.65pt}
\renewcommand{\arraystretch}{0.7}
\begin{longtable}{|p{0.7cm}|p{0.9cm}|p{1.4cm}|p{0.7cm}|p{1.0cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
\caption{Compact summary of papers (key taxonomy fields).}\label{tab:full_dataset_summary}\\
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endfirsthead
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endhead
\hline
\multicolumn{17}{r}{\small Continued on next page}\\
\hline
\endfoot
\hline
\endlastfoot
\csvreader[
    separator=semicolon,
    column count=24,
    late after line=\\\hline
]{all_conferences_analysis_results_clean.csv}{
Year=\Year,
Conference=\Conference,
Filename=\Filename,
Title=\Title,
Authors=\Authors,
G1=\GOne,
G2=\GTwo,
G3=\GThree,
G4=\GFour,
G5=\GFive,
G6=\GSix,
G7=\GSeven,
T1=\Tone,
T2=\Ttwo,
Q1=\Qone,
Q2=\Qtwo,
Q3=\Qthree,
Flag_Grad=\FGrad,
Flag_HighQ=\FHighQ,
Flag_WB=\FWB,
Flag_NoEcon=\FNoEcon,
Flag_NoCode=\FNoCode,
Flag_NoReal=\FNoReal,
Traditional_Score=\TScore
}{
\Year & \Conference & \StrBefore{\Authors}{ } & \GOne & \GTwo & \GThree & \GFour & \yn{\GFive} & \yn{\GSix} & \yn{\GSeven} & \threat{\Tone} & \access{\Ttwo} & \yn{\Qone} & \level{\Qtwo} & \level{\Qthree} & \yn{\FWB} & \TScore
}
\end{longtable}
\end{tiny}
\end{landscape}

\end{document}