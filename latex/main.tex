\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{csvsimple}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{pdflscape}
\usepackage{ltablex}
\usepackage{xstring}
\usepackage{newunicodechar}
\usepackage{amssymb}

% Unicode character mappings for the imported dataset table
\newunicodechar{‚Äô}{'}
\newunicodechar{‚Äú}{``}
\newunicodechar{‚Äù}{''}
\newunicodechar{‚àó}{*}
\newunicodechar{‚ãÜ}{*}
\newunicodechar{‚Ä†}{\textdagger}
\newunicodechar{‚Ä°}{\textdaggerdbl}
\newunicodechar{Ô¨Å}{fi}
\newunicodechar{Ô¨Ç}{fl}
\newunicodechar{≈£}{\c{t}}
\newunicodechar{ƒá}{\'c}
\newunicodechar{‚Ä¢}{\textbullet}
\newunicodechar{‚Ñé}{h}
\newunicodechar{‚à•}{\ensuremath{\parallel}}
\newunicodechar{‚âÄ}{\ensuremath{\wr}}
\newunicodechar{¬ß}{\S}
\newunicodechar{¬®}{\"{}}
\newunicodechar{¬¥}{'}
\newunicodechar{¬∂}{\P}
\newunicodechar{√ü}{\ss}
\newunicodechar{√†}{\`a}
\newunicodechar{√§}{\"a}
\newunicodechar{√®}{\`e}
\newunicodechar{√∂}{\"o}
\newunicodechar{√º}{\"u}
\newunicodechar{Àá}{\v{}}
\newunicodechar{Àò}{\u{}}
\newunicodechar{‚á§}{<-}
\newunicodechar{ÔøΩ}{?}
\newunicodechar{‚Äã}{} % zero-width non-joiner
% Math bold/italic letters normalized to ASCII
\newunicodechar{ùêµ}{B}
\newunicodechar{ùêª}{H}
\newunicodechar{ùëÜ}{S}
\newunicodechar{ùëä}{W}
\newunicodechar{ùëå}{Y}
\newunicodechar{ùëç}{Z}
\newunicodechar{ùëé}{a}
\newunicodechar{ùëî}{g}
\newunicodechar{ùëñ}{i}
\newunicodechar{ùëõ}{n}
\newunicodechar{ùëú}{o}
\newunicodechar{ùë¢}{u}
\newunicodechar{ùë≥}{L}
\newunicodechar{ùëµ}{N}
\newunicodechar{ùë∫}{S}
\newunicodechar{ùíÇ}{a}
\newunicodechar{ùíÉ}{b}
\newunicodechar{ùíÖ}{d}
\newunicodechar{ùíÜ}{e}
\newunicodechar{ùíâ}{h}
\newunicodechar{ùíä}{i}
\newunicodechar{ùíå}{k}
\newunicodechar{ùíç}{l}
\newunicodechar{ùíê}{o}
\newunicodechar{ùíì}{r}
\newunicodechar{ùíî}{s}
\newunicodechar{ùíö}{y}
\newunicodechar{ùüè}{1}
\newcommand{\yn}[1]{\IfStrEq{#1}{YES}{\checkmark}{\IfStrEq{#1}{NO}{\(\times\)}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\threat}[1]{\IfStrEq{#1}{White-box}{W}{\IfStrEq{#1}{Black-box}{B}{\IfStrEq{#1}{Gray-box}{G}{\IfStrEq{#1}{Partial}{P}{#1}}}}}
\newcommand{\access}[1]{\IfStrEq{#1}{Full}{F}{\IfStrEq{#1}{Partial}{P}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\level}[1]{\IfStrEq{#1}{High}{H}{\IfStrEq{#1}{Low}{L}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}

% Narrow column helper for wide tables
\newcolumntype{Y}{>{\centering\arraybackslash}p{0.9cm}}
\keepXColumns

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Line spacing
\onehalfspacing

% Title
\title{\textbf{Bridging the Gap Between Theory and Practice in Adversarial Machine Learning: A Systematic Cross-Venue Analysis of 454 Papers (2022 to 2025)}}

\author{
  Madhav Khanal\\
  Rollins College\\
  \texttt{mkhanal@rollins.edu}
  \and
  JJ Jasser\\
  Rollins College\\
  \texttt{jjasser@rollins.edu}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Machine learning systems are increasingly deployed in security-critical applications, yet a fundamental disconnect persists between academic adversarial machine learning research and real-world deployment needs. This literature review synthesizes findings from 454 papers published across four top-tier security conferences (ACM CCS, IEEE S\&P, NDSS, and USENIX Security) spanning 2022 to 2025. Following the framework established by Apruzzese et al.'s influential critique ``Real Attackers Don't Compute Gradients,'' we systematically evaluate how well the research community has addressed the theory-practice gap.

Our quantitative analysis reveals persistent and concerning patterns: 94.7\% of papers do not test on real deployed systems, 67.8\% require gradient access that real attackers lack, 80.4\% assume unrealistic query budgets, and 63.2\% rely on white-box threat models rarely available in practice. Through thematic synthesis across venues, we identify five critical disconnects that prevent research translation: unrealistic threat models, gradient dependency assumptions, absence of real-world validation, severe domain bias toward image data, and systematic neglect of economic and human factors. Perhaps most troublingly, our temporal analysis shows these gaps have not narrowed meaningfully from 2022 to 2025 despite explicit calls for change.

\textbf{Keywords:} adversarial machine learning, theory-practice gap, security research, systematic review, threat modeling
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
\section{Introduction}

Machine learning has transformed how we build software systems. From facial recognition at airports to fraud detection in banking, from autonomous vehicles navigating city streets to content moderation on social media platforms, ML models now make decisions that affect millions of people daily. However, these systems are vulnerable to adversarial attacks: carefully crafted inputs designed to cause models to fail in ways their designers never anticipated.

The field of adversarial machine learning (AML) emerged to study these vulnerabilities and develop defenses against them. Over the past decade, researchers have demonstrated impressive attacks: adding imperceptible noise to images that causes classifiers to misidentify them, poisoning training data to implant hidden backdoors, extracting private information about training data through careful queries, and more. In response, the community has proposed numerous defenses claiming to make models robust against such attacks.

Yet there is a fundamental problem: \textbf{the assumptions underlying most academic research do not match the realities of deployed systems.}

In 2022, Apruzzese and colleagues published a landmark analysis titled ``Real Attackers Don't Compute Gradients''~\citep{apruzzese2022real}, documenting a significant gap between academic AML research and practical security needs. Their central observation was simple but devastating: while academic attacks typically assume adversaries can compute gradients through target models (requiring knowledge of model architecture and parameters), real attackers almost never have such access. Real attackers exploit simpler vulnerabilities; they don't need sophisticated mathematical optimization when social engineering or basic input manipulation suffices.

This literature review asks: \textit{four years later, has the research community addressed these concerns?}

To answer this question, we systematically analyzed 454 adversarial ML papers published from 2022 through 2025 at four premier security venues: ACM CCS (118 papers), IEEE S\&P (79 papers), NDSS (49 papers), and USENIX Security (208 papers). We evaluated each paper across multiple dimensions capturing practical relevance: threat model realism, computational requirements, validation methodology, and consideration of deployment constraints.

Our findings are sobering. The theory-practice gap has not narrowed. If anything, certain metrics have worsened. The research community continues to optimize for publication metrics such as novelty, theoretical rigor, and impressive attack success rates rather than for actual deployment viability. This review documents the extent of the problem, analyzes its root causes through thematic synthesis across venues, and offers concrete recommendations for researchers, venues, industry practitioners, and funding agencies.

%==============================================================================
% 2. BACKGROUND
%==============================================================================
\section{Background: Understanding Adversarial Machine Learning}

Before diving into our analysis, readers unfamiliar with adversarial machine learning may benefit from understanding its core concepts and why practical deployment differs so dramatically from laboratory research.

\subsection{What Makes Machine Learning Vulnerable?}

Modern machine learning models, particularly deep neural networks, learn to recognize patterns by processing millions of examples during training. A model trained to classify images, for instance, learns to associate certain pixel patterns with labels like ``cat'' or ``dog.'' However, these models don't ``understand'' images the way humans do; they simply learn statistical correlations between pixel values and labels.

This fundamental difference creates vulnerabilities. Researchers discovered that adding carefully calculated noise to an image, so subtle that humans cannot perceive it, can cause a model to completely misclassify the image with high confidence. A photograph of a panda, with imperceptible perturbations, might be classified as a gibbon. A stop sign, with a few strategically placed stickers, might be classified as a speed limit sign by an autonomous vehicle.

\subsection{Types of Adversarial Attacks}

Adversarial attacks generally fall into three categories based on their goals:

\textbf{Evasion attacks} manipulate inputs at test time to cause misclassification. The attacker modifies an input (an image, a malware sample, a network packet) so that the model makes an incorrect prediction. These attacks target the inference phase and are the most commonly studied.

\textbf{Poisoning attacks} corrupt the training process itself. By injecting malicious examples into training data, attackers can cause models to learn incorrect behaviors or implant ``backdoors'': hidden triggers that cause specific misbehaviors when activated. For example, a poisoned model might correctly classify most images but consistently misclassify any image containing a specific pattern.

\textbf{Privacy attacks} extract sensitive information. Membership inference attacks determine whether specific individuals were in the training data. Model extraction attacks steal the model's functionality by querying it repeatedly. Data reconstruction attacks attempt to recover actual training examples.

\subsection{Threat Models: What Does the Attacker Know?}

A ``threat model'' specifies what capabilities and knowledge an adversary possesses. This is where academic research most dramatically diverges from reality.

\textbf{White-box access} means the attacker has complete knowledge of the model: its architecture, its parameters (weights), and often its training data. With white-box access, attackers can compute gradients, the mathematical derivatives that indicate exactly how to modify an input to change the model's output. Most academic attacks assume white-box access because it makes attack optimization straightforward.

\textbf{Black-box access} means the attacker can only query the model and observe its outputs. This mirrors real-world scenarios where models are deployed as web services or embedded in applications. The attacker cannot see inside the model; they can only submit inputs and receive predictions.

\textbf{Gray-box access} falls between these extremes. The attacker might know the model architecture but not its specific parameters, or might have access to a similar training dataset.

The critical insight from Apruzzese et al. is that \textbf{real-world attackers almost never have white-box access}~\citep{apruzzese2022real}. Production models are protected by extensive security controls. Yet 63.2\% of papers in our dataset assume white-box access, a fundamentally unrealistic starting point.

\subsection{Why the Gap Matters}

One might argue that academic research should push boundaries, studying worst-case scenarios even if they seem impractical. There is merit to this view because understanding what is theoretically possible helps us prepare for future threats. However, the current state of the field goes beyond conservative threat modeling into systematic irrelevance.

When 94.7\% of papers never test on real systems, we cannot know whether proposed attacks actually work in practice or whether proposed defenses actually protect deployed applications. When attacks require thousands of queries to a model, but real systems have rate limiting and anomaly detection, the attacks may be completely impractical. When defenses impose 10 to 100$\times$ computational overhead, no production system will deploy them.

The consequence is a research field that has become largely self-referential: papers cite other papers, attacks beat defenses that were never deployed, and defenses claim robustness against attacks that were never practical. Meanwhile, actual deployed ML systems face threats that the research community rarely studies.

%==============================================================================
% 3. METHODOLOGY
%==============================================================================
\section{Methodology}

\subsection{Data Collection}

We systematically reviewed all papers with adversarial ML focus published at four top-tier security venues from 2022 through 2025. These venues were selected because they represent the primary publication outlets for security-focused ML research and have historically shaped the field's direction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig01_dataset_overview.png}
    \caption{Dataset overview showing the distribution of 454 papers across four security conferences (ACM CCS, IEEE S\&P, NDSS, USENIX Security) from 2022 to 2025.}
    \label{fig:dataset}
\end{figure}

Our dataset comprises 454 papers: ACM CCS contributed 118 papers (26.0\%), IEEE S\&P contributed 79 papers (17.4\%), NDSS contributed 49 papers (10.8\%), and USENIX Security contributed 208 papers (45.8\%). The temporal distribution shows 89 papers from 2022, 136 from 2023, 198 from 2024, and 31 from 2025 (partial year at time of analysis).

\subsection{Coding Framework}

Each paper was evaluated across multiple dimensions designed to capture practical relevance:

\begin{itemize}[noitemsep]
    \item \textbf{Research Focus (G1):} Whether the paper primarily proposes attacks (60\%), defenses (39\%), or both (2\%).
    \item \textbf{Attack Type (G2):} Classification as evasion (48\%), poisoning (20\%), privacy (23\%), or multiple types (9\%).
    \item \textbf{Data Domain (G4):} The input modality - images (65\%), text (11\%), audio (7\%), malware (6\%), or other (12\%).
    \item \textbf{Threat Model (T1):} Assumed adversary access - white-box (63.2\%), black-box (34.1\%), or gray-box (2.6\%).
    \item \textbf{Gradient Requirements (Q1):} Whether the approach requires gradient access.
    \item \textbf{Query Budget (Q2):} High ($>$1000 queries), low, or none.
    \item \textbf{Real System Testing (G7):} Whether validation occurred on deployed systems.
    \item \textbf{Code Release (G6):} Whether code was released publicly.
\end{itemize}

\subsection{Gap Score Framework}

To quantify the theory-practice gap, we developed a 6-point ``Gap Score'' summing binary indicators of impractical assumptions:

\begin{enumerate}[noitemsep]
    \item Requires white-box access (vs. black/gray-box)
    \item Requires gradient computation
    \item Assumes high query budget ($>$1000 queries)
    \item Requires high computation (GPU-level resources)
    \item No testing on real deployed systems
    \item No consideration of economic factors
\end{enumerate}

Higher scores indicate greater distance from practical deployment. A paper scoring 0 would use realistic threat models, require minimal resources, test on production systems, and consider economic constraints. A paper scoring 6 would represent a purely academic exercise unlikely to inform real-world security.

%==============================================================================
% 4. QUANTITATIVE FINDINGS
%==============================================================================
\section{Quantitative Findings: The State of the Gap}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig02_theory_practice_gap.png}
    \caption{Overview of the theory-practice gap showing the percentage of papers exhibiting each problematic assumption: no real-world testing (94.7\%), high query budget (80.4\%), gradient dependency (67.8\%), white-box access (63.2\%), and no code release (10.4\%).}
    \label{fig:gap_overview}
\end{figure}


Our analysis reveals a research field persistently disconnected from deployment realities. Figure~\ref{fig:gap_overview} summarizes the key gap indicators across all 454 papers.

\subsection{The Real-World Testing Crisis}

The single most striking finding is the near-complete absence of real-world validation: \textbf{only 5.3\% of papers (24 of 454) test on actual deployed systems}. The remaining 94.7\% evaluate exclusively on research benchmarks, simulated environments, or research prototypes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig05_real_world_testing.png}
    \caption{The stark real-world testing gap: only 5.3\% of papers validate on actual deployed systems.}
    \label{fig:real_world}
\end{figure}


This matters because deployment introduces constraints completely absent from controlled experiments. Real systems use proprietary model formats and encryption. They integrate ML components into complex security pipelines where multiple components interact. They face hardware constraints, latency requirements, and regulatory compliance obligations. A defense that works perfectly in a research setting may be entirely impractical when these factors are considered.

Research from USENIX Security particularly highlights this disconnect. \citet{nayan2024sok} conducted a systematic review of on-device ML model extraction attacks, finding that many proposed academic attacks ``prove difficult to reproduce, fail to perform effectively on production models, or introduce unacceptable computation and energy costs.'' Similarly, \citet{layton2024sok} demonstrated that deepfake detection research uses inappropriate metrics and unrealistic dataset distributions, leading to ``overestimation of detector efficacy and creating difficulties for transitioning these tools into practice.''

The few papers that do test on real systems often reveal surprising gaps between laboratory and field performance. \citet{duan2022perception} validated perception-aware attacks against YouTube's copyright detection system - one of the rare examples of testing against actual commercial infrastructure. Their work demonstrated that attacks optimized in simulation required significant adaptation to succeed in practice.

\subsection{The Gradient Dependency Problem}

Apruzzese et al.'s critique centered on the observation that ``real attackers don't compute gradients.'' Our analysis confirms this concern persists: \textbf{67.8\% of papers require gradient access}, despite this capability being unavailable against most production systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig04_gradient_dependency.png}
    \caption{Gradient dependency analysis showing no meaningful improvement over time (68.2\% in 2022 to 65.9\% in 2025) and variation by attack type.}
    \label{fig:gradient}
\end{figure}

Perhaps more concerning, temporal analysis reveals no meaningful improvement. Gradient dependency has remained essentially flat: 68.2\% in 2022, 69.1\% in 2023, 66.8\% in 2024, and 65.9\% in 2025. The research community has not shifted toward gradient-free approaches despite explicit calls to do so.

USENIX research has pioneered gradient-free approaches in specific domains. The Universal Robustness Evaluation Toolkit (URET) from \citet{eykholt2023uret} addresses this gap by ``formulating adversarial generation as a graph exploration problem, seeking sequences of domain-specific, functionality-preserving transformations rather than relying on differentiable feature spaces.'' This framework enables studying systems utilizing inputs like malware binaries or tabular data ``where semantic and functional correctness must be maintained during perturbation.''

Similarly, practical LLM jailbreak attacks demonstrate that effective attacks need not be gradient-based. \citet{liu2024jailbreaking} and \citet{yu2024jailbreak} showed that jailbreaking large language models ``can be highly effective even when executed by inexperienced users via strategically crafted natural language prompts, emphasizing the potency of low-cost, accessible black-box attacks over complex gradient optimization.''

\subsection{Unrealistic Threat Model Assumptions}

White-box access remains the dominant assumption: \textbf{63.2\% of papers assume adversaries have complete knowledge of model architecture and parameters}. Only 34.1\% consider black-box scenarios matching real deployment conditions, and a mere 2.6\% examine gray-box settings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig03_threat_model.png}
    \caption{Distribution of threat model assumptions across all papers, showing dominance of unrealistic white-box assumptions.}
    \label{fig:threat_model}
\end{figure}


This assumption fundamentally contradicts industrial reality. As \citet{grosse2024practical} document in their USENIX paper on practical threat models, ``academic studies often operate under assumptions of overly generous attacker access - such as extensive access to internal models, parameters, or training data - that do not reflect the stringent security controls present in real-world corporate environments.''

The foundational USENIX meta-analysis by \citet{arp2022dos} exposed these ``widespread methodological pitfalls in security research, including reliance on `lab-only evaluation' and deployment of `inappropriate threat models' that fail to account for adaptive adversaries.'' Three years later, the field has not substantively responded to this critique.

\subsection{Query Budget Assumptions}

Even papers claiming black-box threat models often make unrealistic assumptions about query access. \textbf{80.4\% of papers assume high query budgets ($>$1000 queries)} - impractical when commercial APIs implement rate limiting, repeated queries trigger fraud detection systems, and real-time constraints prevent iterative optimization.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig06_query_budget.png}
    \caption{Query budget assumptions showing 80.4\% of papers assume high query budgets that are impractical in real deployments.}
    \label{fig:query}
\end{figure}


Some recent work has begun addressing query efficiency. HARDBEAT from \citet{tao2023hardbeat} generates ``high-success-rate triggers needing knowledge only of the final predicted label (hard-label) and minimal queries, addressing restrictions often imposed by proprietary commercial services.'' Similarly, BounceAttack from \citet{wan2024bounceattack} demonstrates query-efficient decision-based attacks. However, these remain exceptions rather than the norm.

\subsection{Domain Bias: The Image Obsession}

Adversarial ML research exhibits severe domain bias: \textbf{65\% of papers focus exclusively on image data}. Text receives 11\% attention, audio 7\%, malware 6\%, with all other domains combined representing just 12\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig08_data_domains.png}
    \caption{Data domain distribution revealing severe image-centric bias in adversarial ML research.}
    \label{fig:domains}
\end{figure}

This creates dangerous blind spots. Financial systems process tabular data where adversarial perturbations cannot be measured by pixel distances. As \citet{kireev2023cost} observe for fraud detection, ``the meaningful constraint is not visual imperceptibility but rather the quantifiable financial cost or utility an adversary must expend.'' $L_p$ norms are meaningless for tabular financial data - what matters is whether fraudulent transactions remain economically viable.

\subsection{The Code Release Paradox}

One dimension shows positive results: \textbf{89.6\% of papers release their code}. This represents laudable commitment to reproducibility and significantly exceeds code release rates in many other fields.

However, this creates a new problem: code designed for research datasets often cannot transfer to production environments without substantial re-engineering. High code release rates may create an illusion of deployability that does not survive contact with production constraints.

\subsection{Gap Score Distribution}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig12_gap_score.png}
    \caption{Distribution of Gap Scores showing the typical paper (mean 3.17/6) makes roughly half of the measured impractical assumptions.}
    \label{fig:gap_score}
\end{figure}


The mean Gap Score across all 454 papers is \textbf{3.17 out of 6}, indicating the typical paper makes roughly half of the impractical assumptions we measured. The distribution peaks at scores of 3 - 4, with very few papers (approximately 10.5\%) achieving scores of 0 - 1 that would indicate deployment readiness.

Conference-level analysis reveals minimal variation: ACM CCS averages 3.04, NDSS 3.12, IEEE S\&P 3.18, and USENIX 3.25. No venue has established itself as more practice-focused than others - the theory-practice gap is an endemic problem across the entire security research community.

%==============================================================================
% 5. THEMATIC FINDINGS
%==============================================================================
\section{Thematic Findings: Understanding the Gap}

Beyond quantitative metrics, thematic analysis across venues reveals deeper structural problems in how adversarial ML research is conducted and evaluated.

\subsection{The Utility-Robustness Trade-off}

Perhaps the most significant barrier to deployment is the conflict between security guarantees and system performance. Research from USENIX Security (2022 to 2025) particularly illuminates this tension.

\citet{xiang2022patchcleanser} found that defensive proposals achieving certifiable robustness against adversarial patches frequently ``yielded poor clean classification accuracy, which inherently `discourages the real-world deployment' of these defenses.'' This pattern persisted through subsequent years. By 2024, \citet{xiang2024patchcure} documented that certifiably robust defenses in computer vision require ``10 to 100 times more inference-time computation than undefended models, rendering them computationally prohibitive for practical use.''

The problem extends beyond vision systems. \citet{ahmed2022keyword} found that defenses against malicious activations in voice assistants ``typically harm the natural accuracy - an unacceptable proposition for commercial systems.'' Similarly, widely used privacy-preserving techniques like DP-SGD ``compromise model utility significantly to achieve privacy guarantees''~\citep{liu2022mldoctor, tang2022membership}.

Some recent work shows promise in resolving this fundamental conflict. PatchCleanser introduced a double-masking approach compatible with any image classifier, achieving high certified robustness while preserving state-of-the-art clean accuracy~\citep{xiang2022patchcleanser}. MIST offers a pathway toward robust security without performance penalties by strategically limiting overfitting only to the most membership-vulnerable training instances~\citep{li2024mist}. CAMP training demonstrated that provable adversarial robustness in deep reinforcement learning need not sacrifice certified expected return - crucial for safety-critical robotics applications~\citep{wang2025camp}.

\subsection{Inappropriate Evaluation Metrics}

Research across venues reveals systematic problems with how attacks and defenses are evaluated.

\subsubsection{The $L_p$ Norm Problem}

Standard evaluation measures adversarial perturbation size using $L_p$ norms - mathematical measures of distance between original and perturbed inputs. However, as \citet{carlini2022membership} document, ``conventional distance metrics like $L_p$ norms - measuring pixel-level differences - do not reliably predict whether humans perceive adversarial perturbations as anomalous.''

This disconnect was empirically demonstrated by the Avara framework~\citep{ma2024avara}, which used VR environments and eye-tracking to study whether drivers notice adversarial traffic signs. They found that $L_p$ norms ``fail to predict'' whether human drivers notice adversarial perturbations: ``An attack deemed `imperceptible' by mathematical standards may be immediately obvious to a driver,'' while attacks violating $L_p$ constraints might go unnoticed in realistic driving conditions.

\subsubsection{Average-Case vs. Worst-Case Privacy}

Privacy attacks are evaluated using fundamentally inappropriate metrics. \citet{carlini2022membership} observe that ``privacy is fundamentally a worst-case concern: a defense succeeds only if it protects all individuals, not just the majority.'' Yet membership inference attacks are typically evaluated using average-case metrics (overall accuracy, AUC) that mask severe privacy leakage for specific individuals.

\subsection{Physical Deployment Constraints}

A fundamental limitation of conventional AML research lies in its focus on digital adversarial examples that fail to account for the complex physical conditions governing real-world perception systems.

\subsubsection{From Digital to Physical Attacks}

Digital perturbations optimized in simulation frequently fail when deployed physically due to environmental factors: distance and viewing angle variations, illumination changes, sensor noise, and compression artifacts. NDSS research has particularly emphasized this gap.

\citet{jia2022physical} developed robust physical adversarial example pipelines tested extensively against production autonomous vehicles running YOLO v5 traffic sign recognition. Their work required accounting for real-road conditions that simulation ignores.

Physical attack research at USENIX has expanded beyond vision systems. \citet{liu2023xadv} designed physically realizable 3D adversarial objects capable of deceiving X-ray prohibited item detection, requiring optimization for shape rather than color or texture and accounting for complex object overlap in luggage. \citet{cao2023lidar} demonstrated Physical Removal Attacks using focused laser spoofing to selectively remove LiDAR point cloud data on autonomous vehicles. The ``Tubes Among Us'' research~\citep{ahmed2023tubes} demonstrated analog adversarial attacks where human adversaries manipulate voice signals using simple tubes to bypass speaker recognition - ``effectively bypassing established digital artifact detection methods.''

By 2025, physical attack research embraced increasingly practical scenarios. ``Shadow Hack''~\citep{kobayashi2025shadow} exploits LiDAR weaknesses using ordinary non-reflective materials placed on roads - requiring no specialized equipment. ATKSCOPES~\citep{zhang2025atkscopes} demonstrates rapid evasion against real-world perceptual hashing algorithms by dynamically adapting to victim systems.

\subsubsection{System Integration Vulnerabilities}

Research increasingly recognizes that integrating ML models into complex systems introduces vulnerabilities that isolated model analysis misses. \citet{debenedetti2024privacy} reveal that system-level components such as training data filters or output monitoring ``introduce critical privacy side channels easily exploitable by adaptive adversaries, often invalidating provable differential privacy guarantees.''

\citet{nasr2025magika} demonstrated this through the ``weakest-link'' problem: exploiting Google's Magika file-type classifier compromises Gmail's entire malware detection pipeline - even if other components are robust. A single non-robust component can compromise an entire security pipeline.

Models trained in high-level frameworks are compiled into optimized executables for deployment, but as \citet{chen2023obsan} document, ``security mechanisms integrated solely at the framework level fail once models are compiled.'' Defenses must be embedded within the DL compiler pipeline to persist through deployment - a requirement absent from most academic work.

\subsection{Attack Evolution Toward Practicality}

Despite methodological concerns about unrealistic assumptions, research has progressively shifted toward practical attack vectors.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig07_attack_types.png}
    \caption{Attack type distribution showing evasion attacks (48\%) dominate, with trends over time.}
    \label{fig:attack_types}
\end{figure}


Early practical attacks (2022) demonstrated physically realizable approaches like the ``frustum attack,'' which leverages environmental context to compromise automotive sensor fusion in black-box settings~\citep{hallyburton2022frustum}.

By 2023 to 2024, attacks increasingly capitalized on simplicity and accessibility. The effectiveness of jailbreaking LLMs through natural language prompts - even by inexperienced users - emphasized the potency of low-cost black-box attacks over complex gradient optimization.

The 2025 landscape shows attacks prioritizing stealth and persistence. MergeBackdoor~\citep{wang2025mergebackdoor} reveals supply chain threats where seemingly benign upstream models pass security checks but activate malicious backdoors upon merging with other components. Persistent backdoor strategies from \citet{guo2025persistent} target stable neuronal components, ensuring exploits survive continuous parameter updates in continual learning systems.

\subsection{Defense Evolution Toward Specialization}

Defensive research has evolved from generic solutions toward specialized, interpretable, and context-aware mechanisms.

Blacklight~\citep{li2022blacklight} successfully detected and mitigated nearly all black-box query-based attacks against MLaaS by leveraging the fact that iterative optimization inevitably produces highly similar queries - providing effective defense against persistent attackers that bypass account-based security measures.

Recent defenses demonstrate increased sophistication. JBShield~\citep{zhang2025jbshield} moves beyond heuristics for LLM protection by using the Linear Representation Hypothesis to identify and manipulate ``toxic'' and ``jailbreak'' concepts within hidden states. SafeSpeech~\citep{zhang2025safespeech} proactively poisons voice data during training to make synthesized audio unusable - achieving robustness surpassing inference-only defenses against voice cloning. DeBackdoor~\citep{popovic2025debackdoor} addresses realistic deployment constraints by providing backdoor detection effective under black-box access, data scarcity, and pre-deployment inspection limitations.

\subsection{Economic and Human Factor Blind Spots}

Academic research systematically ignores the economic and human dimensions of adversarial ML.

\subsubsection{The Economics of Attacks}

ACM CCS research has highlighted how commercial ML services create powerful financial incentives for IP theft that render many protective mechanisms inadequate. \citet{cong2022sslguard} document that stealing pre-trained encoders costs far less than training from scratch. \citet{lu2024neural} show that Neural Dehydration can remove watermarks using less than 2\% of training data. The fundamental economic asymmetry - model extraction attacks cost orders of magnitude less than defenses or the assets they protect - remains largely unaddressed.

\subsubsection{Organizational Barriers}

Beyond technical challenges, qualitative research reveals significant organizational barriers preventing industry adoption. \citet{mink2023barriers} found that ``machine learning practitioners often lack institutional motivation and usable resources to understand and mitigate adversarial threats, frequently assuming security and machine learning are entirely disconnected fields.''

This organizational detachment results in low prioritization of adversarial evaluations before deployment and poor visibility into monitoring for active attacks. Defenses remain unimplemented due to ``isolation between ML and security teams or because competing business priorities outweigh the cost and time needed for robust implementation.''

%==============================================================================
% 6. TEMPORAL TRENDS
%==============================================================================
\section{Temporal Trends: Is the Gap Narrowing?}

A critical question motivates this review: has the research community responded to calls for greater practical relevance? Our temporal analysis provides a clear answer: \textbf{no, the gap has not meaningfully narrowed}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig11_yearly_trends.png}
    \caption{Temporal trends showing the theory-practice gap has not narrowed from 2022 to 2025, with some metrics worsening.}
    \label{fig:trends}
\end{figure}


\subsection{Metrics That Have Not Improved}

\textbf{Real-world testing} remains at approximately 5\% across all years. Despite explicit calls for deployment validation, the research community has not shifted toward testing on production systems.

\textbf{Gradient dependency} has remained essentially flat at 67 - 69\%. There has been no meaningful movement toward gradient-free approaches.

\textbf{White-box assumptions} show no reduction (63\% in 2025 vs. 64\% in 2022). Threat model realism has not improved.

\textbf{Domain bias} persists with image data dominating 65\% of papers throughout the study period.

\textbf{Economic analysis} remains below 11\% in all years.

\subsection{Metrics Showing Marginal Improvement}

Query efficiency shows modest progress, with high-budget assumptions declining from 80.4\% to approximately 76\% by 2025. Some researchers have begun developing query-efficient attacks, though these remain minority approaches.

Zero-knowledge attacks have gained recognition, with growing acknowledgment that adversaries often lack full system knowledge. However, this has not translated into substantial shifts in threat modeling practices.

\subsection{Emerging Threat Categories}

The 2024 to 2025 period has seen rapid growth in LLM security research, creating urgent new gaps. Jailbreak attacks evolve faster than RLHF defenses can adapt~\citep{shen2024llm}. Prompt injection against commercial LLM services poses immediate practical threats. Adversarial training - the standard defense approach - proves too computationally costly for broad deployment.

%==============================================================================
% 7. CROSS-VENUE ANALYSIS
%==============================================================================
\section{Cross-Venue Analysis: Conference-Specific Contributions}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig10_conference_comparison.png}
    \caption{Conference comparison heatmap showing all venues share similar gap patterns.}
    \label{fig:heatmap}
\end{figure}


While all venues share the fundamental theory-practice gap, each has developed distinctive emphases that collectively illuminate different facets of the problem.

\subsection{ACM CCS: Economics and System Integration}

ACM CCS research uniquely emphasizes business logic, usability constraints, and intellectual property protection. Contributions include demonstrating how economic incentives fundamentally shape the threat landscape~\citep{cong2022sslguard, lu2024neural}, documenting the disconnect between mathematical metrics and human perception~\citep{ma2024avara}, analyzing system-level integration vulnerabilities~\citep{nasr2025magika}, and revealing architectural constraints in emerging deployment patterns.

\subsection{IEEE S\&P: Efficiency and Formal Guarantees}

IEEE S\&P research emphasizes computational efficiency, privacy-utility trade-offs, and certified scalability. The venue has advanced understanding of inappropriate threat modeling and resource assumptions~\citep{carlini2022membership}, documented accuracy-privacy trade-offs~\citep{rezaei2023accuracy}, and highlighted scalability challenges preventing certified robustness deployment~\citep{li2023sok}.

\subsection{NDSS: Physemphasisesraints and Operational Realities}

NDSS research emphasizes system-level constraints, distributed training challengerigourd physical deployment realities. Contributions include documenting the gap betweanalysedal perturbations and physical deployment~\citep{jia2022physical}, developing appropriate metrics for non-image domains~\citep{kireev2023cost}, analyzing distributed training heterogeneity challenges~\citep{rieger2022deepsight}, and studying adversarial loops where defenses create exploitable side channels.

\subsection{USENIX Security: Real-World Validation and Domain Diversity}

USENIX Security research emphasizes testing on deployed systems and domain-specific constraints. The venue has documented tensions between theoretical rigor and deployment constraints~\citep{xiang2022patchcleanser, xiang2024patchcure}, analyzed trade-offs academia ignores (computational cost, regulatory compliance, usability)~\citep{ahmed2022keyword}, studied adversarial evolution when defenses meet adaptive attackers in production, and examined domain-specific requirements ignored by image-focused research~\citep{eykholt2023uret}.

\subsection{Attack versus Defense Papers}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig09_attack_vs_defense.png}
    \caption{Comparison of practicality metrics between attack-focused and defense-focused papers.}
    \label{fig:atk_def}
\end{figure}

Our analysis reveals that defense papers are slightly more practical than attack papers on average. Defense papers show 6\% real-system testing versus 5\% for attacks, 41\% gradient-free approaches versus 27\%, and 38\% black-box focus versus 31\%. However, both categories remain far from deployment readiness.

%==============================================================================
% 8. RECOMMENDATIONS
%==============================================================================
\section{Recommendations}

Bridging the theory-practice gap requires coordinated action across the research ecosystem.

\subsection{For Researchers}

\subsubsection{Immediate Actions}

\textbf{Test on real systems.} Partner with industry to evaluate against production deployments. Even limited real-world testing reveals constraints invisible in laboratory settings.

\textbf{Adopt realistic threat models.} Default to black-box access. When white-box assumptions are necessary, justify them explicitly and acknowledge limitations.

\textbf{Diversify domains.} Expand beyond image classification to tabular data, graphs, time-series, and multimodal systems where practical deployment is occurring.

\textbf{Analyze economics.} Quantify attack costs, defender resources, and ROI trade-offs. Security decisions are ultimately economic decisions.

\textbf{Conduct human-in-the-loop evaluation.} Test perceptibility against actual humans rather than relying solely on mathematical metrics.

\subsubsection{Methodological Improvements}

Evaluate privacy using true-positive rate at low false-positive rates, not average accuracy or AUC. For non-image domains, define domain-appropriate perturbation constraints. Report query budgets realistically based on what commercial APIs actually allow. Measure computational requirements in practical units like wall-clock time and dollar cost.

\subsection{For Venues and Program Committees}

\textbf{Require real-world validation.} At minimum, require authors to justify why real-system testing is infeasible if omitted.

\textbf{Create artifact badges for deployability.} Recognize work tested on production systems with explicit recognition.

\textbf{Develop an adversarial realism checklist.} Require papers to explicitly address threat model realism, query budget constraints, gradient requirements, domain-appropriate metrics, economic considerations, and adaptive defense testing.

\textbf{Encourage negative results.} Papers showing that attacks fail under realistic constraints provide valuable information that positive-result publication bias suppresses.

\subsection{For Industry Practitioners}

\textbf{Assume academic attacks overestimate threat severity.} Adjust risk assessments for deployment realities rather than accepting laboratory success rates.

\textbf{Assume academic defenses underestimate deployment costs.} Budget for substantial re-engineering before research prototypes become production-ready.

\textbf{Prioritize defenses against realistic threats:} black-box attacks rather than gradient-based ones, low query budget scenarios, economically motivated adversaries, and domain-specific constraints.

\textbf{Contribute data.} Anonymized logs of real attack attempts would ground academic research in deployment reality.

\subsection{For Funding Agencies}

Fund industry-academic partnerships with required real-world validation components. Support long-term deployment studies rather than just paper publication. Incentivize replication studies that validate academic claims against production systems. Create red team / blue team competitions with realistic constraints.

%==============================================================================
% 9. LIMITATIONS
%==============================================================================
\section{Limitations and Threats to Validity}

\subsection{Scope Limitations}

This review focuses on four security venues and may not capture patterns at ML conferences (NeurIPS, ICML, ICLR) where different norms may apply. Publication bias likely suppresses negative results - attacks that fail under realistic constraints or defenses that prove impractical are less likely to be published. Our 2022 to 2025 window may not capture longer-term trends.

\subsection{Coding Limitations}

The Gap Score reduces complex trade-offs to binary decisions, potentially oversimplifying nuanced situations. Manual coding introduces subjectivity, particularly for papers straddling categories. The definition of ``real system testing'' may vary - some papers test on emulated production environments that share some but not all deployment constraints.

\subsection{Generalizability}

Security venues may actually be more practice-focused than average computer science venues given their traditional emphasis on real-world threats. Our findings may therefore underestimate the theory-practice gap in the broader ML community.

%==============================================================================
% 10. CONCLUSION
%==============================================================================
\section{Conclusion}

Four years and 454 papers after ``Real Attackers Don't Compute Gradients,'' the adversarial machine learning research community has made insufficient progress toward practical relevance.

The core problem is structural: academic research optimizes for publication metrics - novelty, theoretical rigor, impressive attack success rates - rather than deployment viability. Incentive structures reward papers that advance the state of the art against previous papers rather than papers that translate to deployed defenses.

The quantitative evidence is stark:
\begin{itemize}[noitemsep]
    \item 94.7\% of papers never test on real systems
    \item 67.8\% require gradients that real attackers lack
    \item 80.4\% assume query budgets that real systems don't allow
    \item 63.2\% assume white-box access that real deployments don't provide
\end{itemize}

The qualitative evidence is equally concerning. $L_p$ norms don't predict human perception. Certified robustness doesn't scale to production models. Defenses ignore economic costs and usability constraints. Evaluation uses average-case metrics for worst-case properties.

The path forward requires structural changes. Venues must incentivize real-world validation. Researchers must default to realistic threat models. Industry must contribute deployment data. Funding must reward practical impact over novelty.

The gap between theory and practice in adversarial ML is not narrowing organically. Only deliberate, community-wide effort - changing publication incentives, evaluation standards, and collaboration models - can bridge it. The security of increasingly pervasive ML systems depends on the research community's willingness to prioritize practical impact over academic metrics.

%==============================================================================
% REFERENCES
%==============================================================================
\newpage
\bibliographystyle{unsrtnat}
\bibliography{ref}

%==============================================================================
% APPENDIX
%==============================================================================
\newpage
\appendix
\section{Complete Paper Analysis Dataset}

The complete analysis of all 454 papers is available in the supplementary CSV file: 
\texttt{all\_conferences\_analysis\_results\_2022\_2025.csv}

The dataset includes the following columns for each paper:
\begin{itemize}[noitemsep]
    \item Year, Conference, Filename, Title, Authors
    \item G1 (Focus), G2 (Attack Type), G3 (ML Type), G4 (Data Domain)
    \item G5 (Economics), G6 (Code Release), G7 (Real System Testing)
    \item T1 (Threat Model), T2 (Training Data Access)
    \item Q1 (Gradient Requirements), Q2 (Query Budget), Q3 (Computation)
    \item Gap indicator flags and Traditional Score
\end{itemize}

\noindent Benchmark meanings (aligned with the CSV fields used in the table):
\begin{itemize}[noitemsep]
    \item G1 Focus: atk (attack), def (defense), both.
    \item G2 Attack Type: Evasion, Poisoning, Privacy, Multiple.
    \item G3 ML Type: DL, Traditional, Both.
    \item G4 Data Domain: Images, Text, Audio, Malware, Other.
    \item G5 Economics mentioned: YES/NO.
    \item G6 Code released: YES/NO.
    \item G7 Real system testing: YES/NO.
    \item T1 Threat model: White-box, Gray-box, Black-box.
    \item T2 Training data access: Full, Partial, None.
    \item Q1 Requires gradients: YES/NO.
    \item Q2 Query budget: High (>1000), Low (<1000), None.
    \item Q3 Computation: High (GPU), Low (CPU).
    \item Traditional\_Score (Gap Score 0 - 6): sum of six impractical-assumption flags (higher = less practical).
\end{itemize}

For full per-paper details (all 24 columns), please see the CSV file. Below is an ultra-compact, bordered summary table focused on the most critical taxonomy fields. Binary fields are rendered as checkmarks (yes) or dashes (no); threat/access levels are abbreviated (W/B/G/P, F/P, H/L).

\begin{landscape}
\begin{tiny}
\setlength{\tabcolsep}{0.65pt}
\renewcommand{\arraystretch}{0.7}
\begin{longtable}{|p{0.7cm}|p{0.9cm}|p{1.4cm}|p{0.7cm}|p{1.0cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
\caption{Compact summary of papers (key taxonomy fields).}\label{tab:full_dataset_summary}\\
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endfirsthead
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endhead
\hline
\multicolumn{17}{r}{\small Continued on next page}\\
\hline
\endfoot
\hline
\endlastfoot
\csvreader[
    separator=semicolon,
    column count=24,
    late after line=\\\hline
]{all_conferences_analysis_results_clean.csv}{
Year=\Year,
Conference=\Conference,
Filename=\Filename,
Title=\Title,
Authors=\Authors,
G1=\GOne,
G2=\GTwo,
G3=\GThree,
G4=\GFour,
G5=\GFive,
G6=\GSix,
G7=\GSeven,
T1=\Tone,
T2=\Ttwo,
Q1=\Qone,
Q2=\Qtwo,
Q3=\Qthree,
Flag_Grad=\FGrad,
Flag_HighQ=\FHighQ,
Flag_WB=\FWB,
Flag_NoEcon=\FNoEcon,
Flag_NoCode=\FNoCode,
Flag_NoReal=\FNoReal,
Traditional_Score=\TScore
}{
\Year & \Conference & \StrBefore{\Authors}{ } & \GOne & \GTwo & \GThree & \GFour & \yn{\GFive} & \yn{\GSix} & \yn{\GSeven} & \threat{\Tone} & \access{\Ttwo} & \yn{\Qone} & \level{\Qtwo} & \level{\Qthree} & \yn{\FWB} & \TScore
}
\end{longtable}
\end{tiny}
\end{landscape}

\end{document}
