\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{csvsimple}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{pdflscape}
\usepackage{ltablex}
\usepackage{xstring}
\usepackage{newunicodechar}
\usepackage{amssymb}

% Unicode character mappings for the imported dataset table
\newunicodechar{‚àó}{*}
\newunicodechar{‚ãÜ}{*}
\newunicodechar{‚Ä†}{\textdagger}
\newunicodechar{‚Ä°}{\textdaggerdbl}
\newunicodechar{Ô¨Å}{fi}
\newunicodechar{Ô¨Ç}{fl}
\newunicodechar{≈£}{\c{t}}
\newunicodechar{ƒá}{\'c}
\newunicodechar{‚Ä¢}{\textbullet}
\newunicodechar{‚Ñé}{h}
\newunicodechar{‚à•}{\ensuremath{\parallel}}
\newunicodechar{‚âÄ}{\ensuremath{\wr}}
\newunicodechar{¬ß}{\S}
\newunicodechar{¬∂}{\P}
\newunicodechar{√ü}{\ss}
\newunicodechar{√†}{\`a}
\newunicodechar{√§}{\"a}
\newunicodechar{√®}{\`e}
\newunicodechar{√∂}{\"o}
\newunicodechar{√º}{\"u}
\newunicodechar{‚á§}{<-}
\newunicodechar{‚Äã}{} % zero-width non-joiner
% Math bold/italic letters normalized to ASCII
\newunicodechar{ùêµ}{B}
\newunicodechar{ùêª}{H}
\newunicodechar{ùëÜ}{S}
\newunicodechar{ùëä}{W}
\newunicodechar{ùëå}{Y}
\newunicodechar{ùëç}{Z}
\newunicodechar{ùëé}{a}
\newunicodechar{ùëî}{g}
\newunicodechar{ùëñ}{i}
\newunicodechar{ùëõ}{n}
\newunicodechar{ùëú}{o}
\newunicodechar{ùë¢}{u}
\newunicodechar{ùë≥}{L}
\newunicodechar{ùëµ}{N}
\newunicodechar{ùë∫}{S}
\newunicodechar{ùíÇ}{a}
\newunicodechar{ùíÉ}{b}
\newunicodechar{ùíÖ}{d}
\newunicodechar{ùíÜ}{e}
\newunicodechar{ùíâ}{h}
\newunicodechar{ùíä}{i}
\newunicodechar{ùíå}{k}
\newunicodechar{ùíç}{l}
\newunicodechar{ùíê}{o}
\newunicodechar{ùíì}{r}
\newunicodechar{ùíî}{s}
\newunicodechar{ùíö}{y}
\newunicodechar{ùüè}{1}
\newcommand{\yn}[1]{\IfStrEq{#1}{YES}{\checkmark}{\IfStrEq{#1}{NO}{\(\times\)}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\threat}[1]{\IfStrEq{#1}{White-box}{W}{\IfStrEq{#1}{Black-box}{B}{\IfStrEq{#1}{Gray-box}{G}{\IfStrEq{#1}{Partial}{P}{#1}}}}}
\newcommand{\access}[1]{\IfStrEq{#1}{Full}{F}{\IfStrEq{#1}{Partial}{P}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}
\newcommand{\level}[1]{\IfStrEq{#1}{High}{H}{\IfStrEq{#1}{Low}{L}{\IfStrEq{#1}{~}{\textit{n/a}}{#1}}}}

% Narrow column helper for wide tables
\newcolumntype{Y}{>{\centering\arraybackslash}p{0.9cm}}
\keepXColumns

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Line spacing
\onehalfspacing

% Title
\title{\textbf{Bridging the Gap Between Theory and Practice in Adversarial Machine Learning: A Systematic Cross-Venue Analysis of 454 Papers (2022--2025)}}

\author{
  Madhav Khanal\\
  Rollins College\\
  \texttt{mkhanal@rollins.edu}
  \and
  JJ Jasser\\
  Rollins College\\
  \texttt{jjasser@rollins.edu}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Machine learning systems are increasingly employed in security-critical contexts. However, difference still exists between academic work in adversarial machine learning studies and the requirements of practical applications in the wild. This systematic review examines 454 articles in four leading security conferences, namely ACM CCS, IEEE S\&P, NDSS, and USENIX Security, between 2022 and 2025. Using a conceptual structure developed in ``Real Attackers Don't Compute Gradients'' by Apruzzese et al., we assess the extent to which current studies have bridged the gap between theory and practice.
    
We found that most papers don't test on deployment systems in 94.7\% of studies, 67.8\% require rare gradient access in real-world settings, 80.4\% query budgets are beyond real-world constraints, and 63.2\% require white-box models. We identified five regions that require more focus for real-world deployment: model specifications, model access to gradients, validation on real systems, diversity regions beyond image classification, and economics and human-centric focus. Our study, from 2022 to 2025, verifies the gap within the identified regions. Our goal is to demonstrate how real-world attacks deviate from research in general while also acknowledging the significant contributions that each paper has made. We conclude with directions for researchers, conferences, and funding bodies to build upon the substantial contribution of prior research and foster research that bridges theoretical rigour with real-world security needs.
    
\textbf{Keywords:} adversarial machine learning, theory-practice gap, security research, systematic review, threat modeling
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
%==============================================================================
\section{Introduction}

Machine learning is becoming an important part of modern software systems. Applications range from face recognition systems at the border control entrance~\citep{grother2019frvt}, fraud analysis in the financial industry~\citep{dalpozzo2014fraud}, routing in self-driving cars~\citep{bojarski2016endtoend}, to moderation in social media sites~\citep{gillespie2018custodians}. Consequently, ML models affect decisions in systems that impact millions of users every day. This widespread deployment comes with the need for security and robustness against adversarial vulnerabilities, described in this paper as input or interaction crafted to cause models to behave in unintended and harmful ways~\citep{biggio2018wild}.

Adversarial machine learning (AML) studies attacks on machine learning systems and defences that aim to improve their security and strength~\citep{papernot2016sok}. In the last decade, researchers have shown various attacks within the machine learning pipeline. These include evasion attacks, which create harmful inputs that lead to misclassification during inference~\citep{szegedy2014intriguing,goodfellow2015explaining}, poisoning attacks, which insert flawed training data to disrupt how models behave~\citep{biggio2012poisoning,gu2017badnets}, and privacy attacks, which pull out sensitive information through model queries or inspections~\citep{shokri2017membership,carlini2021extracting}. In response to these attacks, a substantial body of studies has proposed defences designed to enhance robustness, privacy, and resistance to adversarial influence~\citep{madry2018towards,cohen2019certified}.

\textbf{Real World Scenarios}

Despite significant progress in research and theory being made, the consequences of adversarial vulnerabilities extend beyond academic benchmarks. For instance, in November 2025, Anthropic disclosed what it believed with high confidence to be the first large-scale state-sponsored cyber espionage campaign mostly carried out by an AI agent~\citep{anthropic2025espionage}. Attackers manipulated Claude Code to autonomously conduct reconnaissance, generate exploits, and exfiltrate data from around thirty big global targets. The system handled about 80--90\% of the operational workload with minimal human oversight. Notably, this incident did not depend on gradient-based attacks, sophisticated mathematics or white-box model access. Instead, it exploited deployment-level assumptions about agentic autonomy, tool access via the Model Context Protocol~\citep{anthropic2024mcp}, and circumvented safeguards through chunking cyberattacks into small ``innocent'' looking tasks~\citep{wei2024jailbroken}. This case illustrates a broader trend detailed in Section 3: real adversaries target system integration points, operational weaknesses, human workflows, and economic leverage rather than computing gradients or optimal $L_p$-bounded perturbations~\citep{gilmer2018notdetection,kurakin2018physical}.

The economic and social stakes are significant. Adversarial attacks lead to direct financial losses. For instance, deepfake-enabled fraud has resulted in documented losses of millions of dollars in individual cases~\citep{guardian2024deepfake}. Additionally, data extraction attacks can reveal valuable proprietary training data, leaking sensitive information or private user data~\citep{carlini2021extracting}. Organisations struggle to implement defences against adversarial attacks due to unclear ownership of machine learning security risks, a lack of tools for adversarial testing, and a limited understanding of realistic threat models~\citep{mink2023barriers,kumar2020adversarial}. Industry surveys show that many are unsure how to evaluate adversarial risks in operational systems~\citep{grosse2024threatmodels}. At the same time, regulatory frameworks are increasingly requiring robustness guarantees for high-stakes machine learning implementations~\citep{eu2021aiact}. This creates compliance costs and possible penalties. However, based on recent surveys with industry, practitioners often find that academic defences do not work well under operational constraints~\citep{arp2022dosdonts}.

\textbf{The Apruzzese Critique}

Prior work has revealed this gap. In 2022, Apruzzese et al published ``Real Attackers Don't Compute Gradients''~\citep{apruzzese2022real}. This analysis reveals a serious gap between academic research in adversarial machine learning (AML) and real-world security issues. They found that much of the existing literature assumes attackers have abilities that are rarely available in practice. These include white-box access, meaning complete knowledge of model architectures, parameters and training methods~\citep{carlini2019evaluating}, the ability to compute gradients through target models~\citep{papernot2017practical}, virtually unlimited query budgets~\citep{chen2017zoo}, and no monitoring or rate limiting~\citep{juuti2019prada}.

Through studies of actual machine learning attacks, they showed that real attackers are driven by economic incentives and usually use simpler methods. Since attackers have limited knowledge, budget and computation power, they often rely on basic input manipulation, exploiting weaknesses in the system, or social engineering when these tactics are cheap enough to meet their profitable goals. This critique raises an important question about whether academic research priorities match operational security needs.

\textbf{Research Questions}

Building upon this work, we conduct a systematic review to answer the following questions:
\begin{itemize}[noitemsep]
    \item \textbf{RQ1:} To what extent has Adversarial ML research addressed practical deployment constraints in recent years (2022--2025)?
    \item \textbf{RQ2:} What patterns distinguish research that bridges the theory-practice gap from purely laboratory-focused work?
    \item \textbf{RQ3:} How is this gap relevant to emerging threats in large language models and foundational models? How should future research address these new paradigms?
\end{itemize}

\textbf{Positioning in Prior Work}

This review builds on a growing body of meta-analyses evaluating the practical security research. Kumar et al~\citep{kumar2020adversarial} conducted interviews with 28 organisations and found that practitioners lack relevant tools and understanding for deploying robust machine learning systems. Grosse et al~\citep{grosse2024threatmodels} surveyed 271 industry practitioners and revealed systematic overestimation of realistic attacker capabilities by academic work. Arp et al~\citep{arp2022dosdonts} performed a meta-analysis of security research and documented reliance on simplified laboratory setups that don't account for adaptive adversaries. Apruzzese et al.~\citep{apruzzese2022real} justified these concerns through real-world attack case studies and revealed that real ML attacks rarely resemble academic threat models.

Our work expands on this topic in four ways. First, we offer the first systematic analysis across different venues during the post-Apruzzese period (2022--2025). We look at how recent research compares to the prior work. Second, we introduce a quantitative Gap Score framework that we apply consistently to 454 papers. This allows for statistical comparisons of research practices across different venues and over time. Third, we note the rise of new threat landscapes, especially agentic LLM misuse and multimodal attacks. These present challenges that differ from traditional computer vision issues. Fourth, we provide specific recommendations for four groups of stakeholders instead of general observations.

\textbf{Scope and Boundaries}

To address these questions, we analyse 454 adversarial ML papers published between 2022 and 2025 at four leading security venues: ACM CCS (118 papers), IEEE S\&P (79 papers), NDSS (49 papers), and USENIX Security (208 papers). We chose these venues because they are top security research outlets that emphasise real-world impact and practical relevance in their calls for papers and review criteria. The 2022 to 2025 timeframe captures the research community's response after critiques, giving us enough coverage to identify trends while keeping methodical consistency.

Our analysis does not include papers from general ML conferences (NeurIPS, ICML, ICLR), workshop-only publications, and preprints. This choice is intentional. Security-focused venues have different review criteria and author motivations than ML theory venues, where improving benchmark performance may take precedence over deployment concerns. Security conference review criteria stress realism in threat models, evaluation on operational systems, and consideration of attacker motivations. By focusing on security conferences, we look at research that appears to prioritise practical security impact, highlighting the theory--practice gap when it exists.

Each paper is evaluated using dimensions adapted from Apruzzese et al.~\citep{apruzzese2022real}. We consider threat model assumptions (white-box, gray-box, or black-box adversary knowledge~\citep{carlini2019evaluating}), reliance on gradient access (whether attacks require backpropagation through the target model), query budget needs (number of model inferences required), computational cost (GPU needs and training time), validation setting (evaluation on real production systems versus static benchmarks), and economic or organizational constraints (costs, incentives, and operational barriers). These dimensions are turned into a simple Gap Score (0 to 6), where 0 means full alignment with practical constraints, and 6 means reliance on all six idealised assumptions (detailed in Section 4).

\textbf{Preview of Key Findings}

Our analysis shows that the theory--practice gap identified in 2022 is still significant. Key findings include:
\begin{itemize}[noitemsep]
    \item \textit{Limited real-world evaluation:} Only 5.3\% of papers (24 out of 454) evaluate attacks or defences in deployed systems (operational environments with real users, monitoring, and operational constraints). The rest rely on offline datasets and simulated environments.
    \item \textit{Persistent gradient dependence:} 67.8\% of papers need gradient information from target models (access to model internals that enable backpropagation-based attacks), with a slight change from 2022 (68.2\%) to early 2025 (65.9\%).
    \item \textit{White-box dominance:} 63.2\% assume white-box adversaries with complete model knowledge, even though industry surveys show that such access is rare. In contrast, black-box setups (query-only access without model internals) better reflect most deployment situations.
    \item \textit{High query budgets:} Among papers involving model queries, 80.4\% assume budgets over 1000 queries, often overlooking cost limits, rate limiting, and anomaly detection systems.
    \item \textit{Average Gap Score of 3.17:} Most papers use about half of the idealised assumptions we track, with only 10.5\% scoring 0 to 1, indicating close alignment with deployment constraints.
\end{itemize}

Despite these gaps, we see some positive trends. Code release rates have gone up, improving reproducibility. New studies are looking into query-efficient and gradient-free methods. There is increasing focus on threats specific to LLMs, like jailbreaking and prompt injection, along with some movement beyond computer vision into text, audio, and malware detection.

\textbf{Contributions}

This review makes the following contributions:
\begin{itemize}[noitemsep]
    \item \textit{First post-Apruzzese cross-venue systematic analysis:} We offer the first comprehensive assessment of how 454 papers published across four leading security venues (ACM CCS, IEEE S\&P, NDSS, USENIX Security, 2022--2025) deal with the theory--practice gap in recent years.
    \item \textit{Quantitative Gap Score framework:} We introduce a simple coding framework with a six-dimensional Gap Score, allowing for statistical comparison of practical relevance across venues, years, and research focuses (attack versus defence). This extends earlier qualitative meta-analyses.
    \item \textit{Documentation of recent real-world adversarial incidents:} We summarise documented cases of ML attacks in deployed systems, including the first large-scale agentic AI cyber campaign, prompt injection vulnerabilities, and deepfake fraud, showing the sociotechnical nature of recent adversarial threats.
    \item \textit{Emergence analysis of the LLM threat landscape:} We describe how foundation models and agentic systems introduce new adversarial challenges like jailbreaking, prompt injection, and multimodal attacks that differ from traditional threat models.
    \item \textit{Actionable stakeholder-specific recommendations:} We offer targeted guidance for researchers (evaluation methods), conference organisers (review criteria), practitioners (deployment considerations), and funders (bridging infrastructure), based on our findings.
\end{itemize}

\textbf{Who This Review Serves}

This work serves several groups. This can help researchers who design new attacks/defences or benchmarks use our finding to make evaluation methods fit real world situations. Conference program committees and reviewers can refer to our quantitative analysis and improvise a criterion that further prioritises real-world constraints. Industry practitioners can use our gap model to assess whether proposed attacks/defences are relevant in the real world. This helps them avoid using methods that only work under ideal situations. Funding agencies can take into consideration our findings and encourage collaboration between academia and industry, set realistic goals, and build effective infrastructure. Overall, we hope this work inspires a growing body of adversarial machine learning research to evaluate their work for real-world scenarios and bridge the gap between theory and practice.


%==============================================================================
% 2. BACKGROUND
%==============================================================================
\section{Background: Foundations of Adversarial Machine Learning}

\subsection{The Discovery of Adversarial Vulnerabilities}

The modern study of adversarial machine learning began with the important work of Szegedy et al.~\citep{szegedy2013intriguing}. They showed that small, barely noticeable changes to input data could lead state-of-the-art deep learning models to produce wrong outputs with high confidence. These changes were not just random mistakes; they were systematic vulnerabilities that could transfer between different models. Adversarial examples created for one model often worked against other models trained separately. This ability to transfer highlighted the nature of the vulnerability and sparked further research.

Goodfellow et al.~\citep{goodfellow2014explaining} suggested that the linear behavior of neural networks, instead of their nonlinearity or overfitting, was the main cause of their vulnerability to adversarial attacks. They introduced the Fast Gradient Sign Method (FGSM), which made it easier to generate adversarial inputs using a single gradient step. The speed and effectiveness of FGSM set a standard for later research, as gradient-based attacks became the common approach.

These foundational studies shaped the direction of adversarial ML research in three significant ways. First, they assumed that adversaries had white-box access to models, including details about their architecture and gradient information. Second, they focused on imperceptible changes measured by specific distance metrics. Third, they highlighted optimization-based methods as the best way to generate adversarial inputs. While these frameworks allowed for thorough analysis, they also shifted research away from the practical realities of real-world attacks.

\subsection{The Arms Race: Attacks and Defenses}

After these initial findings, researchers came up with more advanced gradient-based attacks. Moosavi-Dezfooli et al.~\citep{moosavi2016deepfool} introduced DeepFool, which calculated minimal changes iteratively to cross decision boundaries with smaller, more accurate modifications than FGSM. Carlini and Wagner~\citep{carlini2017towards} approached the generation of adversarial examples as a complex optimization problem tailored to various distance metrics ($L_0$, $L_2$, and $L_\infty$), achieving high success rates against certain defenses, including defensive distillation~\citep{papernot2016distillation}.

This led to an arms race between attack and defense research. Defenses were proposed and tested against known attacks, only to be later bypassed by adaptive gradient-based methods. Carlini and Wagner~\citep{carlini2017detecting} systematically defeated ten detection-based defenses, showing that evaluations limited to known attacks fell short when adversaries adapted their strategies to account for defense mechanisms.

Adversarial training became a significant defense approach. Madry et al.~\citep{madry2018towards} redefined adversarial robustness through a min-max optimization framework, training models to perform well against the worst-case changes within set limits. Tramer et al.~\citep{tramer2017ensemble} introduced ensemble adversarial training to enhance black-box robustness. Research on certified robustness~\citep{li2020sok} aimed to provide formal guarantees that models would remain accurate within certain perturbation limits, although these methods often involved trade-offs between accuracy and robustness and required considerable computing power.

\subsection{Expanding Threat Landscape}

The field then expanded beyond evasion attacks to include privacy violations and threats during training. Membership inference attacks~\citep{shokri2017membership, ye2021enhanced} determine whether specific data points were part of model training. Carlini et al.~\citep{carlini2022membership} argued for assessing such attacks at low false positive rates, which reflect the serious nature of privacy issues.

Training-time attacks interfere with the learning process itself. Backdoor attacks introduce hidden triggers that cause specific misbehaviors when activated, with recent studies showing attacks on self-supervised learning~\citep{jia2021badencoder}, along with distribution-preserving methods that avoid detection~\citep{tao2024distribution}. Poisoning attacks that corrupt training data demonstrate that even small amounts of malicious samples can significantly harm model performance~\citep{tramer2022truth}.

Physical-world attacks took gradient-based techniques beyond digital settings. Kurakin et al.~\citep{kurakin2016adversarial} showed that adversarial examples remained effective when printed and photographed. Later work~\citep{jia2022physical} developed robust physical adversarial examples against traffic sign recognition systems in autonomous vehicles, raising questions about whether real-world attackers would use such complex methods when simpler options might work.

\subsection{The Theory-Practice Gap}

As the theoretical level of adversarial ML research increased, worries grew about whether this work addressed actual security needs in the real world. Kumar et al.~\citep{kumar2020adversarial} interviewed practitioners from 28 organizations, discovering that the industry lacked practical tools for dealing with adversarial ML threats. Grosse et al.~\citep{grosse2024practical} surveyed 271 industrial practitioners, revealing that while academic threat models were theoretically valid, research often overestimated what attackers could do, especially regarding access to training data and query limits.

Mink et al.~\citep{mink2023barriers} identified organizational barriers to deploying defenses through interviews with ML practitioners. These barriers included a lack of motivation from institutions, difficulty assessing AML risk, and competing business priorities. Practitioners often saw security as outside their expertise, revealing mismatches between job definitions in ML and security duties.

Apruzzese et al.~\citep{apruzzese2022real} crystallized these issues through real-world case studies showing that actual ML system breaches often involved simple tactics rather than complex gradient-based attacks: input manipulation without gradients, exploiting system weaknesses instead of model-specific faults, operational gaps, and social engineering. This historical trend encourages a systematic analysis of recent publications to see if current research is addressing the theory-practice gap or continuing past patterns.

\subsection{Types of Adversarial Attacks}

Adversarial attacks generally fall into three categories based on their goals:

\textbf{Evasion attacks} manipulate inputs during inference to cause misclassification. The attacker changes an input---such as an image, malware sample, or network packet---so that the model gives an incorrect prediction. These attacks focus on the inference phase and represent the most studied threat type.

\textbf{Poisoning attacks} corrupt the training process. By adding malicious examples to training data, attackers can lead models to learn wrong behaviors or establish backdoors---hidden triggers that cause specific failures when activated. A compromised model might classify most inputs correctly but struggle with ones containing a particular pattern.

\textbf{Privacy attacks} extract sensitive information. Membership inference attacks find out if specific persons were included in the training data. Model extraction attacks aim to replicate model functions through repeated queries. Data reconstruction attacks try to retrieve actual training examples.

\subsection{Threat Models and Their Implications}

A threat model outlines what an adversary can do and what they know. This aspect is a major area where academic research diverges from real-world usage.

\textbf{White-box access} suggests the attacker knows everything about the model: its architecture, parameters (weights), and possibly its training data. With white-box access, attackers can compute gradients---the mathematical changes needed to alter inputs and impact model outputs. Many academic attacks assume white-box access because it simplifies the process of optimizing attacks.

\textbf{Black-box access} means the attacker can only query the model and observe outputs. This situation is closer to real-world uses where models operate as web services or are embedded in applications. The attacker cannot see internal model details; they can only submit inputs and get predictions.

\textbf{Gray-box access} represents middle-ground scenarios. The attacker might know the model architecture but not the specific parameters or might have access to a similar training dataset.

Apruzzese et al. stress that real-world attackers rarely have white-box access~\citep{apruzzese2022real}. Production models usually have security measures in place. Yet, as our analysis shows, 63.2\% of papers in our dataset assume white-box access, which may not reflect what actually happens in deployment.

%==============================================================================
% 3. METHODOLOGY
%==============================================================================
\section{Methodology}

\subsection{Data Collection}

We systematically reviewed all papers with adversarial ML focus published at four top-tier security venues from 2022 through 2025. These venues were selected because they represent primary publication outlets for security-focused ML research and have historically shaped the field's direction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig01_dataset_overview.png}
    \caption{Dataset overview showing the distribution of 454 papers across four security conferences (ACM CCS, IEEE S\&P, NDSS, USENIX Security) from 2022 to 2025.}
    \label{fig:dataset}
\end{figure}

Our dataset comprises 454 papers: ACM CCS contributed 118 papers (26.0\%), IEEE S\&P contributed 79 papers (17.4\%), NDSS contributed 49 papers (10.8\%), and USENIX Security contributed 208 papers (45.8\%). The temporal distribution includes 89 papers from 2022, 136 from 2023, 198 from 2024, and 31 from 2025 (partial year at time of analysis).

\subsection{Coding Framework}

Each paper was evaluated across multiple dimensions designed to capture practical relevance:

\begin{itemize}[noitemsep]
    \item \textbf{Research Focus (G1):} Whether the paper primarily proposes attacks (60\%), defenses (39\%), or both (2\%).
    \item \textbf{Attack Type (G2):} Classification as evasion (48\%), poisoning (20\%), privacy (23\%), or multiple types (9\%).
    \item \textbf{Data Domain (G4):} The input modality: images (65\%), text (11\%), audio (7\%), malware (6\%), or other (12\%).
    \item \textbf{Threat Model (T1):} Assumed adversary access: white-box (63.2\%), black-box (34.1\%), or gray-box (2.6\%).
    \item \textbf{Gradient Requirements (Q1):} Whether the approach requires gradient access.
    \item \textbf{Query Budget (Q2):} High ($>$1000 queries), low, or none.
    \item \textbf{Real System Testing (G7):} Whether validation occurred on deployed systems.
    \item \textbf{Code Release (G6):} Whether code was released publicly.
\end{itemize}

\subsection{Gap Score Framework}

To quantify the theory-practice gap, we developed a 6-point ``Gap Score'' summing binary indicators of assumptions that may limit practical applicability:

\begin{enumerate}[noitemsep]
    \item Requires white-box access (vs. black/gray-box)
    \item Requires gradient computation
    \item Assumes high query budget ($>$1000 queries)
    \item Requires substantial computation (GPU-level resources)
    \item No testing on deployed systems
    \item No consideration of economic factors
\end{enumerate}

Higher scores indicate greater distance from practical deployment considerations. A paper scoring 0 would employ realistic threat models, require minimal resources, validate on production systems, and consider economic constraints. A paper scoring 6 would represent a primarily theoretical contribution with limited immediate applicability to deployed systems.

%==============================================================================
% 4. QUANTITATIVE FINDINGS
%==============================================================================
\section{Quantitative Findings}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig02_theory_practice_gap.png}
    \caption{Overview of the theory-practice gap showing the percentage of papers exhibiting each characteristic: no real-world testing (94.7\%), high query budget (80.4\%), gradient dependency (67.8\%), white-box access (63.2\%), and no code release (10.4\%).}
    \label{fig:gap_overview}
\end{figure}

Our analysis identifies several areas where research practices diverge from deployment realities. Figure~\ref{fig:gap_overview} summarizes the key indicators across all 454 papers.

\subsection{Real-World Validation}

The most pronounced finding concerns real-world validation: \textbf{only 5.3\% of papers (24 of 454) evaluate on actual deployed systems}. The remaining 94.7\% validate exclusively on research benchmarks, simulated environments, or research prototypes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig05_real_world_testing.png}
    \caption{Real-world testing rate: 5.3\% of papers validate on deployed systems.}
    \label{fig:real_world}
\end{figure}

This pattern has implications because deployment introduces constraints absent from controlled experiments. Production systems employ proprietary model formats and encryption. They integrate ML components into complex security pipelines where multiple components interact. They face hardware constraints, latency requirements, and regulatory compliance obligations. Research validated only in laboratory settings may require substantial adaptation for deployment.

The few papers that do validate on real systems often reveal differences between laboratory and field performance. Nayan et al.~\citep{nayan2024sok} conducted a systematic review of on-device ML model extraction attacks, finding that many proposed academic attacks proved difficult to reproduce, performed less effectively on production models, or introduced unacceptable computational and energy costs. Similarly, Layton et al.~\citep{layton2024sok} demonstrated that deepfake detection research employs metrics and dataset distributions that may lead to overestimation of detector efficacy.

Duan et al.~\citep{duan2022perception} validated perception-aware attacks against YouTube's copyright detection system, representing one of the rare examples of testing against commercial infrastructure. Their work demonstrated that attacks optimized in simulation required adaptation to succeed in practice.

\subsection{Gradient Dependency}

Apruzzese et al.'s critique centered on the observation that ``real attackers don't compute gradients.'' Our analysis indicates this pattern persists: \textbf{67.8\% of papers require gradient access}, despite this capability being unavailable against most production systems.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig04_gradient_dependency.png}
    \caption{Gradient dependency analysis showing limited change over time (68.2\% in 2022 to 65.9\% in 2025) and variation by attack type.}
    \label{fig:gradient}
\end{figure}

Temporal analysis reveals limited change in this dimension. Gradient dependency has remained relatively stable: 68.2\% in 2022, 69.1\% in 2023, 66.8\% in 2024, and 65.9\% in 2025. While individual papers have explored gradient-free approaches, the overall distribution has not shifted substantially.

Some research has pioneered gradient-free methods in specific domains. The Universal Robustness Evaluation Toolkit (URET) from Eykholt et al.~\citep{eykholt2023uret} formulates adversarial generation as a graph exploration problem, seeking sequences of domain-specific, functionality-preserving transformations rather than relying on differentiable feature spaces. This framework enables evaluation of systems processing inputs like malware binaries or tabular data where semantic and functional correctness must be maintained during perturbation.

Similarly, practical LLM jailbreak attacks demonstrate that effective attacks need not be gradient-based. Liu et al.~\citep{liu2024jailbreaking} and Yu et al.~\citep{yu2024jailbreak} showed that jailbreaking large language models can be effective even when executed via strategically crafted natural language prompts, illustrating the potential of accessible black-box attacks compared to complex gradient optimization.

\subsection{Threat Model Assumptions}

White-box access remains the predominant assumption: \textbf{63.2\% of papers assume adversaries have complete knowledge of model architecture and parameters}. Only 34.1\% consider black-box scenarios more closely matching deployment conditions, and 2.6\% examine gray-box settings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig03_threat_model.png}
    \caption{Distribution of threat model assumptions across all papers, showing prevalence of white-box assumptions.}
    \label{fig:threat_model}
\end{figure}

This pattern contrasts with industrial practice. As Grosse et al.~\citep{grosse2024practical} document, academic studies often operate under assumptions of attacker access---such as extensive access to internal models, parameters, or training data---that do not reflect the security controls present in production environments.

The foundational meta-analysis by Arp et al.~\citep{arp2022dos} identified methodological patterns in security research, including reliance on laboratory-only evaluation and deployment of threat models that may not account for adaptive adversaries. Our analysis suggests these patterns persist in the 2022--2025 literature.

\subsection{Query Budget Assumptions}

Even papers employing black-box threat models often assume substantial query access. \textbf{80.4\% of papers assume high query budgets ($>$1000 queries)}, which may be impractical when commercial APIs implement rate limiting, repeated queries trigger anomaly detection, and real-time constraints limit iterative optimization.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig06_query_budget.png}
    \caption{Query budget assumptions showing 80.4\% of papers assume high query budgets.}
    \label{fig:query}
\end{figure}

Some recent work has addressed query efficiency. HARDBEAT from Tao et al.~\citep{tao2023hardbeat} generates triggers requiring knowledge only of the final predicted label (hard-label) and minimal queries, addressing restrictions imposed by commercial services. BounceAttack from Wan et al.~\citep{wan2024bounceattack} demonstrates query-efficient decision-based attacks. However, these remain exceptions rather than the predominant approach.

\subsection{Domain Distribution}

Adversarial ML research exhibits notable domain concentration: \textbf{65\% of papers focus exclusively on image data}. Text receives 11\% of attention, audio 7\%, malware 6\%, with all other domains combined representing 12\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig08_data_domains.png}
    \caption{Data domain distribution revealing concentration on image classification in adversarial ML research.}
    \label{fig:domains}
\end{figure}

This concentration creates potential blind spots. Financial systems process tabular data where adversarial perturbations cannot be measured by pixel distances. As Kireev et al.~\citep{kireev2023cost} observe for fraud detection, the meaningful constraint is not visual imperceptibility but rather the quantifiable financial cost or utility an adversary must expend. $L_p$ norms may be less informative for tabular financial data; what matters is whether fraudulent transactions remain economically viable.

\subsection{Code Availability}

One dimension shows encouraging results: \textbf{89.6\% of papers release their code}. This represents substantial commitment to reproducibility and exceeds code release rates in many other fields.

However, this introduces a nuanced consideration: code designed for research datasets may not transfer directly to production environments without substantial re-engineering. High code release rates support reproducibility while not necessarily indicating deployment readiness.

\subsection{Gap Score Distribution}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig12_gap_score.png}
    \caption{Distribution of Gap Scores showing the typical paper (mean 3.17/6) incorporates roughly half of the measured assumptions.}
    \label{fig:gap_score}
\end{figure}

The mean Gap Score across all 454 papers is \textbf{3.17 out of 6}, indicating the typical paper incorporates approximately half of the assumptions we measured. The distribution peaks at scores of 3 to 4, with approximately 10.5\% of papers achieving scores of 0 to 1 that would indicate closer alignment with deployment considerations.

Conference-level analysis reveals similar patterns across venues: ACM CCS averages 3.04, NDSS 3.12, IEEE S\&P 3.18, and USENIX 3.25. No venue has established itself as substantially more practice-focused than others; the patterns we observe appear consistent across the security research community.

%==============================================================================
% 5. THEMATIC FINDINGS
%==============================================================================
\section{Thematic Findings}

Beyond quantitative metrics, thematic analysis across venues reveals structural patterns in how adversarial ML research is conducted and evaluated.

\subsection{The Utility-Robustness Trade-off}

A significant consideration for deployment is the relationship between security guarantees and system performance. Research from USENIX Security (2022--2025) particularly illuminates this tension.

Xiang et al.~\citep{xiang2022patchcleanser} found that defensive proposals achieving certifiable robustness against adversarial patches frequently yielded reduced clean classification accuracy, which may discourage real-world deployment. By 2024, Xiang et al.~\citep{xiang2024patchcure} documented that certifiably robust defenses in computer vision require 10 to 100 times more inference-time computation than undefended models, presenting computational challenges for practical deployment.

This pattern extends beyond vision systems. Ahmed et al.~\citep{ahmed2022keyword} found that defenses against malicious activations in voice assistants typically affect natural accuracy. Similarly, widely used privacy-preserving techniques like DP-SGD may compromise model utility to achieve privacy guarantees~\citep{liu2022mldoctor, tang2022membership}.

Some recent work demonstrates progress in addressing this trade-off. PatchCleanser introduced a double-masking approach compatible with any image classifier, achieving high certified robustness while preserving state-of-the-art clean accuracy~\citep{xiang2022patchcleanser}. MIST offers a pathway toward robust security by strategically limiting overfitting only to the most membership-vulnerable training instances~\citep{li2024mist}. CAMP training demonstrated that provable adversarial robustness in deep reinforcement learning need not sacrifice certified expected return, which may be valuable for safety-critical robotics applications~\citep{wang2025camp}.

\subsection{Evaluation Methodology}

Research across venues reveals considerations regarding how attacks and defenses are evaluated.

\subsubsection{Distance Metrics}

Standard evaluation measures adversarial perturbation size using $L_p$ norms---mathematical measures of distance between original and perturbed inputs. However, as Carlini et al.~\citep{carlini2022membership} document, conventional distance metrics like $L_p$ norms, which measure pixel-level differences, do not reliably predict whether humans perceive adversarial perturbations.

This consideration was empirically examined by the Avara framework~\citep{ma2024avara}, which used VR environments and eye-tracking to study whether drivers notice adversarial traffic signs. They found that $L_p$ norms do not reliably predict whether human drivers notice adversarial perturbations: attacks deemed ``imperceptible'' by mathematical standards may be immediately obvious to a driver, while attacks violating $L_p$ constraints might go unnoticed in realistic driving conditions.

\subsubsection{Privacy Evaluation}

Privacy attacks present particular evaluation challenges. Carlini et al.~\citep{carlini2022membership} observe that privacy is fundamentally a worst-case concern: a defense succeeds only if it protects all individuals, not just the majority. Yet membership inference attacks are typically evaluated using average-case metrics (overall accuracy, AUC) that may mask severe privacy leakage for specific individuals.

\subsection{Physical Deployment Considerations}

A consideration for conventional AML research concerns the gap between digital adversarial examples and the complex physical conditions governing real-world perception systems.

\subsubsection{Physical Attack Challenges}

Digital perturbations optimized in simulation may perform differently when deployed physically due to environmental factors: distance and viewing angle variations, illumination changes, sensor noise, and compression artifacts. NDSS research has particularly emphasized this consideration.

Jia et al.~\citep{jia2022physical} developed robust physical adversarial example pipelines tested against production autonomous vehicles running YOLO v5 traffic sign recognition. Their work required accounting for real-road conditions that simulation may not capture.

Physical attack research at USENIX has expanded beyond vision systems. Liu et al.~\citep{liu2023xadv} designed physically realizable 3D adversarial objects capable of deceiving X-ray prohibited item detection, requiring optimization for shape rather than color or texture and accounting for complex object overlap in luggage. Cao et al.~\citep{cao2023lidar} demonstrated Physical Removal Attacks using focused laser spoofing to selectively remove LiDAR point cloud data on autonomous vehicles. The ``Tubes Among Us'' research~\citep{ahmed2023tubes} demonstrated analog adversarial attacks where adversaries manipulate voice signals using simple tubes to bypass speaker recognition, effectively bypassing established digital artifact detection methods.

By 2025, physical attack research has embraced increasingly practical scenarios. ``Shadow Hack''~\citep{kobayashi2025shadow} exploits LiDAR weaknesses using ordinary non-reflective materials placed on roads, requiring no specialized equipment. ATKSCOPES~\citep{zhang2025atkscopes} demonstrates rapid evasion against real-world perceptual hashing algorithms by dynamically adapting to victim systems.

\subsubsection{System Integration}

Research increasingly recognizes that integrating ML models into complex systems introduces considerations that isolated model analysis may miss. Debenedetti et al.~\citep{debenedetti2024privacy} reveal that system-level components such as training data filters or output monitoring may introduce privacy side channels exploitable by adaptive adversaries, potentially affecting provable differential privacy guarantees.

Nasr et al.~\citep{nasr2025magika} demonstrated this through examining how exploiting Google's Magika file-type classifier affects Gmail's malware detection pipeline---a single non-robust component can affect an entire security pipeline.

Chen et al.~\citep{chen2023obsan} document that security mechanisms integrated solely at the framework level may not persist once models are compiled into optimized executables for deployment. Defenses may need to be embedded within the DL compiler pipeline to persist through deployment, a requirement often absent from academic work.

\subsection{Attack Evolution}

Despite methodological considerations about assumptions, research has progressively explored more practical attack vectors.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig07_attack_types.png}
    \caption{Attack type distribution showing evasion attacks (48\%) predominate, with trends over time.}
    \label{fig:attack_types}
\end{figure}

Early practical attacks (2022) demonstrated physically realizable approaches like the ``frustum attack,'' which leverages environmental context to compromise automotive sensor fusion in black-box settings~\citep{hallyburton2022frustum}.

By 2023--2024, attacks increasingly emphasized simplicity and accessibility. The effectiveness of jailbreaking LLMs through natural language prompts, even by users without ML expertise, illustrated the potential of accessible black-box attacks compared to complex gradient optimization.

The 2025 landscape shows attacks prioritizing stealth and persistence. MergeBackdoor~\citep{wang2025mergebackdoor} reveals supply chain considerations where seemingly benign upstream models pass security checks but activate malicious backdoors upon merging with other components. Guo et al.~\citep{guo2025persistent} describe persistent backdoor strategies targeting stable neuronal components, ensuring exploits survive continuous parameter updates in continual learning systems.

\subsection{Defense Evolution}

Defensive research has evolved from generic solutions toward specialized, interpretable, and context-aware mechanisms.

Blacklight~\citep{li2022blacklight} detected and mitigated black-box query-based attacks against MLaaS by leveraging the observation that iterative optimization produces highly similar queries, providing defense against persistent attackers that bypass account-based security measures.

Recent defenses demonstrate increased sophistication. JBShield~\citep{zhang2025jbshield} moves beyond heuristics for LLM protection by using the Linear Representation Hypothesis to identify and manipulate ``toxic'' and ``jailbreak'' concepts within hidden states. SafeSpeech~\citep{zhang2025safespeech} proactively affects voice data during training to make synthesized audio less usable, achieving robustness against voice cloning. DeBackdoor~\citep{popovic2025debackdoor} addresses deployment constraints by providing backdoor detection effective under black-box access, data scarcity, and pre-deployment inspection limitations.

\subsection{Economic and Organizational Factors}

Academic research has given limited attention to the economic and organizational dimensions of adversarial ML.

\subsubsection{Economic Considerations}

ACM CCS research has highlighted how commercial ML services create incentives for IP theft that may affect protective mechanisms. Cong et al.~\citep{cong2022sslguard} document that extracting pre-trained encoders may cost substantially less than training from scratch. Lu et al.~\citep{lu2024neural} show that Neural Dehydration can remove watermarks using less than 2\% of training data. The economic asymmetry---where model extraction attacks may cost orders of magnitude less than defenses or the assets they protect---remains an area requiring additional attention.

\subsubsection{Organizational Considerations}

Beyond technical considerations, qualitative research reveals organizational factors affecting industry adoption. Mink et al.~\citep{mink2023barriers} found that ML practitioners often lack institutional motivation and resources to understand and mitigate adversarial threats, frequently viewing security and machine learning as disconnected fields.

This organizational separation results in lower prioritization of adversarial evaluations before deployment and limited visibility into monitoring for active attacks. Defenses may remain unimplemented due to isolation between ML and security teams or because competing business priorities outweigh the cost and time needed for robust implementation.

%==============================================================================
% 6. TEMPORAL TRENDS
%==============================================================================
\section{Temporal Trends}

A central question motivates this review: has the research community responded to calls for greater practical relevance? Our temporal analysis provides insight into this question.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig11_yearly_trends.png}
    \caption{Temporal trends showing the theory-practice gap from 2022 to 2025, with limited change in key metrics.}
    \label{fig:trends}
\end{figure}

\subsection{Stable Patterns}

\textbf{Real-world testing} remains at approximately 5\% across all years. Despite calls for deployment validation, the research community has not substantially shifted toward testing on production systems.

\textbf{Gradient dependency} has remained relatively stable at 67--69\%. There has been limited movement toward gradient-free approaches at the aggregate level.

\textbf{White-box assumptions} show no substantial reduction (63\% in 2025 vs. 64\% in 2022). Threat model distributions have remained consistent.

\textbf{Domain distribution} persists with image data representing 65\% of papers throughout the study period.

\textbf{Economic analysis} remains below 11\% in all years.

\subsection{Areas of Modest Progress}

Query efficiency shows some progress, with high-budget assumptions declining from 80.4\% to approximately 76\% by 2025. Some researchers have developed query-efficient attacks, though these represent a minority of approaches.

Awareness of attacker knowledge limitations has increased, with growing acknowledgment that adversaries often lack full system knowledge. However, this awareness has not yet translated into substantial shifts in threat modeling practices.

\subsection{Emerging Research Areas}

The 2024--2025 period has seen growth in LLM security research, introducing new considerations. Jailbreak attacks evolve rapidly relative to RLHF defenses~\citep{shen2024llm}. Prompt injection against commercial LLM services poses practical concerns. Adversarial training, the standard defense approach, remains computationally intensive for broad deployment.

%==============================================================================
% 7. CROSS-VENUE ANALYSIS
%==============================================================================
\section{Cross-Venue Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig10_conference_comparison.png}
    \caption{Conference comparison heatmap showing similar patterns across all venues.}
    \label{fig:heatmap}
\end{figure}

While all venues share similar theory-practice gap patterns, each has developed distinctive emphases that collectively illuminate different facets of the research landscape.

\subsection{ACM CCS: Economics and System Integration}

ACM CCS research uniquely emphasizes business logic, usability constraints, and intellectual property protection. Contributions include demonstrating how economic incentives shape the threat landscape~\citep{cong2022sslguard, lu2024neural}, documenting relationships between mathematical metrics and human perception~\citep{ma2024avara}, analyzing system-level integration considerations~\citep{nasr2025magika}, and examining architectural constraints in emerging deployment patterns.

\subsection{IEEE S\&P: Efficiency and Formal Guarantees}

IEEE S\&P research emphasizes computational efficiency, privacy-utility trade-offs, and certified scalability. The venue has advanced understanding of threat modeling considerations and resource assumptions~\citep{carlini2022membership}, documented accuracy-privacy trade-offs~\citep{rezaei2023accuracy}, and examined scalability considerations for certified robustness deployment~\citep{li2023sok}.

\subsection{NDSS: Physical Constraints and Operational Realities}

NDSS research emphasizes system-level constraints, distributed training considerations, and physical deployment realities. Contributions include documenting gaps between digital perturbations and physical deployment~\citep{jia2022physical}, developing metrics for non-image domains~\citep{kireev2023cost}, analyzing distributed training heterogeneity~\citep{rieger2022deepsight}, and studying adversarial dynamics where defenses create exploitable side channels.

\subsection{USENIX Security: Real-World Validation and Domain Diversity}

USENIX Security research emphasizes testing on deployed systems and domain-specific constraints. The venue has documented tensions between theoretical rigor and deployment constraints~\citep{xiang2022patchcleanser, xiang2024patchcure}, analyzed trade-offs involving computational cost, regulatory compliance, and usability~\citep{ahmed2022keyword}, studied adversarial dynamics when defenses encounter adaptive attackers in production, and examined domain-specific requirements~\citep{eykholt2023uret}.

\subsection{Attack versus Defense Papers}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig09_attack_vs_defense.png}
    \caption{Comparison of practicality metrics between attack-focused and defense-focused papers.}
    \label{fig:atk_def}
\end{figure}

Our analysis reveals that defense papers demonstrate somewhat higher alignment with practical considerations than attack papers on average. Defense papers show 6\% real-system testing versus 5\% for attacks, 41\% gradient-free approaches versus 27\%, and 38\% black-box focus versus 31\%. However, both categories remain distant from deployment-ready research.

%==============================================================================
% 8. RECOMMENDATIONS
%==============================================================================
\section{Recommendations}

Our analysis reveals a persistent theory-practice gap: 94.7\% of papers avoid real-system testing, 67.8\% require gradient access, and 63.2\% assume white-box knowledge. Closing this gap requires coordinated intervention across the research ecosystem. We provide evidence-based recommendations for four stakeholder groups, grounded in recent frameworks and successful initiatives from the security and machine learning communities.

\subsection{For Researchers}

\subsubsection{Specify Realistic Threat Models}

The prevalence of white-box assumptions (63.2\% of papers) contradicts deployment realities where models are protected behind APIs and security controls. Researchers need structured vocabularies to specify attacker capabilities precisely. The NIST AI Risk Management Framework provides such a taxonomy, classifying attacks by attacker knowledge level, lifecycle stage, and objectives~\citep{nist2023airisk}. Importantly, NIST notes that most documented attacks require minimal system knowledge, contradicting the white-box default in academic work.

MITRE ATLAS extends the ATT\&CK framework to AI systems with 66 documented techniques and 33 real-world case studies~\citep{mitre2024atlas}. These case studies reveal that successful attacks, including the 2024 Morris II worm~\citep{nassi2024comworm} and the 2025 state-sponsored AI agent campaign~\citep{anthropic2025espionage}, succeeded without gradient access or white-box knowledge. Researchers should map proposed attacks to ATLAS techniques and justify when assumptions exceed documented attacker capabilities. When white-box access is necessary for theoretical contributions, authors should explicitly discuss the gap between their assumptions and deployment constraints.

\subsubsection{Employ Realistic Evaluation Frameworks}

Evaluation must enforce constraints that deployed systems actually face. For adversarial robustness in computer vision, AutoAttack has become the standard because it revealed that 13 of 50 evaluated defenses had actual robustness at least 10\% lower than initially reported~\citep{croce2020autoattack}. The ensemble includes gradient-free attacks, providing a model for combining white-box and black-box evaluation. RobustBench maintains standardized leaderboards with over 120 models, requiring non-zero gradients to prevent gradient masking~\citep{croce2021robustbench}.

Query budgets present another disconnect. Among papers in our dataset that involve model queries, 80.4\% assume budgets exceeding 1000 queries. Commercial APIs implement rate limiting, costs accumulate with usage, and repeated similar queries trigger anomaly detection. Query-efficient evaluation should become standard practice, with researchers reporting both the number of queries required and whether this budget reflects operational constraints.

For large language model security, standardized benchmarks are emerging. HarmBench provides 400 harmful behaviors across seven categories with explicit evaluation protocols~\citep{mazeika2024harmbench}, while JailbreakBench offers threat model specifications and scoring functions that enable reproducible comparison~\citep{chao2024jailbreakbench}. These benchmarks demonstrate how standardization can improve both rigor and practical relevance.

\subsubsection{Report Practical Attack Economics}

Single-attempt success rates misrepresent operational risk. Recent industry evaluations report success rates across multiple attempts (1, 50, 200), revealing dramatic increases with attacker persistence~\citep{anthropic2024modelevals}. The 2024 SaTML Capture-the-Flag competition, involving over 137,000 adversarial interactions against 44 defenses, found that every defense was eventually bypassed through multi-turn attacks~\citep{schulhoff2024satmlctf}. Single-turn evaluation would have missed this critical finding.

Researchers should report: (1) computational costs and query budgets required for attacks, (2) multi-turn success rates where applicable, (3) transferability across model families, and (4) economic costs to mount attacks. For domains beyond images, the constraints differ. Pierazzi et al. demonstrate how to formalize domain-specific constraints for malware, explicitly modeling semantic preservation and side effects~\citep{pierazzi2020intriguing}. Their work provides a template for extending adversarial ML beyond $L_p$ perturbations.

\subsection{For Conferences and Reviewers}

\subsubsection{Require Artifact Availability}

While 89.6\% of papers in our dataset release code, reproducibility requires more than source availability. USENIX Security now mandates that papers share artifacts on permanent repositories (Zenodo, FigShare, Software Heritage) or provide justification for non-release~\citep{usenix2025artifacts}. This policy establishes a baseline without excessive burden. All four surveyed venues offer three-tier badge systems (Available, Functional, Reproduced), but adoption varies. NDSS introduced artifact evaluation only in 2024, while USENIX and IEEE S\&P have established programs.

We recommend that security venues require artifact availability as a publication condition, with graduated evaluation for functionality and reproducibility. Permanent archival with DOI assignment ensures long-term accessibility. The REFORMS checklist provides 32 questions covering problem specification, data handling, and limitations that can guide both authors and reviewers~\citep{pineau2021reforms}.

\subsubsection{Evaluate Threat Model Realism}

Review criteria should explicitly address assumptions underlying adversarial claims. We propose that authors specify: (1) attacker knowledge using NIST taxonomy categories, (2) query budgets and whether they reflect rate limiting and detection, (3) physical realizability for attacks on deployed systems, (4) economic viability including cost-benefit analysis, and (5) adaptive adversary considerations for defense papers.

The SaTML conference welcomes position papers addressing methodological concerns~\citep{satml2025cfp}, creating space for research that advances evaluation methodology. Other venues should consider similar tracks. The AISec Workshop, co-located with CCS for 18 consecutive years, demonstrates sustained demand for work bridging ML and security~\citep{aisec2024}. Expanding industry review tracks with practitioner input can help identify when assumptions diverge from operational reality.

\subsection{For Practitioners}

\subsubsection{Implement Defense in Depth}

Academic papers typically evaluate single defenses in isolation. Deployed systems require layered protection because no single defense is perfect. Google's Secure AI Framework provides a structured approach with four pillars: secure development, deployment, execution, and monitoring~\citep{google2024saif}. Each pillar includes specific controls and risk assessments.

The OWASP Top 10 for LLM Applications establishes community consensus on priorities~\citep{owasp2025llmtop10}. Prompt injection tops the list because, unlike many academic attacks, it requires no model access or technical sophistication. The 2025 update for agentic systems adds risks specific to multi-agent architectures, including memory poisoning and tool manipulation~\citep{owasp2025agentic}. Practitioners should design systems assuming prompt injection will succeed, implementing architectural isolation that separates control logic from data processing~\citep{harang2024promptinjection}.

\subsubsection{Conduct Continuous Adversarial Testing}

Point-in-time security assessments miss evolving threats. Microsoft's PyRIT automates red teaming with curated attack datasets, enabling continuous evaluation~\citep{microsoft2024pyrit}. Open-source alternatives like Promptfoo and DeepEval provide similar capabilities with OWASP alignment~\citep{promptfoo2024harmbench,deepeval2024}. These tools can integrate into CI/CD pipelines, making adversarial testing part of standard development rather than a separate security audit.

Multi-turn attacks achieve over 90\% success against defenses showing near-zero vulnerability in single-turn evaluation~\citep{anthropic2024adaptive}. This finding underscores why continuous testing matters. Organizations should establish internal red teams that probe systems throughout the development lifecycle, not just before deployment.

\subsubsection{Contribute to Shared Threat Intelligence}

MITRE ATLAS contains 33 real-world case studies because practitioners documented incidents~\citep{mitre2024atlas}. Organizations that experience novel attacks should consider contributing to ATLAS or the AI Incident Database. The Coalition for Secure AI, with members including Google, Microsoft, Amazon, Anthropic, and OpenAI, provides venues for pre-competitive collaboration on supply chain security and risk governance~\citep{cosai2024}. Threat intelligence sharing benefits defenders more than attackers because defenses require comprehensive coverage while attacks need only find one vulnerability.

\subsection{For Funders and Policymakers}

\subsubsection{Require Deployment Validation}

DARPA's GARD program required scenario-based evaluations connecting robustness claims to operational contexts~\citep{darpa2024gard}. The program produced the Adversarial Robustness Toolbox and APRICOT benchmark, both designed for practical application. The successor SABER program explicitly targets the "practical demonstration gap" by funding both research teams and operational assessment teams~\citep{darpa2024saber}.

The NSF National AI Research Institutes program awards 16 to 20 million dollars over four to five years, sufficient for sustained work rather than isolated projects~\citep{nsf2022aiinstitutes}. Theme 6 on AI and cybersecurity requires integration of fundamental advances with practical security problems. This structure provides a model: funding should prioritize proposals that include deployment validation components, partnerships with operational organizations, and plans for transitioning research to practice.

\subsubsection{Support Evaluation Infrastructure}

Individual research groups cannot each build comprehensive evaluation platforms. Shared infrastructure reduces duplication and enables fair comparison. NIST's Dioptra testbed provides open-source infrastructure for evaluating AI security~\citep{nist2024dioptra}. Its modular design allows researchers to swap datasets, models, attacks, and defenses. The UK AI Safety Institute's Systemic Safety Grants Programme funds infrastructure for evaluating deepfakes, misinformation, and system failures~\citep{ukaisafety2024grants}.

The SPHERE Research Infrastructure, used by NDSS for artifact evaluation, demonstrates how standardized testbeds enable reproducibility while reducing reviewer burden~\citep{sphere2024}. Funders should support such shared platforms rather than expecting each institution to develop parallel infrastructure. This investment pays dividends through improved reproducibility and reduced barriers to entry for new researchers.

\subsubsection{Address Critical Gaps}

Our analysis identifies three underfunded areas. First, human factors research represents only 0.09\% of papers despite growing deployment in human-AI systems~\citep{yu2025humanfactors}. Attacks exploiting human cognition, timing, and trust may be more practical than mathematical perturbations. Second, economic analysis remains limited despite Apruzzese et al.'s argument that cost-driven threat modeling is essential~\citep{apruzzese2022real}. Recent work provides foundations~\citep{grosse2024economics}, but domain-specific models for fraud, malware, and other applications require development. Third, multi-turn and agentic evaluation lags behind deployment~\citep{xu2025agentsurvey}. Systems combining multiple models, tools, and external services demand compositional security analysis that current benchmarks do not provide.

\subsubsection{Align Regulation with Research}

The EU AI Act, fully applicable in August 2026, requires high-risk systems to be "accurate, robust, and cybersecurity-resilient" with protection against adversarial attacks~\citep{euaiact2024}. This creates demand for evaluation methodologies that translate academic benchmarks into compliance-ready protocols. The Act's regulatory sandboxes provide testing environments for this translation. Similarly, NIST's AI Risk Management Framework establishes governance structures that organizations use for risk assessment. The forthcoming Control Overlays for Securing AI Systems will adapt security controls specifically for AI vulnerabilities~\citep{nist2024cosais}.

Regulation can drive practical research when compliance requires demonstrated robustness. Funders should support work that bridges academic benchmarks and regulatory requirements, ensuring that compliance obligations incentivize rather than hinder security research.

\subsection{Path Forward}

The theory-practice gap we document is not inevitable. Infrastructure for practical adversarial ML research exists through standardized frameworks (NIST AI RMF, MITRE ATLAS), evaluation tools (AutoAttack, RobustBench, HarmBench), shared testbeds (Dioptra, SPHERE), and industry collaboration venues (CoSAI). The challenge is adoption. Researchers can choose realistic threat models, conferences can require artifact availability and threat model justification, practitioners can implement defense in depth, and funders can prioritize deployment validation. These actions are concrete and achievable. Together, they can ensure that adversarial ML research addresses both theoretical rigor and operational security needs, building on the substantial foundation that existing work has established while directing future effort toward bridging the remaining gaps.
%==============================================================================
% 9. LIMITATIONS
%==============================================================================
\section{Limitations and Threats to Validity}

\subsection{Scope Limitations}

This review focuses on four security venues and may not capture patterns at ML conferences (NeurIPS, ICML, ICLR) where different norms may apply. Publication preferences may suppress negative results; attacks that fail under realistic constraints or defenses that prove impractical are less likely to be published. Our 2022--2025 window may not capture longer-term trends.

\subsection{Coding Limitations}

The Gap Score reduces complex trade-offs to binary decisions, potentially oversimplifying nuanced situations. Manual coding introduces subjectivity, particularly for papers spanning multiple categories. The definition of ``real system testing'' may vary; some papers test on emulated production environments that share some but not all deployment constraints.

\subsection{Generalizability}

Security venues may actually be more practice-focused than typical computer science venues given their traditional emphasis on real-world threats. Our findings may therefore underestimate the theory-practice gap in the broader ML research community.

%==============================================================================
% 10. CONCLUSION
%==============================================================================
\section{Conclusion}

This systematic review involving 454 research papers published from 2022--2025 at four prestigious security conferences bears evidence of some beneficial contributions by the research community of adversarial machine learning studies in spite of the theory-practice gap noticed by Apruzzese et al.

The quantitative trends are striking:
\begin{itemize}[noitemsep]
    \item 94.7\% of the papers don't test on real systems
    \item 67.8\% need gradients which might not be accessible in real life
    \item 80.4\% assume query budgets that could exceed deployment limits
    \item 63.2\% assume white-box access that could not be provided in deployment situations
\end{itemize}

Thematic analysis shows other considerations emerging. $L_p$ norms may not model human perception well. Robustness may not generalize to production-quality settings. Defense systems do not necessarily take into consideration economic losses and use ease-of-use constraints. Assessment metrics are often averages for the worst-case scenario characteristics.

Among the many contributions of the research community is the theoretical foundation and pioneering research in adversarial phenomena. However, to move forward, accomplishing the goal of closing the theory-practice gap will necessitate structural changes in conferences that reward verification of effectiveness in the real world, assumptions of reasonable threats in research, industry inputs of deployment results, and distribution of funding that emphasizes effectiveness along with innovation.

However, the efficacy of ever more widespread ML-dependent systems is contingent on the ongoing involvements of the community of ML researchers with both the theory and the practicalities. Our observations indicate that those practicalities can, through shifts within publishing, evaluation, and collaboration, help ensure that community-driven ML adversarial research and development work increasingly serves a practical, security context.

%==============================================================================
% REFERENCES
%==============================================================================
\newpage
\bibliographystyle{unsrtnat}
\bibliography{ref}

%==============================================================================
% APPENDIX
%==============================================================================
\newpage
\appendix
\section{Complete Paper Analysis Dataset}

The complete analysis of all 454 papers is available in the supplementary CSV file: 
\texttt{all\_conferences\_analysis\_results\_2022\_2025.csv}

The dataset includes the following columns for each paper:
\begin{itemize}[noitemsep]
    \item Year, Conference, Filename, Title, Authors
    \item G1 (Focus), G2 (Attack Type), G3 (ML Type), G4 (Data Domain)
    \item G5 (Economics), G6 (Code Release), G7 (Real System Testing)
    \item T1 (Threat Model), T2 (Training Data Access)
    \item Q1 (Gradient Requirements), Q2 (Query Budget), Q3 (Computation)
    \item Gap indicator flags and Traditional Score
\end{itemize}

\noindent Benchmark meanings (aligned with the CSV fields used in the table):
\begin{itemize}[noitemsep]
    \item G1 Focus: atk (attack), def (defense), both.
    \item G2 Attack Type: Evasion, Poisoning, Privacy, Multiple.
    \item G3 ML Type: DL, Traditional, Both.
    \item G4 Data Domain: Images, Text, Audio, Malware, Other.
    \item G5 Economics mentioned: YES/NO.
    \item G6 Code released: YES/NO.
    \item G7 Real system testing: YES/NO.
    \item T1 Threat model: White-box, Gray-box, Black-box.
    \item T2 Training data access: Full, Partial, None.
    \item Q1 Requires gradients: YES/NO.
    \item Q2 Query budget: High ($>$1000), Low ($<$1000), None.
    \item Q3 Computation: High (GPU), Low (CPU).
    \item Traditional\_Score (Gap Score 0--6): sum of six impractical-assumption flags (higher = less practical).
\end{itemize}

For full per-paper details (all 24 columns), please see the CSV file. Below is a compact, bordered summary table focused on the most critical taxonomy fields. Binary fields are rendered as checkmarks (yes) or blanks (no); threat/access levels are abbreviated (W/B/G/P, F/P, H/L).

\begin{landscape}
\begin{tiny}
\setlength{\tabcolsep}{0.65pt}
\renewcommand{\arraystretch}{0.7}
\begin{longtable}{|p{0.7cm}|p{0.9cm}|p{1.4cm}|p{0.7cm}|p{1.0cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
\caption{Compact summary of papers (key taxonomy fields).}\label{tab:full_dataset_summary}\\
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endfirsthead
\hline
Year & Venue & Paper (1st author) & G1 & G2 & G3 & G4 & G5 & G6 & G7 & T1 & T2 & Grad & Qry & Comp & WB & Trad. \\
\hline
\endhead
\hline
\multicolumn{17}{r}{\small Continued on next page}\\
\hline
\endfoot
\hline
\endlastfoot
\csvreader[
    separator=semicolon,
    column count=24,
    late after line=\\\hline
]{all_conferences_analysis_results_clean.csv}{
Year=\Year,
Conference=\Conference,
Filename=\Filename,
Title=\Title,
Authors=\Authors,
G1=\GOne,
G2=\GTwo,
G3=\GThree,
G4=\GFour,
G5=\GFive,
G6=\GSix,
G7=\GSeven,
T1=\Tone,
T2=\Ttwo,
Q1=\Qone,
Q2=\Qtwo,
Q3=\Qthree,
Flag_Grad=\FGrad,
Flag_HighQ=\FHighQ,
Flag_WB=\FWB,
Flag_NoEcon=\FNoEcon,
Flag_NoCode=\FNoCode,
Flag_NoReal=\FNoReal,
Traditional_Score=\TScore
}{
\Year & \Conference & \StrBefore{\Authors}{ } & \GOne & \GTwo & \GThree & \GFour & \yn{\GFive} & \yn{\GSix} & \yn{\GSeven} & \threat{\Tone} & \access{\Ttwo} & \yn{\Qone} & \level{\Qtwo} & \level{\Qthree} & \yn{\FWB} & \TScore
}
\end{longtable}
\end{tiny}
\end{landscape}

\end{document}