\documentclass[11pt]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{array}

\title{Paper Coding Instructions}
\date{}

\begin{document}
\maketitle

\section{How to Select Papers}

You do NOT randomly pick papers. You work backwards from industry artifacts.

\subsection{Step 1: Go to Each Artifact and Find Papers They Cite}

\begin{tabular}{p{3cm}p{4.5cm}p{5cm}}
\toprule
\textbf{Artifact} & \textbf{Where to Look} & \textbf{What to Extract} \\
\midrule
CleverHans & GitHub README, docstrings & Paper citations for each attack \\
IBM ART & Documentation, attacks/ folder & Paper citations for each technique \\
Foolbox & Docs, source code & Paper citations \\
TextAttack & Docs, source code & Paper citations \\
PyRIT & Docs, source code & Paper citations \\
\midrule
RobustBench & Leaderboard entries & Defense papers on leaderboard \\
AutoAttack & The AutoAttack paper & Component attack papers \\
HarmBench & HarmBench paper/GitHub & Red-teaming method papers \\
\midrule
MITRE ATLAS & atlas.mitre.org technique pages & ``References'' on each page \\
NIST AI RMF & The PDF document & Bibliography \\
OWASP LLM Top 10 & The PDF & References for each vulnerability \\
\bottomrule
\end{tabular}

\subsection{Step 2: For Each Paper You Find, Record It}

If CleverHans implements FGSM and cites Goodfellow et al.\ 2015, that paper goes in your dataset.

\textbf{Your $\sim$100 papers = all papers cited by these artifacts.}

\section{The 12 Coding Columns}

\subsection{Group 1: Basic Info (G1--G7)}

\begin{longtable}{p{1cm}p{3.5cm}p{3.5cm}p{4.5cm}}
\toprule
\textbf{Col} & \textbf{Question} & \textbf{Options} & \textbf{How to Decide} \\
\midrule
\endhead
G1 & Is this an attack, defense, or evaluation? & Attack / Defense / Evaluation & Read abstract. What did they build? \\
\midrule
G2 & What type of attack? & Evasion / Poisoning / Privacy / N/A & \textbf{Evasion} = fool model at test time. \textbf{Poisoning} = corrupt training data. \textbf{Privacy} = steal data/model. \textbf{N/A} = defense papers \\
\midrule
G3 & What domain? & Vision / NLP / Malware / Audio / Tabular / LLM / Cross-domain & What data did they test on? ImageNet = Vision. Text = NLP. ChatGPT = LLM. \\
\midrule
G4 & Where published? & ML / Security / Journal / arXiv-only & \textbf{ML} = NeurIPS, ICML, ICLR, CVPR, ACL. \textbf{Security} = S\&P, CCS, USENIX, NDSS. \\
\midrule
G5 & Is code available NOW? & Yes / No & Google ``paper name github''. Is there code? \\
\midrule
G6 & When was code released? & At-pub / Post-pub / Never & \textbf{At-pub} = within 1 month of paper. \textbf{Post-pub} = later. \\
\midrule
G7 & Publication year & 2014--2025 & Year of first public version \\
\bottomrule
\end{longtable}

\subsection{Group 2: Threat Model (T1--T2) --- Attack Papers Only}

\begin{tabular}{p{1cm}p{3.5cm}p{3cm}p{5cm}}
\toprule
\textbf{Col} & \textbf{Question} & \textbf{Options} & \textbf{How to Decide} \\
\midrule
T1 & How much model access? & White / Gray / Black & \textbf{White} = has weights/gradients. \textbf{Gray} = surrogate model. \textbf{Black} = queries only \\
\midrule
T2 & Uses gradients? & Yes / No & If they compute $\nabla L$ anywhere, it's Yes \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Leave T1 and T2 blank for defense papers.}

\subsection{Group 3: Practical Evaluation (Q1--Q3)}

\begin{tabular}{p{1cm}p{3.5cm}p{3cm}p{5cm}}
\toprule
\textbf{Col} & \textbf{Question} & \textbf{Options} & \textbf{How to Decide} \\
\midrule
Q1 & Tested on real system? & Yes / Partial / No & \textbf{Yes} = Google API, Tesla, ChatGPT. \textbf{Partial} = realistic sim. \textbf{No} = CIFAR/ImageNet only \\
\midrule
Q2 & Reported cost? & Yes / No & Did they say ``X queries'' or ``Y seconds''? \\
\midrule
Q3 & Tested against defenses? & Yes / No / N/A & \textbf{Yes} = tested vs.\ adversarial training. \textbf{N/A} = for defense papers \\
\bottomrule
\end{tabular}

\section{Adoption Tracking}

For each paper, also record:

\begin{tabular}{p{3.5cm}p{9cm}}
\toprule
\textbf{Column} & \textbf{What It Means} \\
\midrule
Artifact & Which artifact cites this paper (e.g., ``CleverHans'', ``MITRE ATLAS'') \\
Artifact Type & Tool / Benchmark / Regulatory / Vendor \\
Adoption Date & When did artifact add this? (Git commit date or document date) \\
Adoption Lag & Months between paper publication and artifact adoption \\
\bottomrule
\end{tabular}

\section{Concrete Example: FGSM Paper}

\textbf{You find:} CleverHans GitHub cites ``Explaining and Harnessing Adversarial Examples'' (Goodfellow 2015) for FGSM.

\textbf{You code:}

\begin{tabular}{lll}
\toprule
\textbf{Column} & \textbf{Value} & \textbf{Why} \\
\midrule
G1 & Attack & Paper proposes an attack \\
G2 & Evasion & Fools classifier at test time \\
G3 & Vision & Tested on MNIST, ImageNet \\
G4 & ML & Published at ICLR \\
G5 & Yes & Code exists on GitHub \\
G6 & At-pub & Released with paper \\
G7 & 2015 & Published 2015 \\
\midrule
T1 & White & Uses target model gradients \\
T2 & Yes & Gradient-based attack \\
\midrule
Q1 & No & Only tested on benchmarks \\
Q2 & No & No runtime/query count reported \\
Q3 & No & No defense evaluation \\
\midrule
Artifact & CleverHans & Where you found it \\
Artifact Type & Tool & It's a library \\
Adoption Date & Oct 2016 & First CleverHans commit with FGSM \\
Adoption Lag & 22 months & Oct 2016 $-$ Dec 2014 \\
\bottomrule
\end{tabular}

\section{Your Workflow}

\begin{enumerate}
    \item \textbf{Pick an artifact} (e.g., IBM ART)
    \item \textbf{Find all papers it cites} (from docs, code, README)
    \item \textbf{For each paper:} fill in G1--G7, T1--T2, Q1--Q3, plus adoption info
    \item \textbf{Repeat} for all 11 artifacts
    \item \textbf{Remove duplicates} (same paper cited by multiple artifacts = one row, multiple adoption events)
\end{enumerate}

\section{Spreadsheet Columns}

\begin{verbatim}
paper_id, title, authors, venue, pub_date, technique_name,
G1, G2, G3, G4, G5, G6, G7, T1, T2, Q1, Q2, Q3,
artifact_name, artifact_type, adoption_date, adoption_lag_months
\end{verbatim}

\end{document}
