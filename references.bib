@inproceedings{apruzzese2023real,
  title={"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice},
  author={Apruzzese, G. and Anderson, H. S. and Dambra, S. and Freeman, D. E. and Pierazzi, F. and Roundy, K.},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  year={2023},
  doi={10.1109/SATML54575.2023.00031}
}

@inproceedings{bagdasaryan2022spinning,
  title={Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures},
  author={Bagdasaryan, E. and Shmatikov, V.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2022},
  doi={10.1109/SP46214.2022.9833572}
}

@inproceedings{balle2022reconstructing,
  title={Reconstructing Training Data with Informed Adversaries},
  author={Balle, B. and Cherubin, G. and Hayes, J.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2022},
  doi={10.1109/SP46214.2022.9833677}
}

@inproceedings{carlini2022membership,
  title={Membership Inference Attacks From First Principles},
  author={Carlini, N. and Chien, S. and Nasr, M. and Song, S. and Terzis, A. and Tramèr, F.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2022},
  doi={10.1109/SP46214.2022.9833649}
}

@article{carlini2023extracting,
  title={Extracting Training Data from Diffusion Models},
  author={Carlini, N. and Hayes, J. and Nasr, M. and Jagielski, M. and Sehwag, V. and Tramèr, F. and Balle, B. and Ippolito, D. and Wallace, E.},
  journal={ArXiv.Org},
  year={2023},
  doi={10.48550/ARXIV.2301.13188}
}

@inproceedings{carlini2017adversarial,
  title={Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  author={Carlini, N. and Wagner, D.},
  booktitle={ACM Workshop on Artificial Intelligence and Security},
  year={2017},
  doi={10.1145/3128572.3140444}
}

@inproceedings{carlini2017towards,
  title={Towards Evaluating the Robustness of Neural Networks},
  author={Carlini, N. and Wagner, D.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2017},
  doi={10.1109/SP.2017.49}
}

@article{chahe2023dynamic,
  title={Dynamic Adversarial Attacks on Autonomous Driving Systems},
  author={Chahe, A. and Wang, C. and Jeyapratap, A. and Xu, K. and Zhou, L.},
  journal={ArXiv.Org},
  year={2023},
  doi={10.48550/ARXIV.2312.06701}
}

@inproceedings{chase2021property,
  title={Property Inference from Poisoning},
  author={Chase, M. and Ghosh, E. and Mahloujifar, S.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2021},
  doi={10.1109/SP46214.2022.9833623}
}

@article{chaudhari2022snap,
  title={SNAP: Efficient Extraction of Private Properties with Poisoning},
  author={Chaudhari, H. and Abascal, J. and Oprea, A. and Jagielski, M. and Tramèr, F. and Ullman, J.},
  journal={ArXiv.Org},
  year={2022},
  doi={10.48550/ARXIV.2208.12348}
}

@inproceedings{goodfellow2014explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, I. and Shlens, J. and Szegedy, C.},
  booktitle={International Conference on Learning Representations},
  year={2014}
}

@inproceedings{goodfellow2015explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, I. and Shlens, J. and Szegedy, C.},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@article{grosse2017statistical,
  title={On the (Statistical) Detection of Adversarial Examples},
  author={Grosse, K. and Manoharan, P. and Papernot, N. and Backes, M. and McDaniel, P.},
  journal={ArXiv: Cryptography and Security},
  year={2017}
}

@inproceedings{he2023large,
  title={Large Language Models for Code: Security Hardening and Adversarial Testing},
  author={He, J. and Vechev, M.},
  booktitle={Conference on Computer and Communications Security},
  year={2023},
  doi={10.1145/3576915.3623175}
}

@inproceedings{he2015deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},
  booktitle={Computer Vision and Pattern Recognition},
  year={2015},
  doi={10.1109/CVPR.2016.90}
}

@inproceedings{he2016deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year={2016},
  doi={10.1109/CVPR.2016.90}
}

@inproceedings{huang2011adversarial,
  title={Adversarial Machine Learning},
  author={Huang, L. and Joseph, A. D. and Nelson, B. and Rubinstein, B. I. P. and Tygar, J. D.},
  booktitle={Security and Artificial Intelligence},
  year={2011},
  doi={10.1145/2046684.2046692}
}

@inproceedings{huang2019adversarial,
  title={Adversarial Machine Learning},
  author={Huang, L. and Joseph, A. D. and Nelson, B. and Rubinstein, B. I. P. and Tygar, J. D.},
  booktitle={Security and Artificial Intelligence},
  year={2019},
  doi={10.1145/2046684.2046692}
}

@article{jia2021badencoder,
  title={BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning},
  author={Jia, J. and Liu, Y. and Gong, N. Z. and Yupei, Gong and Zhenqiang, N.},
  journal={ArXiv (Cornell University)},
  year={2021},
  doi={10.48550/ARXIV.2108.00352}
}

@inproceedings{jia2022fooling,
  title={Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems},
  author={Jia, W. and Lu, Z. and Zhang, H. and Liu, Z. and Wang, J. and Qu, G.},
  booktitle={Proceedings 2022 Network and Distributed System Security Symposium},
  year={2022},
  doi={10.14722/NDSS.2022.24130}
}

@inproceedings{kumar2020adversarial,
  title={Adversarial Machine Learning-Industry Perspectives},
  author={Kumar, R. S. S. and Nyström, M. and Lambert, J. and Marshall, A. and Goertzel, M. and Comissoneru, A. and Swann, M. and Xia, S.},
  booktitle={2020 IEEE Security and Privacy Workshops (SPW)},
  year={2020},
  doi={10.2139/SSRN.3532474}
}

@inproceedings{kurakin2016adversarial,
  title={Adversarial Machine Learning at Scale},
  author={Kurakin, A. and Goodfellow, I. and Bengio, S.},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{li2020sok,
  title={SoK: Certified Robustness for Deep Neural Networks},
  author={Li, L. and Li, L. and Xie, T. and Li, B.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2020},
  doi={10.1109/SP46215.2023.10179303}
}

@article{li2024analyzing,
  title={Analyzing Inference Privacy Risks Through Gradients in Machine Learning},
  author={Li, Z. and Lowy, A. M. and Liu, J. and Koike‐Akino, T. and Parsons, K. and Malin, B. and Wang, Y.},
  journal={ArXiv.Org},
  year={2024},
  doi={10.48550/ARXIV.2408.16913}
}

@article{liu2024precurious,
  title={PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps},
  author={Liu, R. and Wang, T. and Cao, Y. and Xiong, L.},
  journal={ArXiv.Org},
  year={2024},
  doi={10.48550/ARXIV.2403.09562}
}

@inproceedings{madry2017towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Mądry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
  journal={ArXiv (Cornell University)},
  year={2018}
}

@inproceedings{meng2017magnet,
  title={MagNet: A Two-Pronged Defense against Adversarial Examples},
  author={Meng, D. and Chen, H.},
  booktitle={Computer and Communications Security},
  year={2017},
  doi={10.1145/3133956.3134057}
}

@article{metzen2017detecting,
  title={On Detecting Adversarial Perturbations},
  author={Metzen, J. H. and Genewein, T. and Fischer, V. and Bischoff, B.},
  journal={ArXiv: Machine Learning},
  year={2017}
}

@inproceedings{mink2023security,
  title={"Security is not my field, I'm a stats guy": A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry},
  author={Mink, Jaron and others},
  booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
  year={2023}
}

@inproceedings{moosavi2015deepfool,
  title={DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks},
  author={Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  booktitle={Computer Vision and Pattern Recognition},
  year={2015},
  doi={10.1109/CVPR.2016.282}
}

@inproceedings{moosavi2016deepfool,
  title={DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks},
  author={Moosavi-Dezfooli, S.-M. and Fawzi, A. and Frossard, P.},
  year={2016},
  doi={10.1109/CVPR.2016.282}
}

@inproceedings{papernot2016practical,
  title={Practical Black-Box Attacks against Machine Learning},
  author={Papernot, N. and McDaniel, P. and Goodfellow, I. and Jha, S. and Celik, Z. B. and Swami, A.},
  booktitle={ACM Asia Conference on Computer and Communications Security},
  year={2016},
  doi={10.1145/3052973.3053009}
}

@inproceedings{papernot2017practical,
  title={Practical Black-Box Attacks against Machine Learning},
  author={Papernot, N. and McDaniel, P. and Goodfellow, I. and Jha, S. and Celik, Z. B. and Swami, A.},
  booktitle={Computer and Communications Security},
  year={2017},
  doi={10.1145/3052973.3053009}
}

@inproceedings{papernot2015limitations,
  title={The Limitations of Deep Learning in Adversarial Settings},
  author={Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  booktitle={European Symposium on Security and Privacy},
  year={2015},
  doi={10.1109/EUROSP.2016.36}
}

@inproceedings{papernot2016limitations,
  title={The Limitations of Deep Learning in Adversarial Settings},
  author={Papernot, N. and McDaniel, P. and Jha, S. and Fredrikson, M. and Celik, Z. B. and Swami, A.},
  year={2016},
  doi={10.1109/EUROSP.2016.36}
}

@inproceedings{papernot2015distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Papernot, N. and McDaniel, P. and Wu, X. and Jha, S. and Swami, A.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2015},
  doi={10.1109/SP.2016.41}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Papernot, N. and McDaniel, P. and Wu, X. and Jha, S. and Swami, A.},
  year={2016},
  doi={10.1109/SP.2016.41}
}

@inproceedings{pasquini2021eluding,
  title={Eluding Secure Aggregation in Federated Learning via Model Inconsistency},
  author={Pasquini, D. and Francati, D. and Ateniese, G.},
  booktitle={Conference on Computer and Communications Security},
  year={2021},
  doi={10.1145/3548606.3560557}
}

@inproceedings{pasquini2022insecurity,
  title={On the (In)security of Peer-to-Peer Decentralized Machine Learning},
  author={Pasquini, D. and Raynal, M. and Troncoso, C.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2022},
  doi={10.1109/SP46215.2023.10179291}
}

@inproceedings{salem2022sok,
  title={SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning},
  author={Salem, A. and Cherubin, G. and Evans, D. and Köpf, B. and Paverd, A. and Suri, A. and Tople, S. and Zanella-Béguelin, S.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2022},
  doi={10.48550/ARXIV.2212.10986}
}

@inproceedings{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, C. and Zaremba, W. and Sutskever, I. and Bruna, J. and Erhan, D. and Goodfellow, I. and Fergus, R.},
  booktitle={International Conference on Learning Representations},
  year={2013}
}

@inproceedings{tao2024distribution,
  title={Distribution Preserving Backdoor Attack in Self-supervised Learning},
  author={Tao, G. and Wang, Z. and Feng, S. and Shen, G. and Ma, S. and Zhang, X.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2024},
  doi={10.1109/SP54263.2024.00029}
}

@inproceedings{tramer2017ensemble,
  title={Ensemble Adversarial Training: Attacks and Defenses},
  author={Tramèr, F. and Kurakin, A. and Papernot, N. and Boneh, D. and McDaniel, P.},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{tramer2022truth,
  title={Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets},
  author={Tramèr, F. and Shokri, R. and Joaquin, A. S. and Le, H. M. and Jagielski, M. and Hong, S. and Carlini, N.},
  booktitle={Conference on Computer and Communications Security},
  year={2022},
  doi={10.1145/3548606.3560554}
}

@inproceedings{weber2020rab,
  title={RAB: Provable Robustness Against Backdoor Attacks},
  author={Weber, M. and Xu, X. and Karlaš, B. and Zhang, C. and Li, B.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2020},
  doi={10.1109/SP46215.2023.10179451}
}

@inproceedings{ye2021enhanced,
  title={Enhanced Membership Inference Attacks against Machine Learning Models},
  author={Ye, J. and Maddi, A. and Murakonda, S. K. and Bindschaedler, V. and Shokri, R.},
  booktitle={Conference on Computer and Communications Security},
  year={2021},
  doi={10.1145/3548606.3560675}
}

@inproceedings{zhang2024no,
  title={No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML},
  author={Zhang, Z. and Gong, C. and Cai, Y. and Yuan, Y. and Liu, B. and Ding, L. and Guo, Y. and Chen, X.},
  booktitle={IEEE Symposium on Security and Privacy},
  year={2024},
  doi={10.1109/SP54263.2024.00052}
}