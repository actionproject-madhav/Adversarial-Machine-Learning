% =============================================================================
% BIBLIOGRAPHY FOR ADVERSARIAL ML ADOPTION LAG PAPER
% =============================================================================

% -----------------------------------------------------------------------------
% FOUNDATIONAL ATTACKS (2013-2017)
% -----------------------------------------------------------------------------

@inproceedings{szegedy2014intriguing,
  title={Intriguing Properties of Neural Networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014},
  note={ICLR 2024 Test of Time Award}
}

@inproceedings{goodfellow2015explaining,
  title={Explaining and Harnessing Adversarial Examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015},
  note={arXiv:1412.6572, December 20, 2014}
}

@inproceedings{moosavi2016deepfool,
  title={{DeepFool}: A Simple and Accurate Method to Fool Deep Neural Networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2574--2582},
  year={2016},
  doi={10.1109/CVPR.2016.282},
  note={arXiv:1511.04599, November 14, 2015}
}

@inproceedings{carlini2017towards,
  title={Towards Evaluating the Robustness of Neural Networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={IEEE Symposium on Security and Privacy (SP)},
  pages={39--57},
  year={2017},
  doi={10.1109/SP.2017.49},
  note={arXiv:1608.04644, August 16, 2016}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  note={arXiv:1706.06083, June 19, 2017}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={IEEE Symposium on Security and Privacy (SP)},
  pages={582--597},
  year={2016},
  doi={10.1109/SP.2016.41}
}

@inproceedings{kurakin2017physical,
  title={Adversarial Examples in the Physical World},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={International Conference on Learning Representations (ICLR) Workshop},
  year={2017},
  note={arXiv:1607.02533, July 8, 2016}
}

@inproceedings{eykholt2018robust,
  title={Robust Physical-World Attacks on Deep Learning Visual Classification},
  author={Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={1625--1634},
  year={2018},
  doi={10.1109/CVPR.2018.00175}
}

% -----------------------------------------------------------------------------
% PRIVACY AND INTEGRITY ATTACKS
% -----------------------------------------------------------------------------

@inproceedings{shokri2017membership,
  title={Membership Inference Attacks Against Machine Learning Models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  doi={10.1109/SP.2017.41}
}

@inproceedings{tramer2016stealing,
  title={Stealing Machine Learning Models via Prediction {API}s},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th USENIX Security Symposium},
  pages={601--618},
  year={2016}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models},
  author={Carlini, Nicholas and Tram{\`e}r, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and others},
  booktitle={30th USENIX Security Symposium},
  pages={2633--2650},
  year={2021}
}

@article{gu2017badnets,
  title={{BadNets}: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@inproceedings{bagdasaryan2020backdoor,
  title={How to Backdoor Federated Learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={2938--2948},
  year={2020}
}

@article{jia2021badencoder,
  title={{BadEncoder}: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning},
  author={Jia, Jinyuan and Liu, Yupei and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2108.00352},
  year={2021}
}

@inproceedings{zhang2017dolphinattack,
  title={{DolphinAttack}: Inaudible Voice Commands},
  author={Zhang, Guoming and Yan, Chen and Ji, Xiaoyu and Zhang, Tianchen and Zhang, Taimin and Xu, Wenyuan},
  booktitle={ACM Conference on Computer and Communications Security (CCS)},
  pages={103--117},
  year={2017},
  doi={10.1145/3133956.3134052},
  note={ACM CCS 2017 Best Paper Award}
}

% -----------------------------------------------------------------------------
% LLM SECURITY - PROMPT INJECTION AND JAILBREAKING
% -----------------------------------------------------------------------------

@inproceedings{perez2022ignore,
  title={Ignore This Title and {HackAPrompt}: Exposing Systemic Vulnerabilities of {LLM}s through a Global Scale Prompt Hacking Competition},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  booktitle={NeurIPS 2022 Workshop on Machine Learning Safety},
  year={2022},
  note={arXiv:2211.09527}
}

@inproceedings{greshake2023indirect,
  title={Not What You've Signed Up For: Compromising Real-World {LLM}-Integrated Applications with Indirect Prompt Injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={ACM Workshop on Artificial Intelligence and Security (AISec)},
  year={2023},
  doi={10.1145/3605764.3623985},
  note={arXiv:2302.12173, February 23, 2023}
}

@inproceedings{zou2023universal,
  title={Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  note={Spotlight paper; arXiv:2307.15043, July 27, 2023}
}

@article{chao2024jailbreaking,
  title={Jailbreaking Black Box Large Language Models in Twenty Queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2024},
  note={Introduced PAIR attack}
}

@article{mehrotra2024tree,
  title={Tree of Attacks: Jailbreaking Black-Box {LLM}s with Depth-First Search},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2024},
  note={Introduced TAP attack}
}

@inproceedings{schulhoff2023hackaprompt,
  title={Ignore This Title and {HackAPrompt}: Exposing Systemic Vulnerabilities of {LLM}s through a Global Scale Prompt Hacking Competition},
  author={Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis-Fran{\c{c}}ois and Si, Chenglei and others},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024},
  note={137,000+ adversarial interactions, 44 defenses all eventually bypassed}
}

% -----------------------------------------------------------------------------
% LLM DEFENSES AND ALIGNMENT
% -----------------------------------------------------------------------------

@article{ouyang2022instructgpt,
  title={Training Language Models to Follow Instructions with Human Feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={27730--27744},
  year={2022},
  note={arXiv:2203.02155, March 4, 2022}
}

@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} Feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022},
  note={December 15, 2022}
}

@article{inan2023llamaguard,
  title={{Llama Guard}: {LLM}-based Input-Output Safeguard for Human-{AI} Conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023},
  note={Released December 7, 2023}
}

% -----------------------------------------------------------------------------
% DEFENSES AND EVALUATION
% -----------------------------------------------------------------------------

@inproceedings{croce2020autoattack,
  title={Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2206--2216},
  year={2020},
  note={arXiv:2003.01690, March 3, 2020}
}

@inproceedings{croce2021robustbench,
  title={{RobustBench}: A Standardized Adversarial Robustness Benchmark},
  author={Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year={2021},
  note={arXiv:2010.09670, October 2020}
}

@inproceedings{mazeika2024harmbench,
  title={{HarmBench}: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
  author={Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sachan, Mrinmaya and Fredrikson, Matt},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024},
  note={February 2024}
}

@inproceedings{cohen2019certified,
  title={Certified Adversarial Robustness via Randomized Smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1310--1320},
  year={2019},
  note={arXiv:1902.02918, February 8, 2019}
}

% -----------------------------------------------------------------------------
% THEORY-PRACTICE GAP STUDIES
% -----------------------------------------------------------------------------

@inproceedings{apruzzese2023real,
  title={``Real Attackers Don't Compute Gradients'': Bridging the Gap Between Adversarial {ML} Research and Practice},
  author={Apruzzese, Giovanni and Anderson, Hyrum S and Dambra, Savino and Freeman, David and Pierazzi, Fabio and Roundy, Kevin},
  booktitle={IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={339--364},
  year={2023},
  doi={10.1109/SaTML54575.2023.00031}
}

@inproceedings{kumar2020adversarial,
  title={Adversarial Machine Learning -- Industry Perspectives},
  author={Kumar, Ram Shankar Siva and Nystr{\"o}m, Magnus and Lambert, John and Marshall, Andrew and Goertzel, Mario and Comissoneru, Andi and Swann, Matt and Xia, Sharon},
  booktitle={IEEE Security and Privacy Workshops (SPW)},
  pages={69--75},
  year={2020},
  doi={10.1109/SPW50608.2020.00028},
  note={arXiv:2002.05646; 28 organizations surveyed}
}

@article{grosse2023mlsecurity,
  title={Machine Learning Security in Industry: A Quantitative Survey},
  author={Grosse, Kathrin and Bieringer, Lukas and Besold, Tarek R and Biggio, Battista and Krombholz, Katharina},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={18},
  pages={1749--1762},
  year={2023},
  doi={10.1109/TIFS.2023.3251842},
  note={139 practitioners surveyed; 5\% experienced AI-specific attacks, 86\% concerned}
}

@inproceedings{mink2023security,
  title={``Security is not my field, I'm a stats guy'': A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry},
  author={Mink, Jaron and Kaur, Harjot and Schmid, Juliane and Fahl, Sascha and Acar, Yasemin},
  booktitle={32nd USENIX Security Symposium},
  pages={3763--3780},
  year={2023},
  note={21 semi-structured interviews}
}

@inproceedings{arp2022dosdonts,
  title={Dos and Don'ts of Machine Learning in Computer Security},
  author={Arp, Daniel and Quiring, Erwin and Pendlebury, Feargus and Warnecke, Alexander and Pierazzi, Fabio and Wressnegger, Christian and Cavallaro, Lorenzo and Rieck, Konrad},
  booktitle={31st USENIX Security Symposium},
  pages={3971--3988},
  year={2022}
}

% -----------------------------------------------------------------------------
% INDUSTRY TOOLS
% -----------------------------------------------------------------------------

@techreport{papernot2016cleverhans,
  title={cleverhans v2.0.0: an adversarial machine learning library},
  author={Papernot, Nicolas and Faghri, Fartash and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Kurakin, Alexey and Xie, Cihang and Sharma, Yash and Brown, Tom and Roy, Aurko and others},
  institution={arXiv preprint arXiv:1610.00768},
  year={2016},
  note={October 2016}
}

@article{nicolae2018art,
  title={Adversarial Robustness Toolbox v1.0.0},
  author={Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and others},
  journal={arXiv preprint arXiv:1807.01069},
  year={2018},
  note={July 3, 2018; LF AI donation July 2020; graduated February 2022}
}

@article{pyrit2024,
  title={{PyRIT}: Python Risk Identification Tool for Generative {AI}},
  author={{Microsoft Security}},
  year={2024},
  note={Released February 22, 2024; arXiv:2410.02828}
}

% -----------------------------------------------------------------------------
% REGULATORY FRAMEWORKS
% -----------------------------------------------------------------------------

@misc{mitreatlas2024,
  title={{MITRE ATLAS}: Adversarial Threat Landscape for {AI} Systems},
  author={{MITRE Corporation}},
  year={2024},
  howpublished={\url{https://atlas.mitre.org/}},
  note={Launched June 2021; 66 techniques, 33 case studies as of 2024}
}

@techreport{nist2023airisk,
  title={Artificial Intelligence Risk Management Framework ({AI RMF} 1.0)},
  author={{National Institute of Standards and Technology}},
  institution={NIST},
  year={2023},
  number={AI 100-1},
  doi={10.6028/NIST.AI.100-1},
  note={Published January 26, 2023}
}

@techreport{nist2024genai,
  title={{AI RMF} Generative {AI} Profile},
  author={{National Institute of Standards and Technology}},
  institution={NIST},
  year={2024},
  number={AI 600-1},
  note={Published July 26, 2024}
}

@misc{euaiact2024,
  title={Regulation ({EU}) 2024/1689 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)},
  author={{European Parliament and Council}},
  year={2024},
  howpublished={Official Journal of the European Union},
  note={Entry into force August 1, 2024; high-risk requirements effective August 2, 2026}
}

@misc{owasp2024llm,
  title={{OWASP} Top 10 for {LLM} Applications 2025},
  author={{OWASP Foundation}},
  year={2024},
  howpublished={\url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}},
  note={v1.0 August 2023, v2.0 November 18, 2024}
}

% -----------------------------------------------------------------------------
% REAL-WORLD INCIDENTS
% -----------------------------------------------------------------------------

@misc{anthropic2025claude,
  title={Detecting and Countering Malicious Uses of {Claude}: November 2025},
  author={{Anthropic}},
  year={2025},
  howpublished={\url{https://www.anthropic.com/research/malicious-uses-nov-2025}},
  note={First documented large-scale AI-orchestrated cyber campaign; disclosed November 13-14, 2025}
}

@misc{keenlab2019tesla,
  title={Experimental Security Research of {Tesla} Autopilot},
  author={{Tencent Keen Security Lab}},
  year={2019},
  howpublished={\url{https://keenlab.tencent.com/en/2019/03/29/Tencent-Keen-Security-Lab-Experimental-Security-Research-of-Tesla-Autopilot/}},
  note={Published March 29, 2019}
}

@misc{mcafee2020tesla,
  title={Model Hacking {ADAS} to Pave Safer Roads for Autonomous Vehicles},
  author={{McAfee Advanced Threat Research}},
  year={2020},
  howpublished={McAfee Labs Blog},
  note={Published February 19, 2020; 58\% success rate with tape modification}
}

@article{nasr2023extracting,
  title={Scalable Extraction of Training Data from (Production) Language Models},
  author={Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A Feder and Ippolito, Daphne and Choquette-Choo, Christopher A and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023},
  note={November 28, 2023; extracted several MB from ChatGPT for ~\$200}
}

% -----------------------------------------------------------------------------
% BACKGROUND AND SURVEYS
% -----------------------------------------------------------------------------

@article{biggio2018wild,
  title={Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning},
  author={Biggio, Battista and Roli, Fabio},
  journal={Pattern Recognition},
  volume={84},
  pages={317--331},
  year={2018},
  doi={10.1016/j.patcog.2018.07.023}
}

@techreport{grother2019frvt,
  title={Face Recognition Vendor Test ({FRVT}) Part 2: Identification},
  author={Grother, Patrick and Ngan, Mei and Hanaoka, Kayee},
  institution={NIST},
  number={Interagency Report 8271},
  year={2019}
}

@article{dalpozzolo2015fraud,
  title={Calibrating Probability with Undersampling for Unbalanced Classification},
  author={Dal Pozzolo, Andrea and Caelen, Olivier and Johnson, Reid A and Bontempi, Gianluca},
  journal={IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
  pages={159--166},
  year={2015},
  note={Credit card fraud detection}
}

@article{gorwa2020algorithmic,
  title={Algorithmic Content Moderation: Technical and Political Challenges in the Automation of Platform Governance},
  author={Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
  journal={Big Data \& Society},
  volume={7},
  number={1},
  year={2020},
  doi={10.1177/2053951719897945}
}

% -----------------------------------------------------------------------------
% ADDITIONAL FOUNDATIONAL REFERENCES
% -----------------------------------------------------------------------------

@article{carlini2017adversarial,
  title={Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={ACM Workshop on Artificial Intelligence and Security (AISec)},
  pages={3--14},
  year={2017},
  doi={10.1145/3128572.3140444}
}

@inproceedings{athalye2018obfuscated,
  title={Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={274--283},
  year={2018},
  note={Defeated 7 of 9 ICLR 2018 defenses}
}

@article{carlini2019evaluating,
  title={On Evaluating Adversarial Robustness},
  author={Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
  journal={arXiv preprint arXiv:1902.06705},
  year={2019}
}

@inproceedings{papernot2017practical,
  title={Practical Black-Box Attacks against Machine Learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={ACM Asia Conference on Computer and Communications Security (ASIACCS)},
  pages={506--519},
  year={2017},
  doi={10.1145/3052973.3053009}
}

@inproceedings{brown2017adversarial,
  title={Adversarial Patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  booktitle={NeurIPS Workshop on Machine Learning and Computer Security},
  year={2017}
}

% -----------------------------------------------------------------------------
% DOMAIN-SPECIFIC REFERENCES
% -----------------------------------------------------------------------------

@inproceedings{anderson2018ember,
  title={{EMBER}: An Open Dataset for Training Static {PE} Malware Machine Learning Models},
  author={Anderson, Hyrum S and Roth, Phil},
  booktitle={arXiv preprint arXiv:1804.04637},
  year={2018}
}

@inproceedings{pierazzi2020intriguing,
  title={Intriguing Properties of Adversarial {ML} Attacks in the Problem Space},
  author={Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
  booktitle={IEEE Symposium on Security and Privacy (SP)},
  pages={1332--1349},
  year={2020},
  doi={10.1109/SP40000.2020.00073}
}

% -----------------------------------------------------------------------------
% MARKET AND INDUSTRY
% -----------------------------------------------------------------------------

@misc{hiddenlayer2023funding,
  title={{HiddenLayer} raises \$50M for its {AI}-defending cybersecurity tools},
  author={{TechCrunch}},
  year={2023},
  howpublished={TechCrunch},
  note={September 19, 2023; largest Series A for AI security in 2023}
}

@misc{protectai2025acquisition,
  title={Palo Alto Networks to Acquire {Protect AI}},
  author={{Palo Alto Networks}},
  year={2025},
  howpublished={Press Release},
  note={Announced April 28, 2025; estimated \$500-700M}
}

@misc{robustintelligence2024acquisition,
  title={Cisco to Acquire {Robust Intelligence}},
  author={{Cisco}},
  year={2024},
  howpublished={Press Release},
  note={Announced August 28, 2024; approximately \$400M}
}

@misc{calypsoai2025acquisition,
  title={F5 to Acquire {CalypsoAI}},
  author={{F5}},
  year={2025},
  howpublished={Press Release},
  note={September 2025; \$180M}
}